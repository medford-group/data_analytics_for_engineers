{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "<img src=\"images/clusters.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will cover basic considerations and concepts in clustering data, and introduce a few basic classes of algorithms along with examples.\n",
    "\n",
    "* Clustering basics\n",
    "    - Problem statement for clustering\n",
    "    - Types of clustering problems/algorithms\n",
    "    - Accuracy and distance metrics\n",
    "* Expectation-Maximization models\n",
    "    - k-means and vector quantization\n",
    "    - Gaussian mixture models\n",
    "* Density-based models\n",
    "    - Mean shift\n",
    "    - DBSCAN\n",
    "* Hierarchical clustering\n",
    "    - Dendrograms\n",
    "    - Agglomerative clustering\n",
    "* Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "Clustering algorithms seek to identify data points that are similar to each other based on a set of descriptive features.\n",
    "\n",
    "Clustering algorithms are **unsupervised** since they do not include output labels. This is an example of **exploratory data analysis** in which the goal is to extract insight about the dataset based on its inherent structure, rather than to build a model that predicts an output. These algorithms can be used for data compression, group assignment, searching, and/or model evaluation or feature extraction for supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of problems/algorithms\n",
    "\n",
    "There are a few key types of clustering algorithms:\n",
    "\n",
    "**Expectation-maximization** algorthims iteratively compute \"expected\" clusters and then \"maximize\" the parameters of the cluster to optimize the expectations. This is somewhat similar to an iterative version of a generalized linear classification algorithm: \"classes\" are assigned, then boundaries are found to optimize a cost function based on these classes. After this optimization the new class boundaries are used to assign classes again, and the optimization is repeated with new \"class\" labels.\n",
    "\n",
    "**Density-based** algorithms utilize local information about data points to identify regions where the data has similar density. Regions where there is substantially lower density of data form boundaries between these clusters. This is somewhat similar to k-nearest neighbors where classes are defined by local environments.\n",
    "\n",
    "**Hierarchical** algorithms map out the full network of connectivity within a dataset, then use a variable distance cutoff to assign clusters. These algorithms can be understood visually through a dendrogram, and have relatively few hyperparameters but they are more computationally demanding.\n",
    "\n",
    "A few considerations when selecting a clustering algorithm:\n",
    "\n",
    "* Some algorithms requre defining the number of clusters explicitly (e.g. most expectation-maximization algorithms) while others find this implicitly based on choice of hyperparameters (e.g. density-based or hierarchical)\n",
    "\n",
    "* Some algorithms allow **mixed membership** where points can belong to multiple clusters based on probabilities.\n",
    "\n",
    "* Some algorithms can identify/ignore outliers/noise (e.g. density-based), while others attempt to assign clusters to all points (e.g. expectation-maximization and hierarchical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and distance metrics\n",
    "\n",
    "Computing the accuracy of unsupervised models is difficult because there is no \"right answer\". However, it is possible to compute some quantitative metrics based on the concept of a cluster.\n",
    "\n",
    "* **Silhouette score** is defined for *each point* and is related to two distances:\n",
    "    - $a$ is the average distance between a point and all other points in its cluster\n",
    "    - $b$ is the average distance between a point and the points in the next nearest cluster\n",
    "    - $S = \\frac{b-a}{max(a,b)}$ is the silhoutte score\n",
    "    - $S = -1$ implies totally incorrect, $S=1$ implies totally correct\n",
    "    - Works best for dense, well-separated clusters\n",
    "    - Does not work well for density-based clusters (e.g. DBSCAN)\n",
    "    \n",
    "The silhouette score can help identify individual points that are not well-clustered, or an average/max silhouette score can be used to evaluate the quality of the entire clustering model. Other metrics can be used to evaluate the overall model:\n",
    "\n",
    "* **Variance ratio criterion** or \"Calinski-Harabaz score\" is related to the \"within class\" variance (similar to intra-class variance for classificaiton) and the \"between class\" variance (similar to the interclass variance for classification). The mathematical definition is available [here](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101) but is beyond the scope of this course.\n",
    "    - Variance ratio will be higher for dense and well-separated clusters\n",
    "    - Not bounded so it can be difficult to know what is \"good\" and what is \"bad\"\n",
    "    - Does not work well for density-based clusters (e.g. DBSCAN)\n",
    "    \n",
    "These approaches can be utilized to identify hyperparameters such as the number of clusters in the case where there is no *a priori* expectation about the number of clusters.\n",
    "\n",
    "Another common technique is to use clustering for classification problems. In this case the error metrics from classification can be applied (e.g. confusion matrices, precision, recall, etc.). The comparison of clustering and classification can provide insight into how well the classes are captured by proximity in the feature space.\n",
    "\n",
    "Finally, it is worth noting that essentially all clustering algorithms rely on some form of **distance metric**. The way that distance is defined can have substantial impact on how clustering analyses perform. Some common choices to compute the distance between two points $i$ and $j$:\n",
    "\n",
    "* Euclidean distance ($L_2$ norm): $D_{ij} = \\sqrt{sum((\\vec{x}_i - \\vec{x}_j)^2)}$\n",
    "* Manhattan distance ($L_1$ norm): $D_{ij} = sum(abs(\\vec{x}_i - \\vec{x}_j))$\n",
    "* Chebyshev distance ($L_\\infty$ norm): $D_{ij} = max(abs(\\vec{x}_i - \\vec{x}_j))$\n",
    "* Minkowsky distance ($L_P$ norm): $D_{ij} = (sum((\\vec{x}_i - \\vec{x}_j)^P)^{1/P}$\n",
    "\n",
    "It is also possible to define a weighted distance metric that can implicitly standardize the data, or weight nearby points much higher than far away points. An example is the Mahalanobis distance:\n",
    "\n",
    "* Mahalanobis distance: $D_{ij} = (\\vec{x}_i - \\vec{\\mu})^T \\underline{\\underline{C}}^{-1} (\\vec{x}_j - \\vec{\\mu})$\n",
    "    - $\\mu$ is the mean vector\n",
    "    - $\\underline{\\underline{C}}$ is the covariance matrix\n",
    "    \n",
    "   \n",
    "* Kernel distance: $D_{ij} = (\\vec{x}_i)^T \\underline{\\underline{K}} (\\vec{x}_j)$\n",
    "    - $\\underline{\\underline{K}}$ is a kernel-based weight matrix\n",
    "\n",
    "For simplicity we will typically default to Euclidean distance in most examples; however, changing distance metrics can substantially improve performance in real problems so it is worthwhile to experiment. This is usually as simple as changing a keyword for `scikit-learn` models, or writing a short function to compute the necessary distance.\n",
    "\n",
    "It is also useful to consider the **cophenetic correlation coefficient** when dealing with different distance metrics or \"linkages\" in hierarchical representations of high-dimensional data. This can be considered as a comparison between distance metrics and the Euclidean distance. This will be discussed more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-maximization models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means\n",
    "\n",
    "The k-means algorithm is the simplest and most intuitive clustering algorithm. It performs remarkably well under a number of assumptions:\n",
    "\n",
    "* Number of clusters are known\n",
    "* Clusters are roughly spherical\n",
    "* Clusters are separated by linear boundaries\n",
    "\n",
    "Even if these assumptions are violated, it often works anyway, especially in high dimensions (the \"blessing\" of dimensionality).\n",
    "\n",
    "The k-means algorithm works using the principal of **expectation-maximization**. This is an iterative type of algorithm that contains two basic steps:\n",
    "\n",
    "* Expectation: Assign points based on some \"expectation\" metric.\n",
    "* Maximization: Revise expectations based on maximizing a fitness metric.\n",
    "\n",
    "In the case of k-means we:\n",
    "\n",
    "* Expect that points close to the center of a cluster belong to that cluster\n",
    "* Maximize the proximity of points to the center of a cluster by moving the center\n",
    "\n",
    "This process is interated until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few toy data sets and a toy implementation of k-means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_moons, make_biclusters, make_circles\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, calinski_harabaz_score\n",
    "\n",
    "random_state = 1\n",
    "k = 4\n",
    "N = 300\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(N,centers=k,random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X_blobs, transformation)\n",
    "\n",
    "X_circles, y_circles = make_circles(N,noise=0.1, factor=0.4, random_state=random_state)\n",
    "\n",
    "X_moons, y_moons = make_moons(N, noise=0.1, random_state=random_state)\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(15,4))\n",
    "\n",
    "axes[0].scatter(X_blobs[:,0], X_blobs[:,1], c=y_blobs)\n",
    "axes[1].scatter(X_aniso[:,0], X_aniso[:,1], c=y_blobs)\n",
    "axes[2].scatter(X_circles[:,0], X_circles[:,1], c=y_circles)\n",
    "axes[3].scatter(X_moons[:,0], X_moons[:,1], c=y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(pt1, pt2):\n",
    "    \"Euclidean distance between two points\"\n",
    "    #note that this can also be performed with np.linalg.norm(pt1-pt2)\n",
    "    return np.sqrt(sum([(xi-yi)**2 for xi, yi in zip(pt1, pt2)]))\n",
    "\n",
    "def expected_assignment(pt, cluster_centers):\n",
    "    dists = [dist(pt,ci) for ci in cluster_centers] #<- find distance to each center\n",
    "    min_index = dists.index(min(dists)) #<- find the index (cluster) with the minimum dist\n",
    "    return min_index\n",
    "\n",
    "def new_centers(cluster_points, centers):\n",
    "    centers = list(centers)\n",
    "    for i,ci in enumerate(cluster_points):\n",
    "        if ci != []:\n",
    "            centers[i] = np.mean(ci, axis=0)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need an \"initial guess\" of cluster centers and we can apply the algorithm to a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_blobs\n",
    "y = y_blobs\n",
    "\n",
    "#cluster_centers = ([-0.5,0], [0.5,0])\n",
    "cluster_centers = ([-0.5,0], [-4,3])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], color='k', alpha=0.2)\n",
    "colors = {0:'r', 1:'g', 2:'b',3:'m'}\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    ax.plot(ci[0], ci[1], marker='*', markersize='12', color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many clusters will the algorithm find? Approximately where do you expect the centers to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below repeatedly to see how the algorithm converges. Re-run the cell above to re-start the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Plot old centers\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    ax.plot(ci[0], ci[1], marker='+', markersize='12', color=colors[i])\n",
    "    \n",
    "# Which cluster do we \"expect\" each point to belong to?\n",
    "clusters = [[],[],[],[]]\n",
    "for pt in X:\n",
    "    cluster_idx = expected_assignment(pt, cluster_centers)\n",
    "    clusters[cluster_idx].append(pt)\n",
    "    \n",
    "# What centers best represent these new assignments?\n",
    "cluster_centers = new_centers(clusters, cluster_centers)\n",
    "\n",
    "# Plot new assignments\n",
    "for i, ci in enumerate(clusters):\n",
    "    for pt in ci:\n",
    "        ax.plot(pt[0], pt[1], marker='o', color=colors[i], alpha=0.2)\n",
    "        \n",
    "# Plot new centers\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    print(ci)\n",
    "    ax.plot(ci[0], ci[1], marker='*', markersize='12', color=colors[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What will happen if this is applied to other datasets? How sensitive are answers to the initial guesses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is more efficient to utilize the `scikit-learn` implementation of `KMeans`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=4, random_state=random_state)\n",
    "model.fit(X)\n",
    "y_predict = model.predict(X)\n",
    "centers = model.cluster_centers_\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "axes[0].scatter(X[:,0], X[:,1], c=y_predict, cmap='RdBu')\n",
    "axes[1].scatter(X[:,0], X[:,1], c=y)\n",
    "for center in centers:\n",
    "    x_i = center[0]\n",
    "    y_i = center[1]\n",
    "    axes[0].plot(x_i, y_i, marker='*', color='k', mec='w', markersize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector quantization\n",
    "\n",
    "\"Vector quantization\" is a common application of clustering algorithms. The k-means algorithms is utilized to identify clusters of similar vectors, then each vector is replaced with the centroid of its cluster. This is often used for data compression. First let's start with a 1D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1d = X[:,0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X_1d, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram is a \"quantized\" version of the original data. Instead of all 300 (or $N$) x, y values the data can now be represented with 10 (or $bins$) bin locations and 10 counts. This approximates the entire dataset with substantially less information!\n",
    "\n",
    "Of course we can do better than this, since we know that some bins will be empty (or approximately empty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "model = KMeans(n_clusters=k, random_state=random_state)\n",
    "X_1d_skl = X_1d.reshape(-1,1)\n",
    "model.fit(X_1d_skl)\n",
    "y_predict = model.predict(X_1d_skl)\n",
    "centers = model.cluster_centers_\n",
    "unique, counts = np.unique(y_predict, return_counts=True) #<- useful way to count occurences!\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(X_1d, bins=k, alpha=0.5)\n",
    "ax.scatter(centers,counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that k-means performs similarly to the histogram binning, though there are some differences based on initial guesses and the alignment of the bins\\clusters with the data. This is a well-known issue with histograms an can be overcome with \"kernel density estimation\". This is beyond the scope of the lecture, but we will show it here for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "kde = gaussian_kde(X_1d)\n",
    "scale_factor = ((max(X_1d) - min(X_1d))/k)*len(X_1d)\n",
    "xx = np.linspace(min(X_1d), max(X_1d), 1000)\n",
    "ax.plot(xx, kde(xx)*scale_factor)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally k-means tends to give a slightly better, or at least equally good, estimate of the underlying distribution. The issues with histograms only get worse when going to higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider a 3-dimensional dataset with 255 bins in each dimension. How many bins are needed to represent this space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example corresponds exactly to the RGB color pallette used by computers. We can use this test space to illustrate vector quantization in action. We will work with a sample image of the Atlanta skyline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "\n",
    "atl = imread(\"images/atlanta.jpg\") #<-load the image\n",
    "print('Shape:', atl.shape)\n",
    "print('Max value:',atl.max())\n",
    "print('Min value:', atl.min())\n",
    "\n",
    "atl = np.array(atl, dtype=np.float64) / 255\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.imshow(atl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to recognize that this image is simply a list of 1703 $\\times$ 3397 = 5785091 data points in the 3-dimensional RGB color space. We can re-shape the image into a list of 3D points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconstruct_image(img):\n",
    "    n_x, n_y, rgb = img.shape\n",
    "    N = n_x*n_y\n",
    "    return img.reshape(N,rgb)\n",
    "\n",
    "def reconstruct_image(points, n_x,n_y):\n",
    "    N, rgb = points.shape\n",
    "    return points.reshape(n_x,n_y,rgb)\n",
    "\n",
    "n_x,n_y,rgb = atl.shape\n",
    "img_X = deconstruct_image(atl)\n",
    "print(img_X.shape)\n",
    "print(img_X[0])\n",
    "\n",
    "img = reconstruct_image(img_X, n_x,n_y)\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how many colors are in this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block takes too long to run due to the image size! The answer is\n",
    "\"\"\"\n",
    "uniques = []\n",
    "for pt in img_X:\n",
    "    unique = True\n",
    "    for pt_j in uniques:\n",
    "        if (pt_j == pt).all():\n",
    "            unique = False\n",
    "    if unique == True:\n",
    "        uniques.append(pt)\n",
    "    if len(uniques) % 10000 == 0:\n",
    "        print(len(uniques))\n",
    "print(len(uniques))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the goal is to compress the image by identifying the characteristic colors, and approximating each pixel with on of the $N$ characteristic colors instead of one of the 16 million possible colors. Let's consider the case where $N=64$. We don't want to train the k-means model on all 500k data points, so we will take a sub-sample of 1000 to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "k = 8\n",
    "N = 1000\n",
    "model = KMeans(n_clusters=k, random_state=random_state)\n",
    "X_train = shuffle(img_X, random_state=random_state)[:1000] #<- shuffle the points and take the first 1000 samples\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we find the approximated colors for all the data points in the image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.predict(img_X)\n",
    "codebook = model.cluster_centers_\n",
    "\n",
    "print(labels.shape)\n",
    "print(codebook.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to reconstruct the image, but this time we need to reconstruct the colors as well as the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_quantized_image(labels, codebook, n_x,n_y):\n",
    "    img = np.zeros((n_x,n_y,codebook.shape[1]))\n",
    "    idx = 0\n",
    "    for i in range(n_x):\n",
    "        for j in range(n_y):\n",
    "            img[i][j] = codebook[labels[idx]]\n",
    "            idx += 1\n",
    "    return img\n",
    "\n",
    "compressed_img = reconstruct_quantized_image(labels, codebook, n_x, n_y)\n",
    "fig, axes = plt.subplots(2,1,figsize=(15,16))\n",
    "axes[0].imshow(atl)\n",
    "axes[1].imshow(compressed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many total numbers are needed to represent the original image? How many for the compressed image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture models\n",
    "\n",
    "Gaussian mixture models, or GMM's, are another clustering approach based on expectation maximization. The approach is to model each cluster as a Gaussian distribution, and to model the entire dataset as a mixture of Gaussians. Mathematically:\n",
    "\n",
    "$ P(\\vec{x}) = \\sum_k \\phi_k \\mathcal{N}(\\vec{x}, \\vec{\\mu}, \\vec{\\sigma})$\n",
    "\n",
    "where $\\mathcal{N}$ is the normal distribution:\n",
    "\n",
    "* one dimension: $N(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( \\frac{-(x-\\mu)^2}{2 \\sigma ^2} \\right)$\n",
    "\n",
    "* multi-dimensional: $N(\\vec{x}, \\vec{\\mu}, \\underline{\\underline{\\Sigma}}) = \\frac{1}{(2 \\pi |\\underline{\\underline{\\Sigma}}|)} \\exp \\left( \\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\underline{\\underline{\\Sigma}}^{-1}  (\\vec{x} - \\vec{\\mu}) \\right)$\n",
    "\n",
    "where $\\underline{\\underline{\\Sigma}}$ is the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation step:\n",
    "\n",
    "We can calculate the expected probability that a point $i$ is in a cluster $k$ with the following formula for a 1d Gaussian:\n",
    "\n",
    "$\\gamma_{ik} = \\frac{\\phi_k \\mathcal{N}(\\vec{x}_i, \\vec{\\mu}_k, \\vec{\\sigma}_k)}{\\sum_j \\phi_j \\mathcal{N}(\\vec{x}_i, \\vec{\\mu}_j, \\vec{\\sigma}_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximization step:\n",
    "\n",
    "The parameters of the distributions can then be updated by calculating the maximum likelihood estimators for $\\phi$, $\\mu$, and $\\sigma$, similar to the way these parameters would be estimated for a single distribution:\n",
    "\n",
    "* $\\phi_k = \\sum_{i=1}^N \\frac{\\gamma_{ik}}{N}$\n",
    "* $\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}} $\n",
    "* $\\sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k)^2}{\\sum_{i=1}^N \\gamma_{ik}} $\n",
    "\n",
    "These parameters are derived by maximizing $P(\\vec{x})$ with respect to each parameter. The formulas for multi-dimensional Gaussians are derived in the same way but are more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/GMM.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: In a 100-dimensional space with 10 clusters how many total numbers/parameters are needed to define the Gaussian mixture model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture models are much more flexible than k-means models. It turns out that k-means is equivalent to a GMM if the covariance matrix is constrained to a constant diagonal matrix. Let's see how GMM's perform for some of the earlier datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = X_moons\n",
    "y = y_moons\n",
    "\n",
    "model = GaussianMixture(n_components=10, covariance_type='spherical') # diag, spherical\n",
    "model.fit(X)\n",
    "y_predict = model.predict(X)\n",
    "centers = model.means_\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "axes[0].scatter(X[:,0], X[:,1], c=y_predict, cmap='RdBu')\n",
    "axes[1].scatter(X[:,0], X[:,1], c=y)\n",
    "for center in centers:\n",
    "    x_i = center[0]\n",
    "    y_i = center[1]\n",
    "    axes[0].plot(x_i, y_i, marker='*', color='k', mec='w', markersize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many clusters are needed to accurately represent the \"moons\" dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-based models\n",
    "\n",
    "Density-based clustering algorithms consider local density of points and utilize this information to group points into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean shift algorithm\n",
    "\n",
    "The simplest density-based algorithm is the \"mean shift\" algorithm. This is similar to k-means in that we seek the centroid of each cluster. The difference is that in mean shift the number of clusters does not need to be specified. Instead a \"window\" is specified, and at each iteration the centroids are updated to centroid of all points in each window. Let's see how this works for a single point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(x1, x2):\n",
    "    # we will use the numpy 2-norm to calculate Euclidean distance:\n",
    "    return np.linalg.norm(x1-x2, 2) #<- the 2 is optional here since 2 is the default.\n",
    "\n",
    "def get_nearby_points(x, x_list, r):\n",
    "    # r is the radius\n",
    "    dist_pairs = []\n",
    "    for i,xi in enumerate(x_list):\n",
    "        dist = distance(x, xi)\n",
    "        dist_pairs.append([dist, i, xi]) #<- gives us the distance for each point\n",
    "    in_window = [pt[-1] for pt in dist_pairs if pt[0] <= r]\n",
    "    return in_window\n",
    "\n",
    "def get_new_centroid(old_centroid, x_list, r):\n",
    "    in_range = get_nearby_points(old_centroid, x_list, r)\n",
    "    if len(in_range) == 0:\n",
    "        new_centroid = old_centroid\n",
    "    else:\n",
    "        new_centroid = np.array(in_range).mean(axis=0)\n",
    "    return new_centroid\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the similarity to the kNN functions for prior lectures. It is a good idea to \"abstract out\" the distance function so that we could try other distance metrics easily.\n",
    "\n",
    "Let's apply this to a single point in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = [-1,1] #<- set an initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_blobs\n",
    "r = 2\n",
    "\n",
    "nearby = get_nearby_points(guess, X, r)\n",
    "nearby = np.array(nearby)\n",
    "new = get_new_centroid(guess, X, r)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(guess[0], guess[1], marker='*', color='k', markersize=15)\n",
    "ax.scatter(X[:,0], X[:,1], color='k', alpha=0.3)\n",
    "ax.scatter(nearby[:,0], nearby[:,1], color='r', alpha=0.5)\n",
    "ax.plot(new[0], new[1], marker='*', color='r', markersize=15)\n",
    "\n",
    "guess = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the block above to watch the point converge. You can play with the initial guess to see how it changes things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if the initial guess is very far away from a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean shift algorithm causes an initial guess for a centroid to move toward a point of higher density. However, it isn't clear exactly how to get initial guesses. If we choose random points then some will have no points around them and not move. It also isn't clear how to decide how many initial guess points we should use.\n",
    "\n",
    "The solution to this is to use each point of the dataset as an initial guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift_iteration(x_list, r):\n",
    "    centroids = []\n",
    "    for centroid in x_list:\n",
    "        new = get_new_centroid(centroid, x_list, r)\n",
    "        centroids.append(new)\n",
    "    return centroids\n",
    "\n",
    "new_centroids = mean_shift_iteration(centroids, r)\n",
    "\n",
    "news = np.array(new_centroids)\n",
    "olds = np.array(centroids)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], color='k', alpha=0.3)\n",
    "ax.scatter(olds[:,0], olds[:,1], color='k', marker='*', alpha=0.5)\n",
    "ax.scatter(news[:,0], news[:,1], color='r', marker='*', alpha=0.5)\n",
    "\n",
    "centroids = new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the algorithm we just need to iterate until the new centrods are the same as the old centroids, and assign points to the nearest centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift_clustering(x_list, r, tolerance=0.001):\n",
    "    # tolerance will define when new and old centroids are the same.\n",
    "    old_centroids = np.array(x_list)\n",
    "    new_centroids = np.zeros(x_list.shape)\n",
    "    delta = np.linalg.norm(old_centroids - new_centroids)\n",
    "    while delta >= tolerance:\n",
    "        new_centroids = mean_shift_iteration(old_centroids, r)\n",
    "        delta = np.linalg.norm(old_centroids - new_centroids)\n",
    "        old_centroids = np.array(new_centroids)\n",
    "        \n",
    "    unique_centroids = []\n",
    "    for centroid in new_centroids:\n",
    "        unique = True\n",
    "        for uc in unique_centroids:\n",
    "            if np.linalg.norm(uc - centroid) <= tolerance:\n",
    "                unique = False\n",
    "        if unique == True:\n",
    "            unique_centroids.append(centroid)\n",
    "            \n",
    "    labels = []\n",
    "    for pt in x_list:\n",
    "        min_dist = 1e99\n",
    "        for j,centroid in enumerate(unique_centroids):\n",
    "            if get_distance(pt,centroid) < min_dist:\n",
    "                label = j\n",
    "                min_dist = get_distance(pt,centroid)\n",
    "        labels.append(label)\n",
    "            \n",
    "    return labels, np.array(unique_centroids)\n",
    "\n",
    "labels, centroids = mean_shift_clustering(X, r)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=labels, cmap='RdBu', alpha=0.4)\n",
    "ax.scatter(centroids[:,0], centroids[:,1], marker='*', s=120, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why did the algorithm only find 2 clusters here? How could we modify it to find more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "The DBSCAN algorithm also uses a local sliding window similar to mean shift, but instead of defining clusters by centroids it defines the cluster by whether or not a point falls within the sliding window. We will not go through the algorithm in detail, but the general steps are:\n",
    "\n",
    "1) Start with a random point and find its neigbhors within distance $\\epsilon$.\n",
    "2) If there are a sufficient number of neigbhors (defined by a minimum points argument) then the clustering process starts. If not then the point is labeled as noise and a new point is selected until the clustering process starts.\n",
    "3) The neighbors within a distance $\\epsilon$ are added to the cluster.\n",
    "4) The nearest neighbor is selected as the next point, and the same process is repeated until all points within distance $\\epsilon$ of any point within a cluster are defined as being part of that cluster.\n",
    "5) Once a cluster has finished, a new point is selected and a new cluster is started. The process is repeated until all points have been assigned to a cluster or labeled as noise.\n",
    "\n",
    "The key hyperparameters are:\n",
    "    * epsilon - the radius to include in a cluster\n",
    "    * min_samples - the minimum number of samples within a radius of $\\epsilon$ such that a point is not considered noise.\n",
    "    \n",
    "The following animation illustrates how the DBSCAN algorithm works:\n",
    "\n",
    "<img src=\"images/DBSCAN.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of DBSCAN is that it can find clusters defined by highly non-linear boundaries, unlike k-means, mean shift, or even GMM's. Let's see how the `scikit-learn` implementation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = X_moons\n",
    "y = y_moons\n",
    "\n",
    "model = DBSCAN(eps=0.15, min_samples=1)\n",
    "y_predict = model.fit_predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "axes[0].scatter(X[:,0], X[:,1], c=y_predict, cmap='RdBu')\n",
    "axes[1].scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the clustering can be very sensitive to the hyperparameters! These hyperparameters will be related to the density of points, so you may be able to get a good guess based on intuition about the data or by looking at the data. However, some tuning is nearly always necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we want to predict the cluster of a new point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical models\n",
    "\n",
    "The final type of clustering we will discuss are \"hierarchical\" models. These models construct linkages between different points and use distance cutoffs to assign clusters. Examining the hierarchy of points is a useful way to get insight into the structure of a high-dimensional dataset without dimensional reduction. The downside is that it can be rather slow, since the algorithms scale as $N^3$. However, for the relatively small sizes of datasets typically encountered in engineering it is usually feasible to construct these hierarchies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms\n",
    "\n",
    "A \"dendrogram\" is a graphical representation of the distances between different points in some high-dimensional space. One intuitive but qualitative example of a dendrogram are the [species trees](https://www.instituteofcaninebiology.org/how-to-read-a-dendrogram.html) commonly used in biology:\n",
    "\n",
    "<img src=\"images/bio_dendrogram.png\" width=\"500\">\n",
    "\n",
    "We can see that it is possible to create different \"clusters\" of species based on different defining characteristics. By choosing more or less specific \"cutoffs\" we could create a few large clusters or many small clusters. The idea is similar for data sets. Let's see how it looks for some of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "X = X_blobs\n",
    "\n",
    "\n",
    "Z = linkage(X, method='single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look at the \"linkage\" output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape: {}'.format(X.shape))\n",
    "print('Z shape: {}'.format(Z.shape))\n",
    "print('Z[0]:', Z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"linkage\" output has 4 members. Each entry corresponds to the formation of a new cluster from smaller clusters, hence there are (n-1) entries (since the first entry involves 2 points, and each iteration adds one more point).\n",
    "\n",
    "* the first two entries are the index of the two points/clusters that are being combined (point 185 and 270)\n",
    "* the third entry is the distance between these clusters (0.0095)\n",
    "* the fourth entry is the total number of points in the new cluster (2 in this case)\n",
    "\n",
    "Note that we passed a \"method\" argument into the linkage. This describes the method that is used to calculate the distance between two clusters that have multiple points. There are more details available [here](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage), but a very qualitative descripton of some of the options:\n",
    "\n",
    "* `single`: take the minimum distance between any two points in the two clusters\n",
    "* `complete`: take the maximum distance between any two points in the two clusters\n",
    "* `average`: use an average of distances between points in the two clusters\n",
    "* `weighted`: weight distances differently between the agglomerated cluster and one being added\n",
    "* `centriod`: use the distance between cluster centroids\n",
    "* `ward`: use the distance that minimizes the variance between the clusters\n",
    "\n",
    "So, which one should we choose? This is where we can use the \"cophenetic coefficient\", which measures the ratio of the distance in \"linkage\" space to the distance in the high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Dij = pdist(X, metric='euclidean')\n",
    "for method in ['single','complete','average','weighted','centroid','ward']:\n",
    "    Z = linkage(X,method=method)\n",
    "    C, coph_dists = cophenet(Z,Dij)\n",
    "    print('cophenetic coefficient of {}: {}'.format(method, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which distance metric do you recommend? Does this make sense based on the structure of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dendrogram` function is a visual representation of this \"linkage\" structure. The \"color threshold\" tells the dendrogram a distance (y-axis value) below which to identify separate branches of the dendrogram as different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "Z = linkage(X,method='average')\n",
    "dendrogram(Z, color_threshold=4, ax=axes[1])\n",
    "axes[0].scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What will happen if we change the linkage mode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is easier to not show every single datapoint, and truncate the dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "dendrogram(Z, color_threshold=4, truncate_mode='lastp', p=10, ax=axes[1])\n",
    "axes[0].scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative hierarchical clustering\n",
    "\n",
    "Agglomerative clustering is easy to understand once the \"linkage\" structure makes sense. The number of clusters can be defined either explicitly (move up the tree until there are 'k' clusters) or implicitly (provide a linkage distance that defines separate clusters).\n",
    "\n",
    "The following animations illustrates this nicely:\n",
    "\n",
    "<img src=\"images/agglomerative.gif\" width=\"700\">\n",
    "\n",
    "The mechanics of doing this can be a little tricky, but luckily there are built-in functions to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "max_d = 20\n",
    "k = 4\n",
    "\n",
    "clusters_dist = fcluster(Z, max_d, criterion='distance')\n",
    "clusters_k = fcluster(Z, k, criterion='maxclust')\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,6))\n",
    "dendrogram(Z, color_threshold=max_d, truncate_mode='lastp', p=k, ax=axes[0])\n",
    "axes[1].scatter(X[:,0],X[:,1],c=clusters_dist)\n",
    "axes[2].scatter(X[:,0],X[:,1],c=clusters_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are options for determining the cutoffs automatically, but none of them are great! The most common is the inconsistency method, which monitors for \"jumps\" in the distance:\n",
    "\n",
    "* $I = \\frac{h-avg}{std}$\n",
    "    - $h$: merge height of cluster (length in y-directon on dendrogram)\n",
    "    - $avg$: average height of last $d$ merges\n",
    "    - $std$: standard deviation of last $d$ merges\n",
    "    \n",
    "If $I >= t$ where t is a specified threshold then this will be used as the cutoff. Let's see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_cutoff = 4\n",
    "clusters_I = fcluster(Z, I_cutoff, criterion='inconsistent', depth=5)\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(15,6))\n",
    "dendrogram(Z, ax=axes[0])\n",
    "axes[1].scatter(X[:,0],X[:,1],c=clusters_dist)\n",
    "axes[2].scatter(X[:,0],X[:,1],c=clusters_k)\n",
    "axes[3].scatter(X[:,0],X[:,1],c=clusters_I)\n",
    "print('Number of clusters:', max(clusters_I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many hyperparameters are needed for the inconsistency method? Would you recommend this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* Clustering algorithms are **unsupervised**.\n",
    "* Clustering is used for **exploratory data analysis**.\n",
    "* Assessing the accuracy of clustering methods is challenging if the labels are not known.\n",
    "* Distance metrics can make a big difference in how clustering algorithms perform.\n",
    "* Three types of algorithms were discussed:\n",
    "    -  Expectation Maximization\n",
    "    -  Density-based\n",
    "    -  Hierarchical\n",
    "* Some considerations for choosing an algorithm:\n",
    "    -  Is the number of clusters known?\n",
    "    -  Are data points expected to belong to a single cluster, or is **mixed membership** expected?\n",
    "    -  Is noise expected in the data?\n",
    "* Clustering algorithms are useful for:\n",
    "    -  Data compression (vector quantization)\n",
    "    -  Searching data\n",
    "    -  Grouping data\n",
    "    -  Establishing intuition\n",
    "* When clustering data it is a good idea to try many approaches with difference distance/error metrics to get a feel for which factors are important. \n",
    "\n",
    "* If possible, visualizing the data directly can provide substantial intuition about which methods will perform best.\n",
    "* Dendrograms are a useful way to indirectly visualize data in a high-dimensional space, and the difference between a dendrogram and a direct measure of distance can be quantified with the **cophenetic coefficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [Evaluation of clustering algorithms](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
    "* [Overview of 5 key clustering algorithms](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "* [Python Data Science Handbook: k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "* [Python Data Science Handbook: GMM](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n",
    "* [GMM formulas](https://brilliant.org/wiki/gaussian-mixture-model/)\n",
    "* [Color quantization example](http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html)\n",
    "* [Dendrogram and hierarchical clustering tutorial](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n",
    "* Hastie Ch. 13 (pg. 459)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
