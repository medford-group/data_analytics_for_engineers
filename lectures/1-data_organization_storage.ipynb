{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic 1: Data Organization and Storage\n",
    "\n",
    "<center>\n",
    "<img src=\"images/structured_data.jpg\" width=\"300\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (science) is like an iceberg:\n",
    "\n",
    "* Over 80% of time is spent cleaning/structuring data\n",
    "* Over 80% of existing data is \"unstructured\"\n",
    "* 90% of the data ever generated by humanity was generated in the last [two years](https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#4f728b360ba9)!\n",
    "    - ...but how much of it is useful?\n",
    "\n",
    "<center>\n",
    "<img src=\"images/data-iceberg.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "In this course most data will be at least partially cleaned, but in the real world this is rarely the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing an efficient data pipeline\n",
    "\n",
    "* Good integration of data storage + analysis can make or break a real project\n",
    "* Separate raw data from analysis facilitates reproducibility and scalability\n",
    "* Rapidly prototype and iteratevely improve.\n",
    "\n",
    "Important concepts:\n",
    "* Structured data\n",
    "  - Schemas\n",
    "  - Numpy array\n",
    "  - Pandas dataframe\n",
    "* Metadata\n",
    "  - HDF5\n",
    "* Unstructured data\n",
    "  - JSON\n",
    "  - APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data-structure continuum\n",
    "\n",
    "Most structured\n",
    "\n",
    "* **Index data with integers (matrices)**\n",
    "* **Index data with strings/integers (dataframes)**\n",
    "* Access data with structured queries (SQL)\n",
    "* **Access data through web requests (APIs)**\n",
    "* Access data through parsers (scraping)\n",
    "* Access data with unstructured queries (search engines)\n",
    "\n",
    "Least structured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definition of \"structured data\":\n",
    "\n",
    "Datasets that can be defined by a well-determined organizational structure (schema).\n",
    "\n",
    "* Matrices\n",
    "* Spreadsheets (e.g. Excel) or \"dataframes\" (e.g. `R`, `pandas`)\n",
    "* Relational databases (e.g. SQL)\n",
    "\n",
    "**Schemas** enable efficient data storage by separating the context of the data from the data itself.\n",
    "\n",
    "*Raw data:*\n",
    "\n",
    "Car 1:\n",
    "* Make: Toyota\n",
    "* Model: Corolla\n",
    "* Year: 2001\n",
    "\n",
    "Car 2:\n",
    "* Make: Toyota\n",
    "* Model: Prius\n",
    "* Year: 2012\n",
    "\n",
    "Schema:\n",
    "* Row = car number\n",
    "* Columns = Make, Model, Year\n",
    "* Values = [[Toyota, Corolla, 2001], [Toyota, Prius, 2012]]\n",
    "\n",
    "Schema + Values = Raw Data\n",
    "\n",
    "size(Schema + Values) < size(Raw Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "\n",
    "* fast/easy to query/manage\n",
    "* intuitive to work with\n",
    "* forces consideration of analysis ahead of time\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* missing/inconsistent data (data fusion)\n",
    "* balance of comprehensiveness vs. convenience in schema design\n",
    "* analysis may be limited by data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are numerous tools/strategies for storing structured data:\n",
    "\n",
    "* Relational databases (structured query language, SQL) - Good for very large datasets (not covered in this course)\n",
    "* Excel spreadsheets - common, but limited\n",
    "* **Matrices** - common and intuitive but limited to numbers\n",
    "* **Dataframes** - programmatic equivalent of spreadsheets. Implemented with `pandas` in Python. Can also function as \"light\" databases.\n",
    "* **HDF5 files** - hierarchical files similar to directory structure. Extremely useful for huge array-like data structures due to \"chunking\". Several Python libraries, we will use `h5py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices (Numpy arrays)\n",
    "\n",
    "A Numpy array is the Python data type for storing/manipulating multi-dimensional matrices.\n",
    "\n",
    "Matrices are an extreme example of structured data. Data is accessed by providing indices for each dimension. Indices must be integers, and all data must be numerical.\n",
    "\n",
    "Matrices do not have a built-in schema system, so it must be managed separately. We will briefly explore how to work with matrices in Python and how to use them for data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #<- very common shorthand for numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating `numpy` arrays\n",
    "\n",
    "There are a number of ways to initialize new numpy arrays, for example from\n",
    "\n",
    "* a Python list or tuples\n",
    "* using functions that are dedicated to generating numpy arrays, such as `arange`, `linspace`, etc.\n",
    "* reading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a vector: the argument to the array function is a Python list\n",
    "v = np.array([1,2,3,4])\n",
    "\n",
    "print(v)\n",
    "\n",
    "# a matrix: the argument to the array function is a nested Python list\n",
    "M = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "print(M)\n",
    "\n",
    "type(v), type(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The difference between the `v` and `M` arrays is only their shapes. We can get information about the shape and size of an array by using the `shape` and `size` properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(v.shape)\n",
    "print(M.shape)\n",
    "print(v.size)\n",
    "print(M.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Arrays are similar to lists, but they must contain a single type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M[0,0] = \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we want, we can explicitly define the type of the array data when we create it, using the `dtype` keyword argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M = np.array([[1, 2], [3, 4]], dtype=complex)\n",
    "\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Creating arrays with functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is often more efficient to generate large arrays instead of creating them from lists. There are a few useful functions for this in numpy:\n",
    "\n",
    "* `np.arange` - create a range with a specified step size (endpoints not included)\n",
    "* `np.linspace` - create a range with a specified number of points (endpoints *are* included)\n",
    "* `np.logspace` - create a range with a specified number of points in log space (endpoints *are* included)\n",
    "* `np.mgrid` - create points on a multi-dimensional grid (similar to meshgrid in matlab)\n",
    "* `np.random.rand` - create random number matrix from a uniform distribution\n",
    "* `np.random.randn` - create random number matrix from a standard normal distribution\n",
    "* `np.zeros` - create a matrix of zeros\n",
    "* `np.ones` - create a matrix of ones\n",
    "* `np.eye` - create identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 10, 0.5) # arguments: start, stop, step\n",
    "print(x)\n",
    "x = np.linspace(0,10,15)\n",
    "print(x)\n",
    "x = np.logspace(0,3,10,base=10)\n",
    "print(x)\n",
    "print([np.log10(xi) for xi in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x, y = np.mgrid[0:5, 0:5] # similar to meshgrid in MATLAB\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# uniform random numbers in [0,1]\n",
    "rand_uniform = np.random.rand(3,3)\n",
    "print(rand_uniform)\n",
    "# standard normal distributed random numbers\n",
    "rand_normal = np.random.randn(3,3)\n",
    "print(rand_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "z = np.zeros((3,3)) #note that these take 1 tuple argument instead of multiple integers\n",
    "one = np.ones((3,3))\n",
    "I = np.eye(3,3) #but not this one... this is an annoying inconsistency.\n",
    "print(z)\n",
    "print(one)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## File I/O\n",
    "\n",
    "* Numpy has built-in functionality for reading/writing CSV or TSV (tab-separated value) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M = np.random.rand(6,6)\n",
    "np.savetxt(\"random-matrix.csv\", M)\n",
    "M1 = np.genfromtxt(\"random-matrix.csv\")\n",
    "print(M1==M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to read in data from arbitrary text files is also useful. Consider the following `.dat` file that contains tab-separated values for temperature in Stockholm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!head datasets/stockholm_td_adj.dat #<- note that ! lets us run a bash command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('datasets/stockholm_td_adj.dat')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "What is the schema for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manipulating arrays\n",
    "\n",
    "Once we generate `numpy` arrays, we need to interact with them. This involves a few operations:\n",
    "\n",
    "* indexing - accessing certain elements\n",
    "* index \"slicing\" - accessing certain subsets of elements\n",
    "* fancy indexing - combinations of indexing and slicing\n",
    "\n",
    "This is not very different from Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can index elements in an array using square brackets and indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# v is a vector, and has only one dimension, taking one index\n",
    "print(v[0])\n",
    "# M is a matrix, or a 2 dimensional array, taking two indices \n",
    "print(M[1,1])\n",
    "# If an index is ommitted then the whole row is returned\n",
    "print(M[1])\n",
    "# This means that we can also index with multiple brackets if we want to type more:\n",
    "print(M[1][1] == M[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same thing can be achieved with using `:` instead of an index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(M[1,:]) # row 1\n",
    "print(M[:,1]) # column 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can assign new values to elements or rows in an array using indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M[0,0] = 1\n",
    "print(M)\n",
    "M[:,2] = -1\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Index slicing\n",
    "\n",
    "Index slicing is the name for the syntax `M[lower:upper:step]` to extract a subset of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.arange(1,20)\n",
    "print(A)\n",
    "print(A[1:8:2])\n",
    "print(A[1:8]) #This is the most common usage\n",
    "print(A[5:])\n",
    "print(A[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Array values can also be assigned using slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A[1:3] = [-2,-3]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Index slicing works exactly the same way for multidimensional arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R = np.random.rand(10,10,10)\n",
    "print(R.shape)\n",
    "subR = R[3:5, 1:4, 0]\n",
    "print(subR.shape)\n",
    "print(subR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fancy indexing\n",
    "\n",
    "Fancy indexing is the name for when an array or list is used in-place of an index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R = np.random.rand(4,4)\n",
    "print(R)\n",
    "print('-'*10)\n",
    "row_indices = [1, 3]\n",
    "print(R[row_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "col_indices = [1, -1] # remember, index -1 means the last element\n",
    "print(R[row_indices, col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transposing arrays\n",
    "\n",
    "Arrays can easily be transposed with `.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "skinny = np.random.rand(8,2)\n",
    "print(skinny)\n",
    "print(skinny.shape)\n",
    "fat = skinny.T\n",
    "print(fat)\n",
    "print(fat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These manipulations are extremely convenient - we only need a few characters to rapidly access/manipulate our data. However, remembering what each index means is tedious and error-prone. This can be included by baking the schema into the data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pandas dataframes\n",
    "\n",
    "Pandas dataframes are a very convenient way to interact with low-dimensional structured data. The basic dataframe object acts very similarly to an Excel file, but data can be manipulated with Python rather than clumsy Excel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "#Look at csv file - why skip 32 rows?\n",
    "df = pd.read_csv('datasets/oceanic_data-brewer.csv',skiprows=32)\n",
    "df['DATE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas dataframes are a cross between dictionaries and numpy arrays. Unlike arrays, they are allowed to hold multiple types, and they index columns and rows based on \"keys\" rather than numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#print(df.index) #row names\n",
    "#print(df.columns) # column names\n",
    "#print(df.values) #all data\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is easy to view and summarize data with the dataframe object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n",
    "#df.tail(10)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Data can be accessed in several ways:\n",
    "\n",
    "* Columns can be accessed directly with keys. \n",
    "* The `loc` method enables numpy-like indexing, fancy indexing, and slicing. \n",
    "* The `iloc` method is similar to `loc`, but indexes by position (rather than key)\n",
    "\n",
    "These methods return `pandas.Series` objects, that are basically 1D dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#df['LATITUDE']\n",
    "#df.loc[1,'LATITUDE']\n",
    "#df.loc[1]\n",
    "#df.loc[:,['LATITUDE','LONGITUDE']]\n",
    "#df.iloc[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Columns can be deleted in three ways:\n",
    "\n",
    "* del : delete the Series from the dataframe\n",
    "* pop() : delete the Series and return the Series\n",
    "* drop(labels, axis) : return a new dataframe with Series removed (do not modify original df)\n",
    "\n",
    "Rows must be \"dropped\".\n",
    "\n",
    "**Pay attention to whether operations are \"in place\" or not**. Many `pandas` operations are *not* \"in place\" by default. This means that they return a copy of the dataframe with modifications, rather than modifying the original dataframe object. This can be very confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Delete column\n",
    "#times = df.pop('TIME')\n",
    "#print(times)\n",
    "#del df['EXPOCODE']\n",
    "#dfnew = df.drop('CASTNO', axis=1)\n",
    "\n",
    "#Delete row\n",
    "df2 = df.drop(df.index[0])\n",
    "df2 = df2.drop(df.index[-1])\n",
    "df2.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's simplify this data into only the columns that we care about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interesting = ['DATE','LATITUDE','LONGITUDE','DEPTH','SALNTY','OXYGEN','SILCAT','NITRAT','PHSPHT', 'ALKALI']\n",
    "df = df[interesting]\n",
    "df = df.drop(df.index[0])\n",
    "df = df.drop(df.index[-1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It looks like there are some artifacts since a concentration of -999.0 does not make sense. We can get rid of these using \"boolean indexing\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df = df.astype('float')\n",
    "df.dtypes\n",
    "conc = ['OXYGEN','SILCAT','NITRAT','PHSPHT', 'ALKALI']\n",
    "for var in conc:\n",
    "    df = df[df[var] >= 0] #only take rows where concentration is >=0\n",
    "\n",
    "df = df.reset_index() #<- THIS IS IMPORTANT!!!\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This could also be achieved with OpenRefine. There is often no right way to do things in data science, so pick whatever feels easiest or most intuitive.\n",
    "\n",
    "Note that the dates were not parsed correctly. We can fix this with a loop and assign the proper dates to the DATE column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def convert_DATE(date_col):\n",
    "    dates = []\n",
    "    for dt in date_col:\n",
    "        dt = str(int(dt))\n",
    "        y,m,d = dt[:4], dt[4:6], dt[6:]\n",
    "        dates.append('-'.join([y,m,d]))\n",
    "    print(len(dates))\n",
    "    dates = pd.Series(dates)\n",
    "    dates = pd.to_datetime(dates,yearfirst=True)\n",
    "    return dates\n",
    "\n",
    "dates = convert_DATE(df['DATE'])\n",
    "df['DATE'] = dates # [(0, date0), (1, date1) ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas dataframes have some handy plotting features built in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(conc)\n",
    "#df[conc].plot()\n",
    "df_sorted = df.sort_values('PHSPHT')\n",
    "#df_sorted.plot(x='PHSPHT',y='SILCAT')\n",
    "df_sorted.hist('SILCAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas allows easily indexing by different columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.set_index('DATE')\n",
    "df2['1981-06-03':'1981-09-07']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pandas multi-indexing\n",
    "\n",
    "Pandas multi-indexing allows multi-dimensional `DataFrame` objects that act like different sheets/files in Excel. This is very useful for creating \"mini databases\" that allow organization of complex data with multiple dimensions.\n",
    "\n",
    "Let's consider another dataset similar to the one we just worked with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('datasets/oceanic_data-morliere.csv',skiprows=35)\n",
    "#quickly clean up the data:\n",
    "new_df = new_df[interesting]\n",
    "new_df = new_df.drop(0)\n",
    "new_df = new_df.astype('float')\n",
    "for var in conc:\n",
    "    new_df = new_df[new_df[var] > 0]\n",
    "new_df = new_df.reset_index()\n",
    "dates = convert_DATE(new_df['DATE'])\n",
    "new_df['DATE'] = dates\n",
    "new_df = new_df.set_index('DATE')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we might want to create a DateFrame that contains both sets of data, but index them by the scientist that collected the data (Brewer and Morliere). This can be achieved with the Multi Index functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "brewer = df2\n",
    "morliere = new_df\n",
    "full = pd.concat({'Brewer':brewer, 'Morliere':morliere})\n",
    "print(full.columns)\n",
    "full = full.T\n",
    "x = full['Brewer'].T['1981-05'] #<- get all measurements by Brewer in May 1981\n",
    "print(x.columns)\n",
    "x.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to convert from a `pandas` dataframe back to a `numpy` array (assuming all values are numerical):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata provides additional context to datasets\n",
    "\n",
    "* Metadata is \"data about the data\", and often contains details about where the dataset was collected or how it pertains to other similar datasets\n",
    "    - Name of person recording the data\n",
    "    - Place the data was recorded\n",
    "    - Type of equipment used\n",
    "* Data is often useless without its metadata\n",
    "\n",
    "For example, we can think of a matrix/array of numbers as \"data\", and the definition of what a row/column means as \"metadata\". Without the definition of rows/columns the matrix has no meaning.\n",
    "\n",
    "Keeping metadata and data synchronized is critical to ensuring that data remains valuable. This is why keeping a spreadsheet with column/row titles is a much more robust way of storing data than by keeping the matrix in one file and the column/row titles in another.\n",
    "\n",
    "Metadata is particularly necessary in the case of extremely large datasets, since it enables efficient identification of relevant datasets through searches or databases of metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDF 5 files\n",
    "\n",
    "HDF5 files (hierarchical data format) are useful for structured data, especially for very large datasets. HDF5 files act like mini filesystems, and have \"attributes\" to include additional context.\n",
    "\n",
    "We can create and manipulate HDF5 files from within Python using the `h5py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "! rm test.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"test.hdf5\", \"w\") #<- the \"w\" argument tells h5py to create a new file.\n",
    "dset = f.create_dataset(\"dset\", (100,100))\n",
    "X = np.random.rand(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#print(X[0,0])\n",
    "#dset[:,:] = X\n",
    "dset.shape\n",
    "dset.name\n",
    "dset[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDF5 files can have groups so that multiple datasets can be stored/organized in a single file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#grp = f.create_group('random_data')\n",
    "#d2 = grp.create_dataset('rand3D',(10,10,10))\n",
    "R = np.random.randn(10,10,10)\n",
    "d2[:,:,:] = R\n",
    "d2.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The HDF5 object directly interacts with the file, which can be opened with multiple instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "d2.shape\n",
    "print(d2.name)\n",
    "print(f['dset'][6,6])\n",
    "f['dset'].shape\n",
    "f['random_data/rand3D'].shape\n",
    "\n",
    "#iteration goes through all datasets in the file\n",
    "for d in f:\n",
    "    print(d)\n",
    "\n",
    "g = h5py.File('test.hdf5','r')\n",
    "g['dset'][6,6] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDF5 files also support \"attributes\" that can tag datasets with additional info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['dset'].attrs['name'] = 'random_matrix'\n",
    "f['dset'].attrs['size'] = (100,100)\n",
    "dset.attrs['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with \"big\" data\n",
    "\n",
    "Even Python cannot handle datasets of unlimited size. At some point, reading data in may become too slow to be practical, or impossible with RAM limits. Both `pandas` and `HDF5` have solutions for this:\n",
    "\n",
    "* `pandas`: read data from large files in \"chunks\"\n",
    "* `HDF5`: point to data on HDD and only read sub-sets when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: Convert xyz csv file to HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rho = pd.read_csv(\"datasets/electron_density.csv\")\n",
    "rho.shape\n",
    "rho.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv(\"datasets/electron_density.csv\",chunksize=10000):\n",
    "    print(chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "! rm density.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dx = 0.092826\n",
    "\n",
    "rho['i'] = round(rho['x']/dx,0).astype(int)\n",
    "rho['j'] = round(rho['y']/dx,0).astype(int)\n",
    "rho['k'] = round(rho['z']/dx,0).astype(int)\n",
    "\n",
    "\n",
    "xid = rho['i'].values\n",
    "yid = rho['j'].values\n",
    "zid = rho['k'].values\n",
    "\n",
    "rhoval = rho['rho'].values\n",
    "diffval = rho['difference'].values\n",
    "\n",
    "size = (xid.max()+1,yid.max()+1,zid.max()+1)\n",
    "\n",
    "rho_array = np.zeros(size)\n",
    "diff_array = np.zeros(size)\n",
    "\n",
    "size\n",
    "rho_array.shape\n",
    "rho_array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Naive solution takes forever!\n",
    "print(rho.shape)\n",
    "import time\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for idx,row in rho.iterrows():\n",
    "    count +=1\n",
    "    i = int(row.i)\n",
    "    j = int(row.j)\n",
    "    k = int(row.k)\n",
    "    rho_array[i,j,k] = row.rho\n",
    "    diff_array[i,j,k] = row.difference\n",
    "    t1 = time.time()\n",
    "    if t1 > t0+5:\n",
    "        break\n",
    "print(i,j,k)\n",
    "print(\"Total processed:\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Smart solution is almost instant!\n",
    "rho_array[xid,yid,zid] = rhoval\n",
    "diff_array[xid,yid,zid] = diffval\n",
    "\n",
    "\n",
    "#Check to be sure it worked\n",
    "xrand, yrand, zrand = np.random.randint(0,3,size=(3))\n",
    "print(rho_array[xrand,yrand,zrand])\n",
    "\n",
    "try:\n",
    "    rho = rho.set_index(['i','j','k'])\n",
    "except KeyError:\n",
    "    pass #already ran this block\n",
    "\n",
    "print(rho['rho'][int(xrand)][int(yrand)][int(zrand)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Store in HDF5 file:\n",
    "f = h5py.File('density.hdf5','w')\n",
    "rho_hdf5 = f.create_dataset('rho',data=rho_array)\n",
    "diff_hdf5 = f.create_dataset('diff',data=diff_array)\n",
    "f.attrs['dx'] = dx\n",
    "#rho_hdf5 = rho_array #<- this only assigns pointers! use the \"data\" keyword, or array.copy()\n",
    "#diff_hdf5 = diff_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = h5py.File('density.hdf5','r')\n",
    "rho_2 = g['rho']\n",
    "print(rho_array[4,5,6])\n",
    "print(rho_2[4,5,6])\n",
    "print(g.attrs['dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition of unstructured data\n",
    "\n",
    "Data that does not have a clear pre-defined structure.\n",
    "\n",
    "* Text documents\n",
    "* Websites\n",
    "* Videos\n",
    "* Course documents\n",
    "\n",
    "Definition is imprecise because \"structure\" may be implicit or hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Working with \"unstructured\" data\n",
    "\n",
    "To analyze \"unstructured\" data you must impart some structure on it. Schema-free databases facilitate working with data with no (or ill-defined) structure.\n",
    "\n",
    "#### Schema-free advantages\n",
    "\n",
    "* flexible\n",
    "* quick to set up\n",
    "* easy to evolve/reconfigure\n",
    "\n",
    "#### Schema-free disadvantages\n",
    "\n",
    "* slow(er) to query\n",
    "* harder to maintain\n",
    "* some structure must still be defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Working with \"unstructured\" data?\n",
    "\n",
    "#### Extracting \"structure\" from unstructured data\n",
    "* text processing (natural language processing) - entire field of CS!\n",
    "* data \"scraping\" - BeautifulSoup python package\n",
    "* API's (technically not \"unstructured\")\n",
    "\n",
    "Text processing and data scraping are beyond the scope of this class, but there are many tutorials online. API's (Application Programming Interface) are a common way of automatically accessing the structured version of \"unstructured\" data.\n",
    "\n",
    "#### How to work with \"unstructured\" data\n",
    "* JSON files - flexible \"partially structured\" data format\n",
    "* schema-free \"databases\" (MongoDB, ElasticSearch)\n",
    "* parsers for HTML/XML (e.g. BeautifulSoup)\n",
    "\n",
    "We will not cover schema-free databases in lecture, but MongoDB is easy to set up and has a nice Python interface (`pymongo`). Most other interactions are essentially text-based ways of interacting with the internet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## JSON Files\n",
    "\n",
    "JavaScript Object Notation (JSON) files are a common way of adding structure to data so that it is easier to pass between code and interact with programatically. Although originally developed for JavaScript, JSON is now one of the most widespread file types and is supported by most programming languages.\n",
    "\n",
    "JSON files are very intuitive to use with Python because they are basically just dictionaries and lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "info = '{\"course\":\"ChBE 4803\", \"instructors\": [\"Medford\", \"Comer\"], \"size\":45}' #<- note single/double quotes!\n",
    "#info = str(info)\n",
    "#print(info)\n",
    "js_info = json.loads(info) #<- json.loads loads from a string, json.load loads from a file.\n",
    "\n",
    "#js_info.keys()\n",
    "#js_info['instructors']\n",
    "\n",
    "\n",
    "\n",
    "#JSON is a great format for persistent storage of Python data structures:\n",
    "\n",
    "with open('test.json','w') as f:\n",
    "    json.dump(js_info, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json','r') as f:\n",
    "    new_info = json.load(f)\n",
    "    \n",
    "new_info.keys()\n",
    "new_info_dict = dict(new_info)\n",
    "new_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: PubChem database\n",
    "\n",
    "[PubChem Search](https://pubchem.ncbi.nlm.nih.gov/)\n",
    "\n",
    "* Extract and work with JSON representation\n",
    "* Use RESTful API to access data programatically\n",
    "* Demonstrate Python \"wrapper\" for the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Goal 1: Extract SMILES representation, molecular weight, and boiling point from PubChem JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Website: https://pubchem.ncbi.nlm.nih.gov/compound/222\n",
    "#Download -> Data used to generate this page -> JSON -> Save\n",
    "\n",
    "with open('datasets/ammonia.json') as f:\n",
    "    nh3 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Explore JSON structure\n",
    "nh3['Record']['Section'][0]['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Working with JSON data can be challenging if there are many nested structures, headers, etc. It is very useful to use a visualization tool:\n",
    "\n",
    "* [JSON Viewer](http://jsonviewer.stack.hu/)\n",
    "* [Code Beautify](https://codebeautify.org/jsonviewer)\n",
    "* [Chrome Extension](https://chrome.google.com/webstore/detail/json-viewer/gbmdgpbipfallnflgajpaliibnhdgobh?hl=en-US)\n",
    "\n",
    "From the visualizer we can see how to extract the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "SMILES = nh3['Record']['Section'][3]['Section'][2]['Section'][3]['Information'][0]['StringValue']\n",
    "MW = nh3['Record']['Section'][4]['Section'][0]['Information'][0]['Table']['Row'][0]['Cell'][1]['NumValue']\n",
    "BP = nh3['Record']['Section'][4]['Section'][1]['Section'][3]['Information'][2]['StringValue']\n",
    "BP, C = BP.split('Â°')\n",
    "SMILES\n",
    "\n",
    "## What can go wrong here???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Converting unstructured information to structured is tedious! The goal and challenge is to not just do this once, but do it in a way that works for other inputs. This can be even more challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# https://pubchem.ncbi.nlm.nih.gov/compound/441203\n",
    "\n",
    "with open('datasets/cisplatino.json') as f:\n",
    "    cp = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "subsec = cp['Record']['Section']\n",
    "for section in subsec:\n",
    "    try:\n",
    "        SMILES = section['Section'][2]['Section'][3]['Information'][0]['StringValue']\n",
    "    except:\n",
    "        pass\n",
    "SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def section_by_name(sections, name):\n",
    "    \"\"\" Take a list of Sections from PubChem JSON and return the section with a given name\"\"\"\n",
    "    for s in sections:\n",
    "        if s['TOCHeading'] == name:\n",
    "            return s\n",
    "        \n",
    "section_by_name(cp['Record']['Section'], \"Names and Identifiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can use this new function to create a more robust way of extracting info from the PubChem JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(pc_json):\n",
    "    \"\"\" Return the SMILES string, molecular weight, and boiling point from a PubChem JSON file\"\"\"\n",
    "    info = {} #<- we can store the info in this string as we grab it\n",
    "    ## Get SMILES string:\n",
    "    namesec = section_by_name(pc_json['Record']['Section'], \"Names and Identifiers\")\n",
    "    descsec = section_by_name(namesec['Section'], \"Computed Descriptors\")\n",
    "    smilesec = section_by_name(descsec['Section'], \"Canonical SMILES\")\n",
    "    SMILES = smilesec['Information'][0]['StringValue'] #<- we are assuming that there is only one entry here.\n",
    "    info['SMILES'] = SMILES\n",
    "    \n",
    "    ## Get molecular weight\n",
    "    propsec = section_by_name(pc_json['Record']['Section'],'Chemical and Physical Properties')\n",
    "    compsec = section_by_name(propsec['Section'],'Computed Properties')\n",
    "    MW = compsec['Information'][0]['Table']['Row'][0]['Cell'][1]['NumValue'] #<- we are assuming the table has a fixed structure\n",
    "    info['molecular_weight'] = MW\n",
    "    \n",
    "    ## Get boiling point\n",
    "    \n",
    "    ### boiling point is in the same properties section as molecular weight, so start from there\n",
    "    expsec = section_by_name(propsec['Section'],\"Experimental Properties\")\n",
    "    try:\n",
    "        bpsec = section_by_name(expsec['Section'], \"Boiling Point\")\n",
    "        bpstring = bpsec['Information'][2]['StringValue'] #<- two problems!\n",
    "        info['boiling'] = bpstring\n",
    "    except TypeError:\n",
    "        pass\n",
    "    \n",
    "    ## discuss how to handle problems\n",
    "    \n",
    "    return(info)\n",
    "    \n",
    "info = get_info(cp)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even with semi-structured data (JSON), it can be challenging to robustly and reliably extract structured information for analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## API's (Application Programming Interfaces)\n",
    "\n",
    "API's are like GUI's for experts. They are not limited to \"unstructured\" data, or even data in general. API is a term for any programmatic structure that makes it easier to interact with a more complex underlying code or data structure. However, they are particularly prevalent in data science because accessing data is much less painful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RESTful API's\n",
    "\n",
    "REST stands for \"representational state transfer\", and is a protocol that enables accessing data directly through a URL. This is a very common and very powerful approach because it allows the data provider to abstract the database back-end from the API. In other words, data providers can provide a uniform interface to data in relational (schema-driven) databases, schema-free databases, file servers, or services in any programming language. All the user needs to know is how to \"query\" from a URL. If you pay attention to URL's as you browse the web you will see that you use RESTful API's all the time without knowing it!\n",
    "\n",
    "<center>\n",
    "<img src=\"images/RESTful.png\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RESTful API's are simple enough that you can use them without specialized libraries. You just need to use HTTP protocol, which is implemented in the `requests` Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://www.chbe.gatech.edu/\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RESTful API's are designed to return data in specific structures, and respond to specific queries that are embedded in the URL. A few notes:\n",
    "\n",
    "* Many API's require a \"key\" or \"token\". This is to avoid spammers overloading their servers.\n",
    "* Most API's also limit the amount of data per request, and the rate of requests.\n",
    "* It is still necessary to understand the underlying structure of the data you are querying.\n",
    "\n",
    "You should always start by reading the documentation of an API to learn what you can/can't do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Goal 2: Use PubChem RESTful API to automatically get SMILES representation of a given compound\n",
    "\n",
    "[PubChem API tutorial documentation](http://pubchemdocs.ncbi.nlm.nih.gov/pug-rest-tutorial$_Toc458584421)\n",
    "\n",
    "[PubChem API full documentation](http://pubchemdocs.ncbi.nlm.nih.gov/pug-rest)\n",
    "\n",
    "Let's start by seeing if we can get the \"search\" part to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/ammonia/cids/TXT')\n",
    "r.text #<- this is the CID of the compound  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_CID(chemical):\n",
    "    r = requests.get('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/cids/TXT'.format(chemical))\n",
    "    return r.text\n",
    "\n",
    "cid = get_CID('ethanol')\n",
    "print(cid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we need to understand the structure of the query to decide how to search. From the documentation:\n",
    "\n",
    "* prolog: `https://pubchem.ncbi.nlm.nih.gov/rest/pug`\n",
    "\n",
    "* input: `/compound/name/ammonia`\n",
    "\n",
    "* operation: `/cids`\n",
    "\n",
    "* output: `/TXT`\n",
    "\n",
    "We already have the input operation working, and since we just want SMILES the output can also be TXT. We just need to modify the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_SMILES(chemical):\n",
    "    r = requests.get('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/property/CanonicalSMILES/TXT'.format(chemical))\n",
    "    return r.text\n",
    "\n",
    "N = get_SMILES('ethylene glycol')\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is much easier, less memory intensive, and more robust, than trying to extract the property from the full output! However, if you do really want to parse from the full output you can do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_full(chemical):\n",
    "    r = requests.get('https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/record/json'.format(chemical))\n",
    "    return r.text\n",
    "\n",
    "json_string = get_full('ammonia')\n",
    "nh3 = json.loads(json_string)\n",
    "nh3['PC_Compounds'] #<- Note that this JSON is in a very different structure from the original!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python API's\n",
    "\n",
    "RESTful API's are widely used and easy to interact with. However, reading the documentation and converting more complex queries into the proper URL can be tedious and time consuming. Furthermore, not all data sources use RESTful API's.\n",
    "\n",
    "Python is one of the most common languages for API's, and widely-used data sources (e.g. PubChem) will often have a Python \"wrapper\" for their RESTful API.\n",
    "\n",
    "We can use the [PubChemPy](https://pypi.python.org/pypi/PubChemPy/1.0) API to achieve the same goal, but we will need to install it first:\n",
    "\n",
    "`pip install PubChemPy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pubchempy as pcp\n",
    "#Now we have access to some more intuitive function names and documentation\n",
    "help(pcp)\n",
    "#dir(pcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python APIs make code more readable, and are more intuitive to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compounds = pcp.get_compounds('Ammonia','name')\n",
    "nh3 = compounds[0]\n",
    "nh3.atoms #<- the full .json output is already parsed into a nice Python data structure\n",
    "nh3.bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can access the same SMILES string via the Python API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dir(nh3) #<- the Python API doesn't store the SMILES string by default\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "p = pcp.get_properties('CanonicalSMILES', 'ammonia', 'name')\n",
    "print(p) #<- this works, but is it really better than the RESTful version?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "* Structured data is efficient and convenient, but rigid. \n",
    "    - All the work goes into setting up the schema and ensuring that only good data gets in.\n",
    "    - Structured data is valuable but rare\n",
    "* Metadata provides context for structured data\n",
    "    - Separating the context from the data enables rapid searches, especially for large datasets\n",
    "    - Metadata can also be \"structured\" or \"unstructured\"\n",
    "    - Keeping metadata and data \"synchronized\" is facilitated by file types that integrate meta-data with data.\n",
    "* Unstructured data is flexible but more complex to query. \n",
    "    - It is easy to put data in regardless of whether it is good or not, but you have to \"clean\" it after querying\n",
    "    - Most data generated by others is available only as \"unstructured\" data\n",
    "    - Unstructured data can be \"structured\" manually or by using API's\n",
    "\n",
    "When retrieving data it is a good idea to read about all of the available retrieval strategies (web scraping, direct download, RESTful API's, Python API's) and design a strategy that maximizes efficiency and flexibility.\n",
    "\n",
    "When storing your own data you should find a balance between \"unstructured\" and \"structured\" that makes sense based on your project. Consider setting up a (schema-free) database and/or custom API to create a seamless interface between your data source and your analysis code, and/or using file types like HDF5 that keep meta-data and data together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Reading:\n",
    "\n",
    "* [Official pandas tutorials and cookbooks](https://pandas.pydata.org/pandas-docs/stable/tutorials.html)\n",
    "* [10 minutes to pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/10min.html)\n",
    "* [Usefule Pandas features](http://nbviewer.jupyter.org/urls/gist.github.com/wesm/4757075/raw/a72d3450ad4924d0e74fb57c9f62d1d895ea4574/PandasTour.ipynb)\n",
    "* [DataCamp Pandas tutorial](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python)\n",
    "* [Visualization pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/visualization.html)\n",
    "* [Multi-index pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/advanced.html)\n",
    "* [HDF5 quick start](http://docs.h5py.org/en/latest/quick.html)\n",
    "* [Hitchhiker's Guide to Python JSON tutorial](http://docs.python-guide.org/en/latest/scenarios/json/)\n",
    "* [RESTful details](https://restfulapi.net/)\n",
    "* [PubChem RESTful API tutorial](http://pubchemdocs.ncbi.nlm.nih.gov/pug-rest-tutorial)\n",
    "* [PubChem Python API documentation](https://pypi.python.org/pypi/PubChemPy/1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
