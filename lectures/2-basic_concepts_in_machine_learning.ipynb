{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts in Machine Learning\n",
    "\n",
    "<center>\n",
    "<img src=\"images/overview.png\" width=\"900\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will discuss the basic structure and concepts that underly machine learning models, and explore the various types of algorithms used in machine learning through prototype examples. Ultimately the `scikit-learn` python package and other useful programming tools will be introduced. \n",
    "\n",
    "* Programming tools\n",
    "    - scikit-learn\n",
    "    - scipy\n",
    "    - autograd\n",
    "* Machine learning basics:\n",
    "    - The structure of a machine-learing model\n",
    "    - Classes of models\n",
    "    - Model complexity (hyperparameter selection)\n",
    "    - Model parameter optimization (loss/objective function)\n",
    "    - Model validation (cross validation)\n",
    "    - Interpolation vs. Extrapolation\n",
    "    - Local vs. Global models\n",
    "* Mathematical background:\n",
    "    - Linear vs. Non-linear models\n",
    "    - Linear algebra\n",
    "    - Numerical optimization\n",
    "* Prototype algorithms\n",
    "    - Generalized linear regression\n",
    "    - kNN (classification)\n",
    "    - k-means (clustering)\n",
    "    - PCA (dimensional reduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming tools\n",
    "\n",
    "Machine learning algorithms can be complicated. Luckily, many implementations are openly available in Python through the `scikit-learn` package. There are [tutorials](http://scikit-learn.org/stable/tutorial/index.html) available, and plenty of pages with more [background info](https://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [optimization packages](https://docs.scipy.org/doc/scipy/reference/optimize.html) in `scipy` can also be very useful, especially when implementing your own machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `autograd` package provides [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) of python functions. This is a somewhat advanced feature that is generally beyond the scope of this course, but it is a very useful tool to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np   # autograd has its own \"version\" of numpy that must be used\n",
    "from autograd import grad # the \"grad\" function provides derivatives\n",
    "\n",
    "def position(t):\n",
    "    s1 = 5\n",
    "    s2 = 20\n",
    "    s3 = 40\n",
    "    if t < 10:\n",
    "        pos = s1*t\n",
    "    elif t < 15:\n",
    "        pos = s2*t - 10*(s2-s1)\n",
    "    else:\n",
    "        pos = s3*t - 15*(s3-s2) - 10*(s2-s1)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take the derivative of this function using `autograd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "t = np.linspace(0,20,100) # create a time series from 0-20 with 100 points\n",
    "x = [position(ti) for ti in t]\n",
    "speed = grad(position)\n",
    "accel = grad(speed)\n",
    "s = [speed(ti) for ti in t]\n",
    "a = [accel(ti) for ti in t]\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "axes[0].plot(t,x)\n",
    "axes[1].plot(t,s)\n",
    "axes[2].plot(t,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it is most efficient to use a model that is already implemented in `scikit-learn` (or other packages) in a practical scenario, but it is also useful to build your own versions of the algorithms to better understand how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of a machine-learning model\n",
    "\n",
    "The goal of machine learning is to utilize only data to \"learn\" a relationship between an input and an output:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x})$\n",
    "\n",
    "where $f$ is the model, $x$ is the model input and $y$ is the model output. The model inputs, $\\vec{x}$ are often called the **features** of a data point. Sometimes the features are easy to obtain directly from the raw data (e.g. numerical attributes like concentration, temperature, pressure, etc.), but as we will see later in the course extracting features from raw data can be a challenge (e.g. images, audio, etc.). Features are \"fingerprints\" of raw input data.\n",
    "\n",
    "Of course representing the model as $f$ is a gross oversimplification. The model needs:\n",
    "\n",
    "* **parameters**, $\\vec{W}$, that define its behavior (e.g. slope, intercept)\n",
    "* **hyperparameters**, $\\vec{\\eta}$, that define the structure of the model (e.g. the number of polynomial terms)\n",
    "\n",
    "Notably, $\\vec{W}$ will depend on $\\vec{\\eta}$. So, we can be a little more specific:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    "\n",
    "Machine learning seeks to optimize both $\\vec{W}$ (parameter optimization) and $\\vec{\\eta}$ (complexity optimization) in order to obtain a model that generalizes to new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "example = pd.read_csv('datasets/example.csv')\n",
    "example.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = example['x1']\n",
    "y = example['x2']\n",
    "c = example['class']\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(x,y)#,c=c)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes of machine-learning models\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    "\n",
    "We can classify models based on a few criteria:\n",
    "\n",
    "#### Supervised vs. Unsupervised\n",
    "\n",
    "In a \"supervised\" model we have \"training data\" for $\\vec{y}$, meaning that there are examples to define the pattern/relationship we are looking for.\n",
    "\n",
    "In an \"unsupervised\" model $\\vec{y}$ is determined by the structure of the inputs $\\vec{x}$. This is not really intutive given the way $y=f(x)$ is written; another way of thinking about it is that we look for \"inherent\" patterns in $\\vec{x}$ and those are our \"outputs\".\n",
    "\n",
    "#### Supervised models: Classification and regression\n",
    "\n",
    "Supervised models can be classified by the nature of their outputs. If the output $\\vec{y}$ is a continuous variable then it is a **regression** model, while if it is a discrete (boolean, ordinal, integer, etc.) variable then it is a **classification** model.\n",
    "\n",
    "#### Unsupervised models: Dimensional reduction and clustering\n",
    "\n",
    "There are two main types of unsupervised learning. **Dimensional reduction** algorithms project high-dimensional (many parameters) inputs ($\\vec{x}$) to a lower-dimensional space ($\\vec{\\tilde{x}}$, where len($\\vec{\\tilde{x}}$) < len($\\vec{x}$)). **Clustering** algorithms assign labels/groups to data points based on similarity metrics. Clustering is like unsupervised classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Polynomial regression\n",
    "\n",
    "$y = b + m*x + p*x^2 + ...$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4 + ...$\n",
    "\n",
    "$y = \\sum_i^N \\beta_i x_i^i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    return np.array([x**k for k in range(0,N+1)]).T\n",
    "\n",
    "X = polynomial_features(x, 4)\n",
    "print(x.shape,X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` approach to polynomial regression is to build up a \"feature space\" of polynomial transforms of the original data, then use multi-linear regression. The common notation is to refer to the feature space as `X` and output as `y` for supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X,y) #<- \"fit\" the model (optimize the parameters beta_i)\n",
    "yhat = model.predict(X) #<- get the model predictions (evaluate the optimized function)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y, color='g') #<- plot the original data\n",
    "ax.scatter(x,yhat,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model complexity\n",
    " \n",
    " $\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    " \n",
    " The \"complexity\" of a model is defined by its hyperparameters ($\\vec{\\eta}$). The goal of machine learning is to **optimize the complexity** of a model so that it **generalizes to new examples**. In order to achieve this goal we first need a way to quantify complexity so that we can optimize it.\n",
    " \n",
    " In general there are a few strategies:\n",
    " \n",
    " * Number of parameters: \"Complexity\" varies linearly with number of parameters\n",
    " * Information criteria: \"Complexity\" varies logarithmically with number of parameters and is balanced with model accuracy.\n",
    " * \"Smoothness\": \"Complexity\" is related to the maximum curvature of the model\n",
    " \n",
    " \"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\"\n",
    " \n",
    " -- John Von Neumann\n",
    " \n",
    " (see an [example here](https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameter optimization\n",
    "\n",
    "The \"parameters\" of the model, $\\vec{W}$, must also be determined. This is achieved through numerical and/or analytical optimization of a \"loss function\" or \"objective function\" ($L(\\vec{W})$). This function defines the quality of the model.\n",
    "\n",
    "Typical loss functions include:\n",
    "\n",
    "* Sum of squared error (least-squares regression)\n",
    "* Sum of squared error plus size of parameters (regularization)\n",
    "* Mean absolute percentage error (neural networks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = sum((y-yhat)**2)\n",
    "MAE = np.sqrt(SSE)/len(y)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation\n",
    "\n",
    "We also need a strategy to see if our model will **generalize to new examples**. This is achieved by \"cross-validation\", where some examples (\"test\" examples) are hidden when the model is fit to \"training\" examples, and the loss function is assessed on the data that was hidden.\n",
    "\n",
    "There are many strategies for cross-validation:\n",
    "\n",
    "* hold-out: randomly leave out a percentage (usually ~30%) of the data during training.\n",
    "* k-fold: select `k` (usually 3-5) randomly-assigned sub-groups of data, and train `k` times holding each group out.\n",
    "* leave p out: leave `p` (usually 1) samples out of the training and assess the error for the `p` that were left out. Repeat for all possible `p` subsets of the sample.\n",
    "* bootstrapping: random selection with replacement to generate a sample of the same size as the original dataset, with a number of repetitions.\n",
    "\n",
    "Cross-validation is used to determine hyperparameters. In this case, even the \"test\" sets are used to optimize the model. It is common to select an additional \"validation\" or \"holdout\" subset for a final validation of the model.\n",
    "\n",
    "Important (and often violated) assumption: **The collected data is representative of future data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = 2\n",
    "X = polynomial_features(x, N)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)\n",
    "\n",
    "model.fit(X_train,y_train) #<- \"fit\" the model with training data\n",
    "yhat = model.predict(X_test) #<- get the model predictions for the test data\n",
    "\n",
    "SSE = sum((y_test-yhat)**2)\n",
    "MAE = np.sqrt(SSE/len(y)) #<- evaluate the loss function for the test data\n",
    "print('MAE (test):', MAE)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y) #<- plot the original data\n",
    "ax.scatter(X_test[:,1],yhat,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the value of the hyperparameter of the polynomial regression model that optimizes the complexity for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation vs. Extrapolation\n",
    "\n",
    "In general, machine learning models can **only** interpolate. There are possible exceptions, but this requires some specialized model development and/or prior knowledge of the nature of the model.\n",
    "\n",
    "\"Extrapolation\" with machine learning models is typically achieved through search/exploration algorithms or \"adaptive learning\". These algorithms utilize machine-learning models to produce an iterative experimental design scheme that involves collection of new data. This effectively turns extrapolation problems into interpolation problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_extrap = np.linspace(min(x)-5, max(x)+5, 100)\n",
    "X_extrap = polynomial_features(x_extrap, N)\n",
    "\n",
    "y_extrap = model.predict(X_extrap) #<- try to make a prediction out of original bounds\n",
    "\n",
    "ax.scatter(X_extrap[:,1],y_extrap,color='r')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric vs. Non-parametric\n",
    "\n",
    "A \"parametric\" model has parameters that do not explicitly depend on or include the input points. The polynomial regression model is an example of a parametric model. The number of parameters is fixed with respect to the number of data points.\n",
    "\n",
    "A \"non-parametric\" model includes some parameters or hyperparameters that are defined on the domain of the independent variable and depend on the inputs. A spline model is an example of a local model. The number of parameters in the model varies with the number of data points.\n",
    "\n",
    "Nonparametric models are generally excellent for interpolation, but fail miseraly for extrapolation, while parametric models are less accurate for interpolation but provide more reasonable extrapolations. Nonparametric models tend to have many more parameters, and proper optimization of model complexity can lead to similar performance for both types.\n",
    "\n",
    "See [this post](https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and non-linear models\n",
    "\n",
    "<center>\n",
    "<img src=\"images/convex.png\" width=\"300\">\n",
    "<img src=\"images/nonconvex.png\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "Linear models have a unique solution that can typically be obtained analytically and evaluated using linear algebra. Most linear models can be reduced to:\n",
    "\n",
    "$ \\underline{\\underline{A}} \\vec{x} = \\vec{b} $\n",
    "\n",
    "or, in summation notation:\n",
    "\n",
    "$ \\sum_i A_{ij} x_{i} = b_j $\n",
    "\n",
    "Non-linear models have multiple optima, and are considerably more complex. They can be solved with a range of algorithms that are often applied directly to the objective function:\n",
    "\n",
    "$\\frac{\\partial L(\\vec{W})}{\\partial \\vec{W}} = 0$\n",
    "\n",
    "Index notation can be useful for taking derivatives with respect to vectors:\n",
    "\n",
    "$\\frac{\\partial L(W_j)}{\\partial W_i} = 0$\n",
    "\n",
    "This is the \"Jacobian\" ($J_i$) of the loss function. Many optimization algorithms also use the \"Hessian\" matrix:\n",
    "\n",
    "$H_{ij} = \\frac{\\partial^2 L(W_k)}{\\partial W_i \\partial W_j}$\n",
    "\n",
    "The \"Jacobian\" and \"Hessian\" are multi-dimensional first- and second-derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on notation\n",
    "\n",
    "The use of \"index\" or \"summation\" notation is common in machine learning, and makes implementation and vector calculus much more convenient. You should be comfortable with (at least) the standard \"[vector notation](https://en.wikipedia.org/wiki/Vector_notation)\" ($\\underline{\\underline{A}}$, $\\vec{x}$) and \"[index notation](https://en.wikipedia.org/wiki/Index_notation)\" ($A_{ij}, x_i$). We will often switch between these notations, sometimes even within a single problem/derivation, so you should be comfortable with both.\n",
    "\n",
    "If you are struggling with index notation [this post](https://math.stackexchange.com/questions/2063241/matrix-multiplication-notation) may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is polynomial regression a \"linear\" or \"nonlinear\" model? Is it \"parametric\" or \"nonparametric\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "You should already be famililar with linear algebra, but we will briefly review the basics and show how it works in `numpy` by covering the following:\n",
    "\n",
    "Formulating your code as matrix-matrix and matrix-vector operations in Numpy will make it much more efficient. We will briefly cover syntax for:\n",
    "\n",
    "* scalar*vector\n",
    "* scalar*matrix\n",
    "* matrix*vector\n",
    "* matrix*matrix\n",
    "* inverse\n",
    "* solve Ax=b\n",
    "* eigendecomposition\n",
    "\n",
    "`numpy` notes:\n",
    "\n",
    "* reshaping and resizing arrays\n",
    "* boolean and comparison operators on arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scalar-array operations\n",
    "\n",
    "We can use the usual arithmetic operators to multiply, add, subtract, and divide arrays with scalar numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v1 = np.arange(0, 5)\n",
    "print(v1)\n",
    "print('-'*10)\n",
    "print(v1*2)\n",
    "print('-'*10)\n",
    "print(v1+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Same goes for matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "M = np.random.rand(2,2)\n",
    "print(M)\n",
    "print('-'*10)\n",
    "print(M*2)\n",
    "print('-'*10)\n",
    "print(M+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Element-wise array-array operations\n",
    "\n",
    "When we add, subtract, multiply and divide arrays with each other, the default behaviour is **element-wise** operations. This is different from Matlab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "v1 = np.arange(2,6)\n",
    "print(v1)\n",
    "print(v1*v1)\n",
    "print(v1/v1)\n",
    "\n",
    "print('-'*10)\n",
    "\n",
    "M = np.array([[1,2],[3,4]])\n",
    "print(M)\n",
    "print(M*M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix algebra\n",
    "\n",
    "What about matrix mutiplication?\n",
    "\n",
    "* use the `dot` function (recommended)\n",
    "* use the `matrix` class (`+`, `*`, `-` use matrix algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.eye(3,3)\n",
    "v = np.array([1,2,3])\n",
    "print(np.dot(A,v))\n",
    "print(np.dot(A,A))\n",
    "print(np.dot(v,v))\n",
    "\n",
    "A = np.matrix(A)\n",
    "v = np.matrix(v)\n",
    "print(A*v.T)\n",
    "print(A*A)\n",
    "print(v*v.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common matrix operations\n",
    "\n",
    "We can easily calculate the inverse and determinant using `inv` and `det`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[-1,2],[3,-1]])\n",
    "print(A)\n",
    "print(np.linalg.inv(A))\n",
    "print(np.linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear equation systems\n",
    "\n",
    "Linear equation systems on the matrix form\n",
    "\n",
    "$\\underline{\\underline{A}} \\vec{x} = \\vec{b}$\n",
    "\n",
    "where $A$ is a matrix and $x,b$ are vectors can be solved like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.linalg  import solve\n",
    "\n",
    "N = 3\n",
    "A = np.random.rand(N,N)\n",
    "b = np.random.rand(N)\n",
    "\n",
    "x = solve(A, b)\n",
    "\n",
    "print(x)\n",
    "# check\n",
    "np.dot(A, x) - b\n",
    "np.isclose(np.dot(A,x),b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same with\n",
    "\n",
    "$\\underline{\\underline{A}} \\underline{\\underline{X}} = \\underline{\\underline{B}}$\n",
    "\n",
    "where $A, B, X$ are matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(N,N)\n",
    "B = np.random.rand(N,N)\n",
    "\n",
    "X = solve(A, B)\n",
    "# check\n",
    "np.isclose(np.dot(A, X), B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Convince yourself that $\\underline{\\underline{A}} \\underline{\\underline{X}} = \\underline{\\underline{B}}$ is equivalent to solving `N` independent systems of the form $\\underline{\\underline{A}} \\vec{x} = \\vec{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and eigenvectors\n",
    "\n",
    "The eigenvalue problem for a matrix $\\underline{\\underline{A}}$:\n",
    "\n",
    "$\\underline{\\underline{A}} v_n = \\lambda_n v_n$\n",
    "\n",
    "where $v_n$ is the $n$th eigenvector and $\\lambda_n$ is the $n$th eigenvalue.\n",
    "\n",
    "To calculate eigenvalues of a matrix, use the `eigvals` function, and for calculating both eigenvalues and eigenvectors, use the function `eig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvals, eig\n",
    "evals = eigvals(A)\n",
    "print(evals)\n",
    "\n",
    "evals, evecs = eig(A)\n",
    "print(evals)\n",
    "print(evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors corresponding to the $n$th eigenvalue (stored in evals[n]) is the $n$th column in evecs, i.e., evecs[:,n]. To verify this, let's try mutiplying eigenvectors with the matrix and compare to the product of the eigenvector and the eigenvalue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "\n",
    "np.dot(A, evecs[:,n]) - evals[n] * evecs[:,n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Eigendecompositions are very common in machine learning and data analysis, so you should brush up on this if you have forgotten what it means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numpy notes:\n",
    "\n",
    "### Reshaping and resizing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shape of an Numpy array can be modified without copying the underlaying data, which makes it a fast operation even for large arrays. There are rules that govern how this reshaping takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "R = np.random.rand(3,3,3)\n",
    "print(R.shape)\n",
    "n,m,p = R.shape\n",
    "Q = R.reshape((n, m*p))\n",
    "print(Q.shape)\n",
    "F = R.flatten() #the \"flatten\" function turns the whole array into a vector\n",
    "print(F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two common pitfalls in reshaping arrays:\n",
    "\n",
    "* Reshaping rules do not behave as expected\n",
    "* Reshaping provides a different \"view\" of the data, but **does not copy it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(R[0,0,0])\n",
    "print(F[0])\n",
    "print(R[0,1,0])\n",
    "print(F[1])\n",
    "print(F[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(R[0,0,0])\n",
    "Q[0] = 10\n",
    "print(R[0,0,0]) #resize does not copy the data\n",
    "F[0] = 6\n",
    "print(R[0,0,0]) #flatten makes copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Making a \"deep copy\"\n",
    "\n",
    "If you really want a copy of an array, use the `np.copy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "print(A)\n",
    "B = A\n",
    "B[0,0] = 10\n",
    "print(A)\n",
    "Acopy = np.copy(A)\n",
    "Acopy[1,1] = 6\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization (finding minima or maxima of a function) is a large field in mathematics, and optimization of complicated functions or in many variables can be rather involved. Here we will only look at a few very simple cases. For a more detailed introduction to optimization with SciPy see: http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html\n",
    "\n",
    "We can typically achieve good results using `scipy`'s built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4*x**3 + (x-2)**2 + x**4\n",
    "\n",
    "fig, ax  = plt.subplots()\n",
    "x = np.linspace(-5, 3, 100)\n",
    "ax.plot(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of optimizers available. We will use the common BFGS and CG optimizers here, but you can read more in the [documentation](https://docs.scipy.org/doc/scipy/reference/optimize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.optimize  import minimize\n",
    "\n",
    "x_min = minimize(f, 0.5, method='BFGS')\n",
    "x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related problem is solving an equation, which can be achieved with the `fsolve` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return np.sin(3*x)*(1/x)\n",
    "\n",
    "fig, ax  = plt.subplots(figsize=(10,4))\n",
    "x = np.linspace(1, 10, 100)\n",
    "ax.plot(x, g(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "ans = fsolve(g, 0)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype algorithms\n",
    "\n",
    "We will introduce a prototype for each class of algorithm with a derivation and/or custom implementation. These algorithms will be revisited later in the course (along with others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear regression\n",
    "\n",
    "Generalized linear regression has two basic parts:\n",
    "\n",
    "1) Generate non-linear transforms of the input data to form new features (optional)\n",
    "\n",
    "2) Perform multi-linear least-squares regression between the features and the output data.\n",
    "\n",
    "Polynomial regression is an example of generalized linear regression where the \"features\" generated in step 1 are polynomials of the original input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    return np.array([x**k for k in range(0,N+1)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write our own multilinear regression function instead of relying on `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilinear_regression(X,y):\n",
    "    ## Derive expression\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors (classification)\n",
    "\n",
    "A simple prototype classification method is the k-nearest neighbors algorithm. This operates on a principle that is very easy to understand: democracy.\n",
    "\n",
    "The class of a point is determined by letting its k-nearest neighbors \"vote\" on which class it should be in. The point is assigned to whichever class has the most votes. In the case of a tie, `k` is decreased by 1 until the tie is broken.\n",
    "\n",
    "The advantage of democracy is that it is \"nonlinear\" - we can distinguish classes with very complex structures.\n",
    "\n",
    "We need 3 functions to implement kNN:\n",
    "\n",
    "* distance metric - calculate the distance between 2 points. We will use the Euclidean distance.\n",
    "* get neighbors - find the k points nearest to a given point.\n",
    "* assign class - poll the neighbors to assign the point to a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    # we will use the numpy 2-norm to calculate Euclidean distance:\n",
    "    return np.linalg.norm(x1-x2, 2) #<- the 2 is optional here since 2 is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_idxs(x, x_list, k):\n",
    "    dist_pairs = []\n",
    "    for i,xi in enumerate(x_list):\n",
    "        dist = distance(x, xi)\n",
    "        dist_pairs.append([dist, i, xi]) #<- gives us the distance for each point\n",
    "    dist_pairs.sort() #<- sort by distance\n",
    "    k_dists = dist_pairs[:k] #<- take the k closest points\n",
    "    kNN_idxs = [i for di, i, xi in k_dists] #<- we will get the indices of neighbors instead of the point itself.\n",
    "    return kNN_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def assign_class(x, x_list, y_list, k): #<- now we need to know the responses\n",
    "    neighbors = get_neighbor_idxs(x, x_list, k)\n",
    "    y_list = list(y_list) #<- this ensures that indexing works properly if y_list is a `pandas` object.\n",
    "    votes = [y_list[i] for i in neighbors]\n",
    "    assignment = mode(votes)[0][0] #<- we won't deal with ties for this simple implementation\n",
    "    return assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can \"wrap\" all of these functions into a single function that predicts the class of an array of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(X, k, X_train, y_train):\n",
    "    y_out = []\n",
    "    for xi in X:\n",
    "        y_out.append(assign_class(xi, X_train, y_train, k))\n",
    "    y_out = np.array(y_out)\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works on our example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = example['x1'] #<- both of the X columns are now independent variables\n",
    "x2 = example['x2']\n",
    "c = example['class'] #<- the \"class\" column is now the dependent variable\n",
    "ax.scatter(x1,x2,c=c)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = example[['x1','x2']]\n",
    "X = X_df.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = kNN(X,3,X,c)\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "axes[0].scatter(x1, x2, c=c)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].scatter(x1, x2, c=y_model)\n",
    "axes[1].set_title('kNN prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the points on the right being mis-classified? How can we fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_neighbors(X, X_train, k):\n",
    "    ## helper function to visualize neighbors\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0], X[:,1], color='k', alpha=0.2)\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], color='r', alpha=0.2)\n",
    "    for xi in X:\n",
    "        neighbors = get_neighbor_idxs(xi,X_train,k)\n",
    "        for nj in neighbors:\n",
    "            xj = X_train[nj]\n",
    "            ax.plot([xi[0],xj[0]],[xi[1],xj[1]],ls='-',color='b', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_neighbors(X, X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "visualize_neighbors(X_scaled, X_scaled, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = kNN(X_scaled,3,X_scaled,c)\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "axes[0].scatter(X_scaled[:,0], X_scaled[:,1], c=c)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].scatter(X_scaled[:,0], X_scaled[:,1], c=y_model)\n",
    "axes[1].set_title('kNN prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data is like \"non-dimensionalizing\" or normalizing for different units. This is often critical to ensure that certain variables are not weighted more than others.\n",
    "\n",
    "Statistical methods don't know about physical units, so we can normalize or \"scale\" features to aid in comparison:\n",
    "\n",
    "* rescaling: 0 = min, 1 = max\n",
    "* mean scaling: 0 = mean, 1 = max, -1 = min\n",
    "* **standardization: 0 = mean, 1 = standard deviation**\n",
    "* unit vector: the length of each multi-dimensional vector is 1\n",
    "\n",
    "See the [scikit-learn documentation](http://scikit-learn.org/stable/modules/preprocessing.html) for more examples and discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN models are non-parameteric, and can easily be \"overfit\".\n",
    "\n",
    "In both examples above setting `k=1` will recover the exact correct solution. This is because all the data is included in the training set - the model hasn't \"learned\" anything, it just memorized the answers. We can avoid this by creating a testing and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, c, test_size=0.5)\n",
    "\n",
    "visualize_neighbors(X_scaled, X_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = kNN(X_scaled,3,X_train,y_train)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "axes[0].scatter(X_scaled[:,0], X_scaled[:,1], c=c)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].scatter(X_scaled[:,0], X_scaled[:,1], c=y_model)\n",
    "axes[1].set_title('kNN prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the number of \"parameters\" is equal to the number of training data points multiplied by (`X.shape[-1] + y.shape[-1]`). This means that the `test_size` and the method by which the training set is selected (e.g. `random`) are effectively hyperparameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN advantages and disadvantages\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Simple to understand/implement\n",
    "* Intuitive\n",
    "* Highly non-linear class boundaries\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Slow for large training sets and/or high dimensions\n",
    "* Difficult to interpret in high dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k means (clustering)\n",
    "\n",
    "The k-means algorithm is the simplest and most intuitive clustering algorithm. It performs remarkably well under a number of assumptions:\n",
    "\n",
    "* Number of clusters are known\n",
    "* Clusters are roughly spherical\n",
    "* Clusters are separated by linear boundaries\n",
    "\n",
    "Even if these assumptions are violated, it often works anyway, especially in high dimensions (the \"blessing\" of dimensionality).\n",
    "\n",
    "The k-means algorithm works using the principal of **expectation-maximization**. This is an iterative type of algorithm that contains two basic steps:\n",
    "\n",
    "* Expectation: Assign points based on some \"expectation\" metric.\n",
    "* Maximization: Revise expectations based on maximizing a fitness metric.\n",
    "\n",
    "In the case of k-means we:\n",
    "\n",
    "* Expect that points close to the center of a cluster belong to that cluster\n",
    "* Maximize the proximity of points to the center of a cluster by moving the center\n",
    "\n",
    "This process is interated until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(pt1, pt2):\n",
    "    \"Euclidean distance between two points\"\n",
    "    #note that this can also be performed with np.linalg.norm(pt1-pt2)\n",
    "    return np.sqrt(sum([(xi-yi)**2 for xi, yi in zip(pt1, pt2)]))\n",
    "\n",
    "def expected_assignment(pt, cluster_centers):\n",
    "    dists = [dist(pt,ci) for ci in cluster_centers] #<- find distance to each center\n",
    "    min_index = dists.index(min(dists)) #<- find the index (cluster) with the minimum dist\n",
    "    return min_index\n",
    "\n",
    "def new_centers(cluster_points, centers):\n",
    "    centers = list(centers)\n",
    "    for i,ci in enumerate(cluster_points):\n",
    "        if ci != []:\n",
    "            centers[i] = np.mean(ci, axis=0)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need an \"initial guess\" of cluster centers and we can run the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cluster_centers = ([-0.5,0], [0.5,0])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_scaled[:,0], X_scaled[:,1], color='k', alpha=0.2)\n",
    "colors = {0:'r', 1:'g'}\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    ax.plot(ci[0], ci[1], marker='*', markersize='12', color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Run the cell below repeatedly to see how the algorithm converges. Re-run the cell above to re-start the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Plot old centers\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    ax.plot(ci[0], ci[1], marker='+', markersize='12', color=colors[i])\n",
    "    \n",
    "# Which cluster do we \"expect\" each point to belong to?\n",
    "clusters = [[],[],[]]\n",
    "for pt in X_scaled:\n",
    "    cluster_idx = expected_assignment(pt, cluster_centers)\n",
    "    clusters[cluster_idx].append(pt)\n",
    "    \n",
    "# What centers best represent these new assignments?\n",
    "cluster_centers = new_centers(clusters, cluster_centers)\n",
    "\n",
    "# Plot new assignments\n",
    "for i, ci in enumerate(clusters):\n",
    "    for pt in ci:\n",
    "        ax.plot(pt[0], pt[1], marker='o', color=colors[i], alpha=0.2)\n",
    "        \n",
    "# Plot new centers\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    print(ci)\n",
    "    ax.plot(ci[0], ci[1], marker='*', markersize='12', color=colors[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we run the algorithm repeatedly we can see that the centers converge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i, ci in enumerate(cluster_centers):\n",
    "    print('Cluster {} center: {}'.format(i,ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means summary:\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Fast\n",
    "* Simple\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Need to know the number of clusters\n",
    "* Need to have a decent initial guess\n",
    "* Boundaries are linear and distinct (no probability!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (dimensional reduction)\n",
    "\n",
    "Principal component analysis, or PCA, is a very common and useful algorithm for dimensional reduction. PCA identifies the linear combinations of input variables that 1) explain the most variance and 2) are orthogonal. It also provides the projection of each data point onto this new principal component space. This is like rotating the coordinate system of the data so that the axes are defined by the variance of the data.\n",
    "\n",
    "PCA of a dataset yields:\n",
    "\n",
    "* Principal component vectors\n",
    "* Principal component values\n",
    "* Principal component coefficients of data projected onto PC vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is based on the covariance structure of a set of variables. It involves several steps:\n",
    "\n",
    " 1) Scale the data to remove units (technically optional, but nearly always necessary)\n",
    "\n",
    " 2) Compute the covariance matrix:\n",
    "\n",
    "  * Center the data\n",
    "  \n",
    "  * Multiply the data by its transpose\n",
    "     \n",
    "3) Compute the eigenvalues/eigenvectors of the covariance matric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by looking at our example data. We have already scaled it, and can think of the covariance matrix as the correlation between the different variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize = (8,8))\n",
    "\n",
    "for i in range(X_scaled.shape[1]):\n",
    "    for j in range(X_scaled.shape[1]):\n",
    "        xi = X_scaled[:,i]\n",
    "        xj = X_scaled[:,j]\n",
    "        axes[i,j].scatter(xi, xj)\n",
    "        m,b = np.polyfit(xi,xj,1)\n",
    "        print(m)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations in features represent redundancy in information. This is called \"co-variance\", and it is quantified by the covariance matrix:\n",
    "\n",
    "$\\Sigma_{jk} = \\frac{1}{n-1}\\sum_{i=1}^{N}\\left(  x_{ij}-\\bar{x}_j \\right)  \\left( x_{ik}-\\bar{x}_k \\right)$\n",
    "\n",
    "Or, in vector notation:\n",
    "\n",
    "$\\Sigma = \\frac{1}{n-1} \\left( (\\mathbf{X} - \\mathbf{\\bar{x}})^T\\;(\\mathbf{X} - \\mathbf{\\bar{x}}) \\right)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_scaled.shape[0]\n",
    "covar = (1/(N-1))*np.dot((X_scaled - X_scaled.mean(axis=0)).T, (X_scaled - X_scaled.mean(axis=0)))\n",
    "print(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more easily accomplished with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covnp = np.cov(X_scaled.T)\n",
    "print(covnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this to the plots above and see that the covariance value is similar to the slope of a linear regression line between the variables.\n",
    "\n",
    "Note what happens if we use the unscaled version of `X` instead. Variances (and covariances) have units! If your features do not all have the same unit then it is critical to re-scale them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components are computed by taking the eigenvalues and eigenvectors of the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCvals, PCvecs = eig(covar)\n",
    "print(PCvals)\n",
    "print(PCvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can super-impose these on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(X_scaled[:,0],X_scaled[:,1])\n",
    "ax.set_xlabel('x1 [unitless]')\n",
    "ax.set_ylabel('x2 [unitless]')\n",
    "\n",
    "PCvec1 = PCvecs[:,0] #<- remember that eigenvectors are in the columns!\n",
    "PCvec2 = PCvecs[:,1]\n",
    "\n",
    "ax.plot([0, PCvec1[0]],[0,PCvec1[1]], ls='-', color='b', alpha=0.5) #<- plot eigenvectors\n",
    "ax.plot([0, PCvec2[0]],[0,PCvec2[1]], ls='-', color='b', alpha=0.5)\n",
    "\n",
    "ax.plot([0, PCvec1[0]*PCvals[0]],[0,PCvec1[1]*PCvals[0]], ls='-', color='r', alpha=0.5) #<- plot eigenvectors scaled by eigenvalues\n",
    "ax.plot([0, PCvec2[0]*PCvals[1]],[0,PCvec2[1]*PCvals[1]], ls='-', color='r', alpha=0.5)\n",
    "ax.set_xlim([-2,2])\n",
    "ax.set_ylim([-2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PC values are a multi-dimensional generalization of variance. The variance of the data along each principal component is equal to the PC value (standard deviation is equal to the square root of the PC value). This means we can compute the **explained variance** for each principal component axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance = np.sum(PCvals)\n",
    "explained_variance = PCvals/total_variance\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the first principal component explains ~85% of the variance of the entire data set, and the second principal component explains the additional 15%.\n",
    "\n",
    "So far we have transformed the dimensions, but we have not reduced them since the data is still 2 dimensional. Reducing the data involves dropping some of the PC vectors and \"projecting\" the original data onto this subspace. In this case, we will just drop the second one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1d = np.dot(X_scaled, PCvec1.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(X_1d)\n",
    "print(X_1d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this against the `scikit-learn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCA_model = PCA(n_components=2)\n",
    "PCA_model.fit(X_scaled) #<- the \"fit\" method finds the PC vectors\n",
    "print(PCA_model.components_) #<- this is the syntax for PC vectors\n",
    "print(PCA_model.explained_variance_) #<- this is the syntax for PC values\n",
    "print(PCA_model.explained_variance_ratio_) #<- this is the syntax for explained variance of each component\n",
    "\n",
    "X_PCA = PCA_model.transform(X_scaled) #<- the \"tranform\" method projects the data onto the PC vectors\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(X_PCA[:,0])\n",
    "print(X_PCA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the **sign of PC vectors is arbitrary**. This means that PCA projections can vary by a +/- depending on the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isclose(X_PCA[:,0], -X_1d).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Summary\n",
    "\n",
    "This example may not seem very useful since the original data is only 2 dimensions, but consider a dataset with 10 features, or 100, or even 10000. PCA identifies which features contain redundant information and provides a route to systematically reduce the number of features in the model. Analyzing the data after projection onto the PC space also allows visualization of the data to look for patterns, etc.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Relatively fast\n",
    "\n",
    "* Easy to visualize/interpret\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Only linear\n",
    "\n",
    "* Maximum variance may not be a good metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "* Machine-learning models map inputs and their **features** to outputs. Their **parameters** control the numerical behavior and the **hyperparameters** control the model structure.\n",
    "\n",
    "* Model **parameters** are determined by optimizing (minimizing) a loss/objective function. This optimization can be numerical or analytical, and typically relies on linear algebra (linear models) and gradient-based optimization routines (non-linear models).\n",
    "\n",
    "* Model **hyperparameters** are determined by assessing the tradeoff between model complexity and accuracy to achieve generalizable models. Complexity is controlled by the hyper-parameters, and **cross-validation** is used to assess the generalizability of the model's accuracy.\n",
    "\n",
    "* Machine learning models can be classified as **supervised** (trained to known dependent variable data) and **unsupervised** (no training data is provided). Supervised models include **regression** (continuous output variable) and **classification** (discrete/ordinal output variable). Unsupervised models include **clustering** (identification of similar groups) and **dimensional reduction** (projection onto fewer independent variables).\n",
    "\n",
    "* Machine learning models can be **linear** (single unique solution) or **non-linear** (multiple possible solutions), and **parametric** (fixed number of model parameters) or **non-parameteric** (number of parameters varies with amount of input data).\n",
    "\n",
    "* Prototype models provide insight into the basics of machine learning. You should understand the basics of generalized linear regression, k-nearest neighbor classification, k-means clustering, and principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading:\n",
    "\n",
    "* Watt Machine Learining Refined [Ch1](http://docs.wixstatic.com/ugd/f09e45_ed4e95c1bc8c4f01a3e9858f9a74a5de.pdf) and [Ch2](http://docs.wixstatic.com/ugd/f09e45_2c741285adf14e5aa9fd0696128a1275.pdf) (basics and numerical optimization)\n",
    "* [Johansson Numpy tutorial](http://github.com/jrjohansson/scientific-python-lectures)\n",
    "* Hastie Ch. 3 - linear regression\n",
    "* [Python data science handbook linear regression](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb)\n",
    "* [Sebastian Rauschka regression discussion](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)\n",
    "* Hastie Ch. 13 (pg. 459) - k means and kNN\n",
    "* [Python kNN implementation](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)\n",
    "* [Python Data Science Handbook: k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "* Hastie Ch. 14 (page 534) - PCA\n",
    "* [PythonMachineLearning notebook](https://github.com/tirthajyoti/PythonMachineLearning/blob/master/Principal%20Component%20Analysis.ipynb)\n",
    "* [Sebastian Raushka PCA tutorial](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "* [Scikit-Learn PCA documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
