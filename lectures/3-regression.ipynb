{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"images/underfitting_overfitting.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will discuss basic types of regression algorithms and explore strategies for optimizing and improving regression models. The focus will be on linear and pseudo/generalized linear problems, with some limited examples of non-linear regression.\n",
    "\n",
    "* Regression basics\n",
    "    - Problem statement for regression\n",
    "    - Properties of error distributions\n",
    "    - Accuracy metrics\n",
    "\n",
    "* Improving models by adding features\n",
    "    - Generating non-linear features\n",
    "    - Colinearity of features\n",
    "    - Principal Component Regression and Partial Least Squares\n",
    "    - Regularization of coefficients\n",
    "    - Kernel ridge regression\n",
    "\n",
    "* Improving performance by changing models\n",
    "    - Non-linear regression\n",
    "    - Neural networks\n",
    "    - kNN regression\n",
    "\n",
    "* Quantifying uncertainty\n",
    "    - Standard deviation of error\n",
    "    - Ensembles from re-sampling\n",
    "    - Gaussian process regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression problem statement\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}) + \\vec{\\epsilon}$\n",
    "\n",
    "where the output, $\\vec{y}$, is a vector (or scalar) of continuous, real values, $f$ is the model, $\\vec{x}$ is a vector (or scalar) of input features, and $\\vec{epsilon}$ is an error between the real and predicted values. It is common to write the predicted values as $\\vec{\\hat{y}}$:\n",
    "\n",
    "$\\vec{\\hat{y}} = f(\\vec{x})$\n",
    "\n",
    "such that $\\vec{\\epsilon}$ = $\\vec{y} - \\vec{\\hat{y}}$.\n",
    "\n",
    "The goal of regression is to minimize the error, $\\vec{\\epsilon}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of error distributions\n",
    "\n",
    "The distribution of errors, or \"residuals\" contains a lot of information about the performance of a model. There are two common classes of error distributions:\n",
    "\n",
    "* Normal (or Gaussian) vs. Non-normal error distributions\n",
    "\n",
    "\n",
    "Determined by how well the error distribution is described by the Gaussian probability distribution:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/normal_distribution.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "* Homoskedastic vs. Heteroskedastic error distributions\n",
    "\n",
    "Homoskedastic error distributions are *constant in the independent variables (features)*, while heteroskedastic errors vary across the input space:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/heteroskedastic.gif\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "Least-squares regression and many other regression models *assume* that errors are normally-distributed and homoskedastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics\n",
    "\n",
    "It is important to consider the context of a regression model and choose accuracy metrics that are relevant to its application. There are several common options:\n",
    "\n",
    "* #### Mean absolute error (MAE)\n",
    "\n",
    "$MAE = \\frac{1}{N} \\sum_{i=0}^N |y_i - \\hat{y}_i|$\n",
    "\n",
    "* #### Root-mean-sqaured error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=0}^N (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "MAE and RMSE are very similar. Both have units of the dependent variable ($y$), and can vary from 0 to $\\infty$ with lower values being better. MAE is less affected by outliers and sample size, but it is always lower than RMSE, so it is a less conservative estimate. MAE and RMSE are related by the inequalities:\n",
    "\n",
    "$MAE \\leq RMSE \\leq MAE \\times \\sqrt{N}$\n",
    "\n",
    "* #### $R^2$ value\n",
    "\n",
    "The $R^2$ metric is very common in regression models, and is the default \"score\" in `scikit-learn`. $R^2$ varies from 0-1, with higher values corresponding to better models. The $R^2$ value corresponds to the amount of variance in the independent variable that is explained by the model, and is defined as:\n",
    "\n",
    "$R^2 = \\frac{\\sum_{i=0}^N (y_i - \\bar{y})^2 - \\sum_{i=0}^N (y_i - \\hat{y})^2}{\\sum_{i=0}^N (y_i - \\bar{y})^2}$\n",
    "\n",
    "where $\\bar{y}$ is the mean of $y$. This is often written as:\n",
    "\n",
    "$R^2 = \\frac{SST - SSE}{SST}$\n",
    "\n",
    "where $SST = \\sum_{i=0}^N (y_i - \\bar{y})^2$ and $SSE = \\sum_{i=0}^N (y_i - \\hat{y})^2$\n",
    "\n",
    "* #### Parity plots\n",
    "\n",
    "Plotting $y$ vs. $\\hat{y}$ provides a visual analysis of the error.\n",
    "\n",
    "* #### Maximum error\n",
    "\n",
    "Sometimes it is useful to assess the maximum error of a model, $max(\\epsilon_i)$. This is useful to assess a worst-case scenario, and provides a conservative estimate of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How could you replicate the behavior of a model with $R^2 =0$ using a single parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Anscomb's Quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\n",
    "y1 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "y2 = np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])\n",
    "x4 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(x,y):    \n",
    "    y_bar = np.mean(y)\n",
    "    y_std = np.std(x)\n",
    "    m, b = np.polyfit(x,y,deg=1)\n",
    "    SST = sum((y - y_bar)**2)\n",
    "    SSE = sum((y - (m*x+b))**2)\n",
    "    R2 = (SST - SSE)/SST\n",
    "    return y_bar, y_std, m, b, R2\n",
    "\n",
    "stats1 = calc_stats(x,y1)\n",
    "print(\"Dataset 1: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats1))\n",
    "stats2 = calc_stats(x,y2)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats2))\n",
    "stats3 = calc_stats(x,y3)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats3))\n",
    "stats4 = calc_stats(x4,y4)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats4))\n",
    "avg, std, m, b, r2 = stats1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the statistics are identical for these datasets, and the $R^2$ implies that linear regression describes the same amount of variance in all cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "yhat = m*x + b\n",
    "axes[0].scatter(x,y1)\n",
    "axes[0].plot(x, yhat, ls='-', color='k')\n",
    "axes[1].scatter(x,y2)\n",
    "axes[1].plot(x, yhat, ls='-', color='k')\n",
    "axes[2].scatter(x,y3)\n",
    "axes[2].plot(x, yhat, ls='-', color='k')\n",
    "axes[3].scatter(x4,y4)\n",
    "axes[3].plot(x4, m*x4 + b, ls='-', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parity plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "yhat = m*x + b\n",
    "axes[0].scatter(y1, yhat)\n",
    "axes[0].plot(y1, y1, ls='--', color='k')\n",
    "axes[1].scatter(y2, yhat)\n",
    "axes[1].plot(y2, y2, ls='--', color='k')\n",
    "axes[2].scatter(y3, yhat)\n",
    "axes[2].plot(y3, y3, ls='--', color='k')\n",
    "axes[3].scatter(y4, m*x4 + b)\n",
    "axes[3].plot(y4, y4, ls='--', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the error distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "axes[0].hist(y1 - yhat,density=1)\n",
    "axes[1].hist(y2 - yhat,density=1)\n",
    "axes[2].hist(y3 - yhat,density=1)\n",
    "axes[3].hist(y4 - (m*x4 + b),density=1)\n",
    "\n",
    "mu = 0\n",
    "sigma = np.std(y1 - yhat)\n",
    "x_resid = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x_resid,norm.pdf(x_resid, mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Based on these sets of plots, which dataset is best described by linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to improve performance\n",
    "\n",
    "One strategy to improve regression models is to give them more information in the form of independent variables, or \"features\". Even without changing the underlying model, this can lead to considerable improvements in accuracy. We will consider two strategies in improving the features of regression models:\n",
    "\n",
    "* Generating non-linear responses from \"linear\" models by creating *derived* features\n",
    "\n",
    "* Adding new information about samples and analyzing the property of these independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating non-linear derived features for least-squared regression\n",
    "\n",
    "We have already seen a classic example of this in previous lectures where we considered polynomial regression:\n",
    "\n",
    "$y_j = b + m*x_j + p*x_j^2 + ...$\n",
    "\n",
    "$y_j = \\beta_0 + \\beta_1 x_j + \\beta_2 x_j^2 + \\beta_3 x_j^3 + \\beta_4 x_j^4 + ...$\n",
    "\n",
    "$y_j = \\sum_i^N \\beta_i x_j^i$\n",
    "\n",
    "We can think of this as \"deriving\" non-linear polynomial features from $x$, then using these new features in a linear least-squares regression model.\n",
    "\n",
    "$x_j^{(k)} = x_j^k$\n",
    "\n",
    "$y_j = \\sum_k^N \\beta_k x_j^{(k)} = \\sum_k^N \\beta_k x_{jk} \\Rightarrow \\vec{y} = \\underline{\\underline{X}}\\vec{\\beta}$\n",
    "\n",
    "However, there is no reason that we have to use polynomials. To demonstrate this we will use the following three example datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/three_regression_examples.csv')\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(df['0'], df['1'])\n",
    "axes[1].scatter(df['0'], df['2'])\n",
    "axes[2].scatter(df['0'], df['3'])\n",
    "\n",
    "x = df['0'].values\n",
    "y1 = df['1'].values\n",
    "y2 = df['2'].values\n",
    "y3 = df['3'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we will consider two types of \"derived features\":\n",
    "\n",
    "* polynomials\n",
    "* Gaussian distributions\n",
    "\n",
    "We have already seen the \"polynomial features\" in a prior lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    # One-liner uses \"list comprehension\" to iterate through range 0 - N (note N+1 since range function is not inclusive)\n",
    "    # The input, x, is raised to the power of N for each value of N\n",
    "    # The result is converted to an array and transposed so that columns correspond to features and rows correspond to data points (individual x values)\n",
    "    return np.array([x**k for k in range(0,N)]).T\n",
    "\n",
    "X_poly = polynomial_features(x,2)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_poly.shape[1]):\n",
    "    ax.plot(x,X_poly[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is to expand the function in terms of Gaussian (normal) distributions with evenly-spaced means and a fixed standard deviation:\n",
    "\n",
    "$x^{(k)} = \\exp{\\left(-\\frac{(x-x_k)^2}{2\\sigma^2}\\right)}$\n",
    "\n",
    "where $x_k$ are the means and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_features(x, N , sigma = 1):\n",
    "    # x is a vector\n",
    "    # sigma is the standard deviation\n",
    "    xk_vec = np.linspace(min(x), max(x), N)\n",
    "    features = []\n",
    "    for xk in xk_vec:\n",
    "        features.append(np.exp(-((x - xk)**2/(2*sigma**2))))\n",
    "    return np.array(features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gauss = gaussian_features(x,6)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_gauss.shape[1]):\n",
    "    ax.plot(x,X_gauss[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have several strategies for creating non-linear derived features from the input. Let's see how they work on the example datasets. We will use our linear regression functions from last lecture. You should know how to derive these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.linalg  import solve\n",
    "def multilinear_regression(X,y):\n",
    "    ## Derive expression\n",
    "    A = np.dot(X.T, X)\n",
    "    b = np.dot(X.T, y)\n",
    "    beta = solve(A,b)\n",
    "    return beta\n",
    "\n",
    "def multilinear_prediction(X,beta):\n",
    "    return np.dot(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15 #<- Number of features\n",
    "\n",
    "# Generate features\n",
    "X_poly = polynomial_features(x,N)\n",
    "X_gauss = gaussian_features(x,N)\n",
    "\n",
    "#plot the original data\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(x, y1)\n",
    "axes[1].scatter(x, y2)\n",
    "axes[2].scatter(x, y3)\n",
    "\n",
    "# Iterate through feature options\n",
    "X_features = [X_poly, X_gauss]\n",
    "feature_functions = [polynomial_features,  gaussian_features]\n",
    "colors = ['r','b']\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "\n",
    "for yi, ax in zip([y1, y2, y3], axes):\n",
    "    for X, f_features, color in zip(X_features,feature_functions,colors):#<- useful way to iterate through synced lists\n",
    "        beta = multilinear_regression(X, yi)\n",
    "        X_dense = f_features(x_dense,N)\n",
    "        yhat = multilinear_prediction(X_dense,beta)\n",
    "        ax.plot(x_dense, yhat, color=color, ls='-')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Does N > 10 provide a reliable model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colinearity of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the properties of the features that we generated by investigating their covariance. We can get an intuitive feel for this by plotting them against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(N,N, figsize = (15,15))\n",
    "X = X_poly\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xi = X[:,i]\n",
    "        xj = X[:,j]\n",
    "        axes[i,j].scatter(xi, xj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a clearer picture by looking at the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X[1:,1:] - X[1:,1:].mean(axis=0))/X[1:,1:].std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial features are highly correlated. This causes numerical issues for multi-linear regression since the system of equations becomes nearly redundant. This is the linear algebra equivalent of (almost) dividing by zero. Let's take a look at the coefficient values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "beta = multilinear_regression(X, y3)\n",
    "ax.plot(range(0,N),beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two basic strategies for \"stabilizing\" the model:\n",
    "\n",
    "1) Ortogonalization of feature vectors\n",
    "\n",
    "2) Regularization of coefficients through the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonalizing coefficients\n",
    "\n",
    "Now we know that co-linear features cause unpredictable behavior in the model, so one strategy is to ensure that features are not co-linear. For derived features, we can enforce this when we create the feature set. A classic example is \"Legendre Polynomials\" which are an orthogonal polynomial basis set:\n",
    "\n",
    "$L_n(x) = \\frac{1}{2^n n!}\\frac{\\mathrm{d}^n}{\\mathrm{d}x^n}(x^2 - 1)^n $\n",
    "\n",
    "$L_0(x) = 1$\n",
    "\n",
    "$L_1(x) = x$\n",
    "\n",
    "$L_2(x) = \\frac{1}{2} (3x^2 -1)$\n",
    "\n",
    "$L_3(x) = \\frac{1}{2} (5x^3 - 3x)$\n",
    "\n",
    "...\n",
    "\n",
    "Luckily these are implemented in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.legendre import legval\n",
    "\n",
    "def legendre_features(x, N):\n",
    "    features = []\n",
    "    for k in range(1,N+1):\n",
    "        alphas = np.zeros(k)\n",
    "        alphas[-1] = 1 #<- this gives us the kth legendre polynomial (see docs for legval)\n",
    "        x_normed = (x - min(x))/(max(x) - min(x))\n",
    "        x_normed = x_normed*2 - 1\n",
    "        features.append(legval(x_normed, alphas))\n",
    "    return np.array(features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the Legendre polynomials with regular polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "N = 6\n",
    "X_poly = polynomial_features(x,N)\n",
    "X_leg = legendre_features(x,N)\n",
    "\n",
    "for col in range(X_leg.shape[1]):\n",
    "    axes[0].plot(x,X_leg[:,col])\n",
    "\n",
    "for col in range(X_poly.shape[1]):\n",
    "    axes[1].plot(x,X_poly[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that they are orthogonal by checking the covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dense = np.linspace(min(x),max(x),100) #<- note that they will not be exactly orthogonal due to numerical issues, but as X gets more dense they become more orthogonal.\n",
    "X_leg = legendre_features(x_dense,N)\n",
    "cov_leg = np.cov(X_leg.T)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(cov_leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating orthogonal features is an elegant solution, but it isn't always practical. It is not always obvious how to determine the form of an orthogonal basis, and more importantly not all features are derived. Often features are \"observed\", not derived, so it is impossible to ensure that they are orthogonal.\n",
    "\n",
    "Fortunately, we have a solution. Remember that principal component analysis (PCA) gives us orthonormal eigenvectors of the covariance matrix. These eigenvectors can be used as features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA to create a set of orthogonal features from our Gaussian kernel basis set. We just need to remember that the principal component vectors are **eigenvectors of the covariance matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X_gauss - X_gauss.mean(axis=0))/X_gauss.std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "eigvals, eigvecs = np.linalg.eig(covar)\n",
    "print(eigvecs.shape)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can project the original feature space onto the eigenvector space using a dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gauss_PCA = np.dot(X_gauss, eigvecs)\n",
    "covar = np.cov(X_gauss_PCA.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our features are not only orthogonal, but they are also ordered by the amount of variance they explain. This provides a very convenient way to generate features for regression models. We can create a `PCA_features` function to do this for any given set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_features(X):\n",
    "    X_scaled = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "    covar = np.cov(X_scaled.T)\n",
    "    eigvals, eigvecs = np.linalg.eig(covar)\n",
    "    return np.dot(X, eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 #<- Number of features\n",
    "\n",
    "# Generate features\n",
    "X_poly = polynomial_features(x,N)\n",
    "X_leg = legendre_features(x,N)\n",
    "X_gauss = gaussian_features(x,N)\n",
    "\n",
    "X_gauss_full = gaussian_features(x,len(x)) #<- first create a lot of Gaussians\n",
    "X_gauss_PCA = PCA_features(X_gauss_full)[:,:N] #<- select the first N principal components\n",
    "\n",
    "\n",
    "#plot the original data\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(x, y1)\n",
    "axes[1].scatter(x, y2)\n",
    "axes[2].scatter(x, y3)\n",
    "\n",
    "# Iterate through feature options\n",
    "X_features = [X_poly, X_leg, X_gauss, X_gauss_PCA]\n",
    "colors = ['r','m','b','k']\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "\n",
    "for yi, ax in zip([y1, y2, y3], axes):\n",
    "    for X,  color in zip(X_features,colors):#<- useful way to iterate through synced lists\n",
    "        beta = multilinear_regression(X, yi)\n",
    "        yhat = multilinear_prediction(X,beta)\n",
    "        ax.plot(x, yhat, color=color, ls='-')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of features approaches the number of data points all models converge; however, the behavior at low N varies considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which of these choices of features is the *worst*? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Regression and Partial Least Squares\n",
    "\n",
    "Ultimately we want to not only numerically stabilize regression models, but also identify the simplest possible model (e.g. fewest number of features or parameters). We will explore two techniques for doing this:\n",
    "\n",
    "* Principal component regression (PCR) - determine principal component(s) that give the best fit.\n",
    "\n",
    "* Partial least squares (PLS) - determine the linear combinations of principal components that maximize covariance between inputs and outputs.\n",
    "\n",
    "We will look at a real data set of sensor data as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example [data set](https://archive.ics.uci.edu/ml/datasets/Air+Quality) consists of the raw output of metal oxide sensors for air pollutants in an Italian city. These metal oxide sensors are much cheaper than the standard sensors, but they are less accurate. The data set also includes measured concentrations from a calibrated sensor. The calibrated sensor will serve as the *ground truth*, and the goal is to determine the pollutant concentration based on the output of the metal oxide sensors. The units are mg/m$^3$ (CO), $\\mu$g/m$^3$ (hydrocarbons, NO2), and ppb (NOx).\n",
    "\n",
    "S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, [On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario](https://www.sciencedirect.com/science/article/pii/S0925400507007691?via%3Dihub), Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('datasets/pollution_sensors.xlsx')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's rename the columns to make life a little easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "naming_dict = {'CO(GT)':'true_CO', \n",
    "               'PT08.S1(CO)':'SnOx_CO', \n",
    "               'NMHC(GT)':'true_hydrocarbon', \n",
    "               'C6H6(GT)':'true_benzene',\n",
    "               'PT08.S2(NMHC)':'TiOx_hydrocarbon',\n",
    "               'NOx(GT)':'true_NOx',\n",
    "               'PT08.S3(NOx)':'WOx_NOx',\n",
    "               'NO2(GT)':'true_NO2',\n",
    "               'PT08.S4(NO2)':'WOx_NO2',\n",
    "               'PT08.S5(O3)':'InOx_O3',\n",
    "               'RH':'relative_humidity',\n",
    "               'AH':'absolute_humidity'\n",
    "              }\n",
    "df = df.rename(columns=naming_dict)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try to build a model to predict CO concentration. The simplest assumption is that CO concentration is correlated with the CO-targeted tin oxide (SnOx) sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = df['SnOx_CO'].values\n",
    "y = df['true_CO'].values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that there are some weird -200 values here. If you read the dataset description this is the default value for no data, so let's throw these datapoints out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "valids = np.logical_and(y>0, x>0)\n",
    "x = x[valids]\n",
    "y = y[valids]\n",
    "print('There are {} valid points'.format(len(x)))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the tin oxide (SnOx) sensor is a \"feature\" and the true CO concentration is the output. We can use this to create a baseline linear regression model. This time around we will do it with `scikit-learn` instead of our own functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(x.size,1)\n",
    "intercept = np.ones((x.size,1))\n",
    "X = np.append(intercept,x,1)\n",
    "y = y.reshape(y.size,1)\n",
    "beta = multilinear_regression(X, y)\n",
    "yhat = multilinear_prediction(X,beta)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "# print R^2\n",
    "ybar = np.mean(y)\n",
    "SST = np.sum((y-ybar)**2)\n",
    "SSE = np.sum((y-yhat)**2)\n",
    "print((SST - SSE) / SST)\n",
    "# print MAE\n",
    "print(np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What other features can we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =['SnOx_CO', 'TiOx_hydrocarbon', 'WOx_NOx', 'WOx_NO2','InOx_O3', 'T', 'relative_humidity'] \n",
    "X = df[names]\n",
    "y = df['true_CO']\n",
    "X = X.values\n",
    "y = y.values\n",
    "valid_X = (X > 0).min(axis=1) #<- if any are false, this will be false\n",
    "valids = np.logical_and(valid_X, y > 0)\n",
    "valids = np.logical_and(valids, np.isfinite(X).all(axis=1)) #<- this removes inf and nan\n",
    "X = X[valids, :]\n",
    "y = y[valids]\n",
    "intercept = np.ones((X.shape[0],1))\n",
    "X = np.append(intercept, X, 1)\n",
    "X_full = X.copy() #<- we will use X_full later when we want the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how well a linear model works with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = multilinear_regression(X, y)\n",
    "yhat = multilinear_prediction(X,beta)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "\n",
    "# print R^2\n",
    "ybar = np.mean(y)\n",
    "SST = np.sum((y-ybar)**2)\n",
    "SSE = np.sum((y-yhat)**2)\n",
    "print((SST - SSE) / SST)\n",
    "# print MAE\n",
    "print(np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is improved, but do we really need all the features? We can check the covariance structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X[1:,1:] - X[1:,1:].mean(axis=0))/X[1:,1:].std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are some co-linear features here. We can use PCA to form orthogonal feature sets. This time we will use `scikit-learn` instead of our own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PC_model = PCA(n_components=X.shape[1])\n",
    "PC_model.fit(X)\n",
    "X_PCA = PC_model.transform(X)\n",
    "\n",
    "covar = np.cov(X_PCA.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a linear regression model with the first `N` principal component features. We will use the `scikit-learn` linear model instead of our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "N = 3\n",
    "\n",
    "X = X_PCA[:,:N]\n",
    "\n",
    "lr.fit(X,y)\n",
    "yhat = lr.predict(X)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "# print R^2\n",
    "print('R^2', lr.score(X,y))\n",
    "# print MAE\n",
    "print('MAE', np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which principal component improves the model the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA decomposition maximizes **variance within the feature space** along each orthogonal vector. However, what we really want is to maximize the **covariance between the features and the output**. This can be achieved with partial least squares, or PLS.\n",
    "\n",
    "We won't go into the math of PLS, but conceptually it is a **supervised** alternative to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "N = 4\n",
    "PLS = PLSRegression(n_components = N, scale=False) #<- N_components tells the model how many sub-components to select\n",
    "PLS.fit(X_full,y) #<- we have to pass y into the fit function now\n",
    "yhat = PLS.predict(X_full)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "# print R^2\n",
    "print(PLS.score(X_full,y))\n",
    "# print MAE\n",
    "print(np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How many components are needed to achieve an $R^2$ similar to the model that contains all features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given number of components, the $R^2$ value of PLS will always be greater than PCA regression. For this reason PLS is typically preferred to PCA regression.\n",
    "\n",
    "Note that the MAE is not always lower. Again, this highlights the importance of making a good choice for the accuracy metric when assessing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the models we have seen so far are **parametric** - the number of parameters in the model does not depend on the number of data points. Parametric models are relatively straightforward to simplify by reducing the number of derived or observed features through linear combinations.\n",
    "\n",
    "Another possibility is to use features that are derived from the data points themselves. We will consider two possibilities:\n",
    "\n",
    "* Piecewise polynomials (or splines)\n",
    "* Gaussian kernels\n",
    "\n",
    "These are **non-parametric** models, and are much more susceptible to over-fitting. Simplifying them by removing feature terms is also not so intuitive, since it is equivalent to dropping specific data points. This is typically overcome by **regularizing** the coefficients of the model by including them in the loss function.\n",
    "\n",
    "We will return to our original example datasets to illsutrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/three_regression_examples.csv')\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(df['0'], df['1'])\n",
    "axes[1].scatter(df['0'], df['2'])\n",
    "axes[2].scatter(df['0'], df['3'])\n",
    "\n",
    "x = df['0'].values\n",
    "y1 = df['1'].values\n",
    "y2 = df['2'].values\n",
    "y3 = df['3'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise polynomials\n",
    "\n",
    "Consider \"features\" of the form:\n",
    "\n",
    "$x^{(k)} = max(0, x-x_k)^n$\n",
    "\n",
    "where $x_k$ are the input data points and $n$ is the order of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_polynomial_kernel(x, order, x_dense=None):\n",
    "    if x_dense is None:\n",
    "        x_dense = x #<- we will want to project this basis on to a finer grid sometimes.\n",
    "    features = []\n",
    "    for xi in x:\n",
    "        features.append(np.array([max(0,xj-xi)**order for xj in x_dense]))\n",
    "    return np.array(features).T\n",
    "\n",
    "X_pieces = piecewise_polynomial_kernel(x,1)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_pieces.shape[1]):\n",
    "    ax.plot(x,X_pieces[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to use Gaussian kernels centered at each data point:\n",
    "\n",
    "$x^{(k)} = \\exp{\\left(-\\frac{(x-x_k)^2}{2\\sigma^2}\\right)}$\n",
    "\n",
    "Note that if $k=N$ and the data points are evenly spaced then this is equivalent to the Gaussian features from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, sigma = 1, x_dense=None):\n",
    "    # x is a vector\n",
    "    # sigma is the standard deviation\n",
    "    if x_dense is None:\n",
    "        x_dense = x #<- we will want to project this basis on to a finer grid sometimes.\n",
    "    features = []\n",
    "    for xk in x: #<- compare this to the function for parametric Gaussian basis\n",
    "        features.append(np.exp(-((x_dense - xk)**2/(2*sigma**2))))\n",
    "    return np.array(features).T\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "#x_dense = x\n",
    "X_kernel = gaussian_kernel(x,0.5,x_dense)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_kernel.shape[1]):\n",
    "    ax.plot(x_dense,X_kernel[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "\n",
    "#plot the original data\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(x, y1)\n",
    "axes[1].scatter(x, y2)\n",
    "axes[2].scatter(x, y3)\n",
    "\n",
    "# Generate features\n",
    "N_poly = 1\n",
    "X_poly = piecewise_polynomial_kernel(x,N_poly)\n",
    "sigma = 0.5\n",
    "X_gauss = gaussian_kernel(x,sigma)\n",
    "\n",
    "# Iterate through feature options\n",
    "X_features = [X_poly, X_gauss]\n",
    "\n",
    "# Create \"dense\" versions for predictions\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "dense_features = [piecewise_polynomial_kernel(x, N_poly, x_dense), gaussian_kernel(x, sigma, x_dense)]\n",
    "feature_functions = [piecewise_polynomial_kernel,  gaussian_kernel]\n",
    "\n",
    "colors = ['r','b']\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "for yi, ax in zip([y1, y2, y3], axes):\n",
    "    for X, X_dense, color in zip(X_features,dense_features,colors):#<- useful way to iterate through synced lists\n",
    "        lr.fit(X, yi)\n",
    "        yhat = lr.predict(X_dense)\n",
    "        ax.plot(x_dense, yhat, color=color, ls='-')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These \"kernel\" functions are very good at interpolating, but terrible at extrapolating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What happens if `N_poly` is increased or `sigma` is decreased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient values for these non-parametric models can also span a huge range, especially if the model is substantially over-fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = piecewise_polynomial_kernel(x,3)\n",
    "lr.fit(X_poly,y1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the features are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X_poly - X_poly.mean(axis=0))/X_poly.std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the magnitude of the parameters corresponds to the \"smoothness\" of the function. We can see this if we take the derivative of the basis functions, or empirically by playing with the parameters above.\n",
    "\n",
    "This is the basis of **regularization** as a route to control co-linearity and model complexity.\n",
    "\n",
    "Recall that least-square regression uses the sum of squared errors as the objective function:\n",
    "\n",
    "$min(\\sum_i \\epsilon_i^2)$ with respect to parameters $\\beta$. This is achieved by setting the derivative equal to zero:\n",
    "\n",
    "$\\frac{\\sum_i \\epsilon_i^2}{\\partial \\vec{\\beta}}=0$\n",
    "\n",
    "In a **regularized linear model** we include a **penalty** term on the magnitude of the coefficients in the loss function:\n",
    "\n",
    "$min(\\sum_i \\epsilon_i^2 + \\lambda || \\vec{\\beta} ||)$\n",
    "\n",
    "where $\\lambda$ is a regularization strength hyperparameter, $||.||$ is a \"norm\". The most common types of regularization are **ridge regression** which uses an $L_2$ norm ($\\sqrt{\\sum_i \\beta_i^2}$), **LASSO regression** which uses an $L_1$ norm ($\\sum_i | \\beta_i |$), and **elastic net** regression which uses a combination.\n",
    "\n",
    "We will revisit these in more detail, but for now you can remember the following:\n",
    "\n",
    "* Ridge regression: mininizes the magnitude of the coefficients, but not the number of coefficients. It is very stable, but does not reduce the number of features needed.\n",
    "\n",
    "* LASSO regression: minimizes both magnitude and number of coefficients. However, it can be unstable if features are very co-linear.\n",
    "\n",
    "* Elastic net regression: provides a flexible tradeoff between ridge and LASSO, but uses an additional hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel ridge regression (KRR)\n",
    "\n",
    "Kernel ridge regression is one of the most common regression models in machine learning. It is a **non-parametric** model that uses a \"kernel\" (typically Gaussian) at each training data point, and uses **regularization** through ridge regression to control the complexity of the model.\n",
    "\n",
    "We have already seen Gaussian kernels, and will combine this with ridge regression from `scikit-learn` to implement the model.\n",
    "\n",
    "We will not derive the equations for KRR, but **you should be able to derive them following the same procedure that was used for least-squares**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on a single dataset to illustrate the technique, and in order to test the model we will need to use **cross validation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.3)\n",
    "\n",
    "axes[0].scatter(x_train,y_train,color='b')\n",
    "axes[0].scatter(x_test,y_test,color='r')\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "X_kernel_dense = gaussian_kernel(x_train,sigma,x_dense) #<- Only TRAINING POINTS are included in the feature set!\n",
    "\n",
    "for col in range(X_kernel_dense.shape[1]):\n",
    "    axes[1].plot(x_dense,X_kernel_dense[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = 0) #<- the regularization strength is called \"alpha\" in scikit-learn (instead of lambda)\n",
    "sigma = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_kernel =  gaussian_kernel(x_train,sigma)\n",
    "X_kernel_test = gaussian_kernel(x_train, sigma, x_test) #<- make sure you understand why x_test used as x_dense instead of x?\n",
    "\n",
    "ridge.fit(X_kernel, y_train)\n",
    "yhat = ridge.predict(X_kernel_dense)\n",
    "y_predict = ridge.predict(X_kernel_test)\n",
    "ax.plot(x_dense, yhat, color=color, ls='-')\n",
    "\n",
    "ax.scatter(x_train,y_train,color='b')\n",
    "ax.scatter(x_test,y_test,color='r')\n",
    "\n",
    "prediction_error = np.mean(np.abs(y_test - y_predict))\n",
    "print(prediction_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How many hyper-parameters are in this model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cross-validation to optimize the hyperparameters, but it will be a lot easier if we just use the `scikit-learn` KRR implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "min_error = 1e99\n",
    "\n",
    "for alpha in [0, 0.01, 0.1, 0.5, 1, 10]:\n",
    "    for sigma in [0.05, 0.25, 0.5, 0.75, 1, 2, 4]:\n",
    "        gamma = 1./sigma #<- scikit learn uses the inverse of sigma\n",
    "        KRR = KernelRidge(alpha=alpha, gamma=gamma, kernel='rbf') #<- the \"rbf\" kernel is the same as a Gaussian kernel\n",
    "        X = x_train.reshape(x_train.size,1) #<- must be a column vector\n",
    "        KRR.fit(X,y_train)\n",
    "        X_test = x_test.reshape(x_test.size,1)\n",
    "        ypredict = KRR.predict(X_test)\n",
    "        prediction_error = np.mean(np.abs(y_test - y_predict))\n",
    "        if prediction_error < min_error:\n",
    "            best = [alpha, sigma, prediction_error]\n",
    "            min_error = prediction_error\n",
    "            \n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case regularization isn't necessary since the points are evenly spaced and not too dense. However, in many cases it is critical to use regularization to obtain a reliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approaches to regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles from re-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian process regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [Hastie Elements of Statistical Learning Ch. 5](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "* [Python data science handbook linear regression](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb)\n",
    "* [Raschka regression discussion](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)\n",
    "* [Scikit-learn PLS regression](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html)\n",
    "* [Scikit=learn linear models](http://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
