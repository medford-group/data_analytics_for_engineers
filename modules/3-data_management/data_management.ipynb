{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#The-Iceberg-Analogy\" data-toc-modified-id=\"The-Iceberg-Analogy-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The Iceberg Analogy</a></span></li><li><span><a href=\"#Data-pipelines\" data-toc-modified-id=\"Data-pipelines-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data pipelines</a></span></li><li><span><a href=\"#Concepts-in-Data-Management-and-Storage\" data-toc-modified-id=\"Concepts-in-Data-Management-and-Storage-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Concepts in Data Management and Storage</a></span></li><li><span><a href=\"#Structured-Data-and-Metadata:-HDF5\" data-toc-modified-id=\"Structured-Data-and-Metadata:-HDF5-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Structured Data and Metadata: HDF5</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Store-the-values-of-the-following-simulation-parameters-in-a-single-HDF5-file-called-&quot;concentration_scenarios.hdf5&quot;.\" data-toc-modified-id=\"Exercise:-Store-the-values-of-the-following-simulation-parameters-in-a-single-HDF5-file-called-&quot;concentration_scenarios.hdf5&quot;.-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Exercise: Store the values of the following simulation parameters in a single HDF5 file called \"concentration_scenarios.hdf5\".</a></span></li></ul></li></ul></li><li><span><a href=\"#Structured-Data:-Spreadsheets\" data-toc-modified-id=\"Structured-Data:-Spreadsheets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Structured Data: Spreadsheets</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Referencing-data-in-Pandas-dataframes\" data-toc-modified-id=\"Exercise:-Referencing-data-in-Pandas-dataframes-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Exercise: Referencing data in Pandas dataframes</a></span></li><li><span><a href=\"#Discussion:-What-are-we-doing-to-numbers-with-<-symbols?-What-is-the-implication?\" data-toc-modified-id=\"Discussion:-What-are-we-doing-to-numbers-with-<-symbols?-What-is-the-implication?-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Discussion: What are we doing to numbers with &lt; symbols? What is the implication?</a></span></li><li><span><a href=\"#Excercise:-Plot-the-&quot;Total-Coliform&quot;-content-from-2016-2019-at-the-USGS-02336526-monitoring-station.\" data-toc-modified-id=\"Excercise:-Plot-the-&quot;Total-Coliform&quot;-content-from-2016-2019-at-the-USGS-02336526-monitoring-station.-6.0.3\"><span class=\"toc-item-num\">6.0.3&nbsp;&nbsp;</span>Excercise: Plot the \"Total Coliform\" content from 2016-2019 at the USGS-02336526 monitoring station.</a></span></li></ul></li></ul></li><li><span><a href=\"#Custom-File-Format:-GIS-Shapefiles\" data-toc-modified-id=\"Custom-File-Format:-GIS-Shapefiles-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Custom File Format: GIS Shapefiles</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-How-many-streams/rivers-intersect-Proctor-Creek?\" data-toc-modified-id=\"Exercise:-How-many-streams/rivers-intersect-Proctor-Creek?-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>Exercise: How many streams/rivers intersect Proctor Creek?</a></span></li></ul></li></ul></li><li><span><a href=\"#Application-Programming-Interfaces:-RESTful-API\" data-toc-modified-id=\"Application-Programming-Interfaces:-RESTful-API-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Application Programming Interfaces: RESTful API</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Write-a-function-that-takes-the-station-ID-and-returns-the-Monitoring-Location-Name,-Latitude,-and-Longitude-for-a-given-station\" data-toc-modified-id=\"Exercise:-Write-a-function-that-takes-the-station-ID-and-returns-the-Monitoring-Location-Name,-Latitude,-and-Longitude-for-a-given-station-8.0.1\"><span class=\"toc-item-num\">8.0.1&nbsp;&nbsp;</span>Exercise: Write a function that takes the station ID and returns the Monitoring Location Name, Latitude, and Longitude for a given station</a></span></li></ul></li></ul></li><li><span><a href=\"#Combining-Data\" data-toc-modified-id=\"Combining-Data-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Combining Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Find-the-most-recent-&quot;Total-Coliform&quot;-for-each-of-the-stations-along-the-Chattahoochee.\" data-toc-modified-id=\"Exercise:-Find-the-most-recent-&quot;Total-Coliform&quot;-for-each-of-the-stations-along-the-Chattahoochee.-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Exercise: Find the most recent \"Total Coliform\" for each of the stations along the Chattahoochee.</a></span></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iceberg Analogy\n",
    "\n",
    "Data (science) is [like an iceberg](https://towardsdatascience.com/the-iceberg-secret-in-machine-learning-37b216baa903):\n",
    "* Over 80% of time is spent cleaning/structuring data\n",
    "* Over 80% of existing data is \"unstructured\"\n",
    "* 90% of the data ever generated by humanity was generated in the last [two years](https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#7b183e60ba99)!\n",
    "\n",
    "...but how much of it is useful?\n",
    "In this course most data will be at least partially cleaned, but in the real world this is rarely the case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipelines\n",
    "\n",
    "Good integration of data storage + analysis can make or break a real project\n",
    "* Separate raw data from analysis facilitates reproducibility and scalability\n",
    "* Rapidly prototype and iteratevely improve.\n",
    "\n",
    "A \"data pipeline\" makes it easy to run an analysis routine on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts in Data Management and Storage\n",
    "\n",
    "* Structured data\n",
    "    - Data with strict or standardized rules for accessing entries\n",
    "        - Matrices and arrays (numpy, **HDF5**)\n",
    "        - **Spreadsheets** (Excel, csv, **pandas**)\n",
    "        - Schema-based databases (SQL)\n",
    "        \n",
    "* Schemas\n",
    "    - Rules defining how data is organized\n",
    "        - Schema-based databases (SQL)\n",
    "        - Schema-free databases (MongoDB)\n",
    "    \n",
    "* Meta-data\n",
    "    - Data that describes the conditions or details of a dataset\n",
    "        - Headers in spreadsheets\n",
    "        - File formats with built-in meta-data (JSON, **HDF5**)\n",
    "        \n",
    "* Custom file formats\n",
    "    - Application-specific file formats\n",
    "        - Many different examples (geospatial information (**shapefiles**), crystallographic information (CIF files), finite element meshes (MSH files), ...)\n",
    "        - Human readable (ASCII) or machine readable (binary)\n",
    "        - Open formats and proprietary formats\n",
    "    \n",
    "* Application Programming Interfaces (APIs)\n",
    "    - Programmatic way to access data from databases or websites\n",
    "        - **RESTful interface** (access via URL's)\n",
    "        - Custom APIs (access via programming language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will cover a few examples of how data can be stored or accessed. The theme of the lecture is water quality, and we will explore a dataset related to Atlanta water quality. There is no case study associated with this lecture, but you will have an opportunity to work with this dataset for a final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data and Metadata: HDF5\n",
    "\n",
    "It is very common in engineering to encounter large and highly structured data such as very large matrices. For example, you might measure or simulate a quantity on a finite-difference grid, and want to store the result. There are two challenges with this:\n",
    "\n",
    "1) You often need some \"metadata\" to interpret the results. A giant matrix doesn't mean much if you don't know the details of how it was generated. You could store this in a separate file (e.g. a text file), but if the text file is ever lost or separated from the data then the data becomes meaningless.\n",
    "\n",
    "2) The data structures can be *very* large. For a 3-d simulation, the size scales as $N^3$, where $N$ is the number of points in each dimension. If you have 1000 points in each dimension you will have 1 billion points! This can be very slow to read in.\n",
    "\n",
    "The `HDF5` file format is very convenient for dealing with this problem. You can store metadata within the same file as the raw data, ensuring that the metadata is never separated. You can also access only certain subsets of the data, making it easy to only access relevant sub-sections as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off the theme of water quality, consider the following analytical form of the one-dimensional advection-diffusion equation as a function of time, inspired by Eq. 16 in [Samalerk and Pochai, 2018](https://www.hindawi.com/journals/aaa/2018/1926519/):\n",
    "\n",
    "$\\theta(x, t, [A, B, C, D, E, F]) = \\frac{A}{\\sqrt{B + Ct}}\\exp{-\\left( \\frac{(x + D - t)^2}{E + F*t} \\right)}$\n",
    "\n",
    "We can implement this as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta(x,t, params = [2.5, 0.00625, 0.02, 0, 0.00125, 0.04]):\n",
    "    A, B, C, D, E, F = params\n",
    "    prefactor = A/(np.sqrt(B + C*t))\n",
    "    numer = (x + D - t)**2\n",
    "    denom = (E + F*t)\n",
    "    return prefactor*np.exp(-(numer/denom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the scenario where this models bacteria concentration along river downstream of a pollutant after a sewage leak. You might want to generate many different scenarios with different values of the parameters to compare to measurements. Let's start by generating some high-resolution data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_range = np.linspace(0, 1, 1001)\n",
    "t_range = np.linspace(0, 1, 1001)\n",
    "tt, xx = np.meshgrid(t_range, x_range)\n",
    "theta_matrix = theta(xx, tt)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.contourf(t_range, x_range, theta_matrix)\n",
    "ax.set_xlabel('Time [unitless]')\n",
    "ax.set_ylabel('Position [unitless]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the pollutant is introduced at $x=0, t=0$ and moves along $x$ as time increases. We can use `numpy` to save this to a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('concentration_data.txt', theta_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load this back in as a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fromfile('concentration_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a custom `numpy` format that is a lot faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#numpy native format timing\n",
    "t0 = time.time()\n",
    "np.save('concentration_data.npy', theta_matrix)\n",
    "A = np.load('concentration_data.npy')\n",
    "t_numpy = time.time() - t0\n",
    "\n",
    "#txt format timing\n",
    "t0 = time.time()\n",
    "np.savetxt('concentration_data.txt', theta_matrix)\n",
    "A = np.fromfile('concentration_data.txt')\n",
    "t_txt = time.time() - t0\n",
    "\n",
    "print(\"numpy timing: {:.4f} \\n text timing: {:.4f}\".format(t_numpy, t_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even if we read this back in, we still won't know all the details of the dataset. If it is just one simulation it might not be too bad, but what if we want to compare a few different sets of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_1 = [2.5, 0.00625, 0.02, 0, 0.00125, 0.04]\n",
    "theta_1 = theta(xx, tt, params = param_1)\n",
    "\n",
    "param_2 = [5, 0.00625, 0.02, 0.2, 0.00125, 0.04]\n",
    "theta_2 = theta(xx, tt, params = param_2)\n",
    "\n",
    "param_3 = [2.5, 0.05, 0.02, 0, 0.00125, 0.04]\n",
    "theta_3 = theta(xx, tt, params = param_3)\n",
    "\n",
    "param_4 = [2.5, 0.05, 0.02, 0, 0.01, 0.04]\n",
    "theta_4 = theta(xx, tt, params = param_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create different files for each one, but there will be a lot of files floating around, and it could be hard to keep track of what the matrices mean. The HDF5 file format is great because it is \"self describing\", and allows us to save metadata along with the data. This quick-start guide is a good starting point, and we will show an example here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "h5f = h5py.File(\"concentration_data.hdf5\", \"w\") #the w means \"write\"\n",
    "h5f.create_dataset(\"paramset_1\", data=theta_1, dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check for the datasets using the `keys()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(h5f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set attributes of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f['paramset_1'].attrs['parameters'] = param_1\n",
    "dx = x_range[1] - x_range[0]\n",
    "dt = t_range[1] - t_range[0]\n",
    "h5f['paramset_1'].attrs['dx'] = dx\n",
    "h5f['paramset_1'].attrs['dt'] = dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also assign the dataset to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset2 = h5f.create_dataset(\"paramset_2\", data=theta_2, dtype='float')\n",
    "#note that we can only run this cell once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = {'parameters':param_2, 'dt':dt, 'dx':dx}\n",
    "for key, val in attrs.items():\n",
    "    dset2.attrs[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing with HDF5 files is that we always have to remember to close them or they can become corrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data into a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h5f = h5py.File(\"concentration_data.hdf5\",'r') #the 'r' means read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how fast the load step was. This is because HDF5 doesn't read into temporary memory. Instead, it just loads data when it is needed.\n",
    "\n",
    "Let's check the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(new_h5f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_one = new_h5f['paramset_1']\n",
    "theta_one.shape\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(t_range, x_range, theta_one)\n",
    "ax.set_xlabel('Time [unitless]')\n",
    "ax.set_ylabel('Position [unitless]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily compare the datasets by changing the `paramset_1` key to `paramset_2`. We can also just read in a single slice from an HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = new_h5f['paramset_2'][0,:]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_range, x_0)\n",
    "ax.set_xlabel('x @ t=0 [unitless]')\n",
    "ax.set_ylabel('Concentration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key thing to note is that we are *only* reading in the 1000 values that are plotted here, not the entire HDF5 file. This means that even if the file is TB's, we can read just a few kB at a time if that is all we need. This can save a ton of time if you are working with really large arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Store the values of the following simulation parameters in a single HDF5 file called \"concentration_scenarios.hdf5\".\n",
    "\n",
    "Be sure to store the appropriate meta-data so that the code block below reads your file in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = [2.5, 0.00625, 0.02, 0, 0.00125, 0.04]\n",
    "params_2 = [5, 0.0625, 0.02, 0.2, 0.00125, 0.2]\n",
    "params_3 = [2.5, 0.05, 0.2, 0, 0.00125, 0.04]\n",
    "params_4 = [2.5, 0.05, 0.02, 0, 0.01, 0.15]\n",
    "\n",
    "##insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @Gabriel - can you read a code block that will read and plot one of the datasets from a correctly-generated HDF5 file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Data: Spreadsheets\n",
    "\n",
    "Spreadsheets are a common route to working with data, but they have some key disadvantages. It can be challenging to automate analyses done with spreadsheets, and they are often limited by the size. The `pandas` package in Python is a commonly used tool to interact with spreadsheet data. In this example we will see how `pandas` works by examining a spreadsheet of water quality data retrived from the [National Water Quality Monitoring Council](https://www.waterqualitydata.us/) ([download the data with this link](https://www.waterqualitydata.us/data/Result/search?bBox=-84.49,33.79,-84.45,33.80&startDateLo=01-01-2010&mimeType=csv)).\n",
    "\n",
    "First, we can import the `pandas` library and read the data in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500) #we need this to display columns for datasets with >50 columns.\n",
    "\n",
    "df = pd.read_csv('data/water_quality.csv', low_memory=False) #Some columns have multiple types of data, so more memory is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `head` or `tail` commands to examine the resulting \"dataframe\" object, and see that it looks similar to a spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head(5)\n",
    "#df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes are a cross between dictionaries and numpy arrays. Unlike arrays, they are allowed to hold multiple types, and they index columns and rows based on \"keys\" rather than numbers. We can explore some key properties here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.index) #row names\n",
    "#print(df.columns) # column names\n",
    "#print(type(df.values)) #all data\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Referencing data in Pandas dataframes\n",
    "\n",
    "In the code blocks below, write code to achieve the following tasks:\n",
    "\n",
    "1) Return all entries of the column called `CharacteristicName` and return the entries as a Python list.\n",
    "\n",
    "2) Return all entries of the 8th row as a Pandas \"Series\" object.\n",
    "\n",
    "3) Return all entries of the columns called `ActivityStartDate` and `CharacteristicName` as a Pandas dataframe.\n",
    "\n",
    "Bonus: Finish each task in a single line.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "Data can be accessed in several ways:\n",
    "\n",
    "* Columns can be accessed directly with keys.\n",
    "* The `loc` method enables numpy-like indexing, fancy indexing, and slicing.\n",
    "* The `iloc` method is similar to `loc`, but indexes by position (rather than key)\n",
    "* These methods return `pandas.Series` objects, that are basically 1D dataframes. They can be converted to various data types using methods like `tolist`.\n",
    "\n",
    "Columns can be deleted in three ways:\n",
    "\n",
    "`del` : delete the Series from the dataframe\n",
    "`pop()` : delete the Series and return the Series\n",
    "`drop(labels, axis)` : return a new dataframe with Series removed (do not modify original df)\n",
    "\n",
    "Rows must be \"dropped\".\n",
    "\n",
    "**Pay attention to whether operations are \"in place\" or not.** Many pandas operations are not \"in place\" by default. This means that they return a copy of the dataframe with modifications, rather than modifying the original dataframe object. This can be very confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include exercise code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are quite a lot of columns here! If we explore the data we can see which ones we might be interested in. In this case, let's say we are interested in the time/location, the type of contaminant measured, and the unit for that contaminant. By examining the dataframe object above, we can find the column names and narrow it down.\n",
    "\n",
    "Note how we can use the names of columns as \"indexes\" in the same way we would index a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interesting = ['ActivityStartDate', 'ActivityStartTime/Time', 'MonitoringLocationIdentifier', \n",
    "               'CharacteristicName', 'ResultSampleFractionText', 'ResultMeasureValue', \n",
    "               'ResultMeasure/MeasureUnitCode']\n",
    "dfi = df[interesting]\n",
    "dfi.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a slightly more manageable dataset to work with, but there is still a lot going on. One critical indicator of water quality is the presence of Escherichia Coli bacteria, also known as E. Coli. We can easily narrow the dataset down to only E. Coli concentration using a handy filtering technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contaminant = dfi[dfi['CharacteristicName'] == 'Escherichia coli']\n",
    "df_contaminant.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This filtering technique uses a combination of [list comprehensions](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) and [boolean indexing](https://www.geeksforgeeks.org/boolean-indexing-in-pandas/). It is a little complex so don't worry if it takes a while to figure out what is going on. We can also do the same thing in multiple lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_vector = dfi['CharacteristicName'] == 'Escherichia coli'\n",
    "print(boolean_vector[:5])\n",
    "dfi[boolean_vector].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer examination shows that not all of the data is consistent. Some are reported in `MPN` (most probable number), and others are reported in `MPN/100 ml`. It is unclear if these units are consistent or not, so let's re-filter to only work with data in `MPN` since it is the more common unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contaminant_clean = df_contaminant[df_contaminant['ResultMeasure/MeasureUnitCode'] == 'MPN']\n",
    "print(len(df_contaminant_clean))\n",
    "df_contaminant_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a relatively clean dataset. Let's say we want to plot the dissolved nitrogen concentration as a function of time. This will require a little more data manipulation, since Python doesn't currently understand the dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = df_contaminant_clean.iloc[0]['ActivityStartDate']\n",
    "time = df_contaminant_clean.iloc[0]['ActivityStartTime/Time']\n",
    "print(date, time)\n",
    "print(type(date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Pandas indexing can be very confusing since `loc` still uses the numbers in the index column (which are no longer sequential). However, `iloc` indexes sequentially from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, Pandas also has some built-in functionality to make it easy to work with dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime('2013-05-09 13:10:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_contaminant_clean['ActivityStartDate'].tolist()\n",
    "times = df_contaminant_clean['ActivityStartTime/Time'].tolist()\n",
    "datetimes = [di+' '+ti for di, ti in zip(dates, times)] #<- list comprehensions are hard to understand, but magical\n",
    "#print(datetimes)\n",
    "datetimes = pd.to_datetime(datetimes)\n",
    "#print(datetimes)\n",
    "df_contaminant_clean['DateTimes'] = datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we got a weird `SettingWithCopy` warning. This is an annoying warning in `pandas`, and is discussed in detail [here](https://www.dataquest.io/blog/settingwithcopywarning/). This is similar to the discussion of copying arrays in `numpy`, but `pandas` warns you about it. At first, this is annoying, but in the long run it is a good thing because it is better to fix the warning here than to have it cause unexpected problems later. All we have to do is tell `pandas` that we definitely want a copy before modifying a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contaminant_clean = df_contaminant_clean.copy()\n",
    "df_contaminant_clean['DateTimes'] = datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more warning! In practice it is tempting to ignore this warning, but it is not recommended since it can cause issues that are very hard to debug.\n",
    "\n",
    "There is one more wrinkle to iron out, since it turns out that some (but not all) of the values include commas, which must be removed before they can be converted to floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentrations = df_contaminant_clean['ResultMeasureValue']\n",
    "floats = []\n",
    "for ci in concentrations:\n",
    "    if ',' in ci:\n",
    "        ci = ci.replace(',','')\n",
    "    if '<' in ci:\n",
    "        ci = ci.replace('<','')\n",
    "    floats.append(float(ci))\n",
    "df_contaminant_clean.loc[:,'Concentration'] = floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What are we doing to numbers with < symbols? What is the implication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a nice dataset that is consistent with what you are handed in most other assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_concentration = df_contaminant_clean[['DateTimes', 'Concentration']].values\n",
    "time_concentration[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, it is useful to keep your data in Pandas, especially if you are working with time series, since Pandas can treat time as an \"index\". This leads to lots of useful shortcuts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_concentration = df_contaminant_clean[['DateTimes', 'Concentration']]\n",
    "df_time_concentration = df_time_concentration.set_index('DateTimes') # set the index\n",
    "df_time_concentration.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make use of Pandas built-in plotting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_concentration.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the water quality varies widely! We can isolate some of the worst times using our filtering trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_times = df_time_concentration[df_time_concentration['Concentration'] > 40000]\n",
    "bad_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about [time series analysis in Pandas](https://www.dataquest.io/blog/tutorial-time-series-analysis-with-pandas/). There are many additional features that are useful for creating nice plots and performing more detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise: Plot the \"Total Coliform\" content from 2016-2019 at the USGS-02336526 monitoring station.\n",
    "\n",
    "There are other types of coliform bacteria in the water. The `CharacteristicName` is \"Total Coliform\". Use a similar analysis workflow to plot the total colifrom concentration, but restrict the values to measurements with units of `MPN/100 ml` measured at the `USGS-02336526` monitoring station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom File Format: GIS Shapefiles\n",
    "\n",
    "It is very common to encounter data that has some structure, but the structure is more complex than a matrix or data table, and may contain many special cases or irregularities. The data structures are typically specialized to specific applications, so we will work with a specific data type called `shapefiles` that are commonly used to store geospatial data. Geospatial data is useful in many engineering contexts, and in this example we will consider an environmental application. In particular, we will look at data pertaining to the Proctor Creek watershed in the Atlanta area, which is monitored by community partners at the [West Atlanta Watershed Alliance](http://wawa-online.org/) (WAWA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the `geopandas` library. This is a version of the `pandas` library that you already learned about, but it has been modified to make it easier to work with GIS data. If you are ever working with custom file types the first thing to do is see if someone already wrote a Python package for the data type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the line below and run it to install geopandas on OSX or Unix. For Windows, type the command in the Conda Prompt\n",
    "#! pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will read in a `shapefile` of watersheds in the Atlanta region. This file was provided by partners from [WAWA](http://wawa-online.org/), and can also be obtained directly from [here](https://opendata.atlantaregional.com/datasets/rivers-streams-atlanta-region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file('data/Rivers_Streams_Atlanta_Region.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data. You can see that it looks like a `pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that is new here is the `LINESTRING` type in the \"geometry\" column. Let's see what happens if we plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information here, but it looks like it could be a map of all the rivers, lakes, and streams in the Atlanta region. The `geopandas` library seems to understand the linestring type! We can also examine it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = data.loc[0,'geometry']\n",
    "type(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot going on under the hood here. The `geopandas` library is built on top of another Python library called [`shapely`](https://pypi.org/project/Shapely/) which is built for manipulating planar geometric objects. We won't go into the details, but you can see how Python makes things easier by creating objects around different data structures. The downside is that this is often not standard, so we need to spend time investigating the different data structures.\n",
    "\n",
    "Let's see what we can do with one of these objects by checking its methods/attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(line)\n",
    "line.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods make it easy to interact with the lines or shapes that correspond to water features.\n",
    "\n",
    "We can also use this dataset to analyze the water features in the Atlanta area. For example, we can count how many streams/rivers there are by filtering on the `FEATURE_TY` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_rivers = data[data['FEATURE_TY'] == 'Stream/River']\n",
    "print(\"Total Number of Streams/Rivers = {}\".format(len(streams_rivers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if any are named \"Proctor Creek\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, stream in streams_rivers.iterrows():\n",
    "    name = stream['NAME']\n",
    "    if name is not None and 'Proctor' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are 2 objects in the dataframe named \"Proctor Creek\". We can create a sub-frame with just these objects and take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proctor_creek = streams_rivers[streams_rivers['NAME'] == 'Proctor Creek']\n",
    "proctor_creek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these are in different counties. The Proctor Creek we are interested in is in Fulton County. We can use the [county FIPS codes](https://en.wikipedia.org/wiki/List_of_FIPS_codes_for_Georgia_(U.S._state)) to determine that the county FIP should be 121:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proctor_creek = proctor_creek[proctor_creek['COUNTY_FIP'] == '121']\n",
    "proctor_creek.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can compare this to the [interactive map of Proctor Creek](http://interact.proctormap.org) provided by WAWA. This shapefile only contains the main branch of the creek, and significantly more data and analysis would be needed to reproduce the full map. Programs like ArcGIS are much more efficient for this type of interactive visualization. However, Python is useful for automating analyses or working with large quantities of data. In particular, accessing data with Python is a good way of combining data from multiple sources to perform analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: How many streams/rivers intersect Proctor Creek?\n",
    "\n",
    "**Hint:** Check the help documentation for the `intersect` method of a `LINESHAPE` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Programming Interfaces: RESTful API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to think of data as being stored in files, since this is how most users interact with the data on their computers. However, when working with data from the internet or from large databases things get a little trickier. For example, you probably wouldn't want to work with a single file that contains all the data from all the water monitoring stations in the entire country because it would be far too large.\n",
    "\n",
    "Instead, data can be pulled directly from various data servers using queries, or even scraped directly from webpages by analyzing HTML. While all of these data sources have some sort of structure, the structure is not standardized. We can interact directly with websites using the `requests` package in Python along with their URL. Let's see what happens if we try to interact with the water quality data portal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #this library is used for making URL requests\n",
    "\n",
    "response = requests.get(\"https://www.waterqualitydata.us/portal\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is messy! This is the raw HTML for the webpage, and while there is clearly some data there, it would be very difficult to analyze. If you ever have to \"scrape\" data from HTML, I recommend the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) package, and lots of coffee. The problem with this approach is that it is *very* tedious and time-consuming, and it is very sensitive to changes in the website. If the HTML is updated even slightly, the scraper can break and require more tedious and time-consuming work.\n",
    "\n",
    "Fortunately, there is another way. \"Application Programming Interfaces\", or API's, provide a route to programmatically ask for data from a web server. Different data sources will use different API's, but the concept is widely used and is much more efficient and robust than scraping data directly from HTML. Let's take a look at the API used for the Water Quality Portal to find more information about the water quality data.\n",
    "\n",
    "The water quality data we collected earlier was collected from the location `USGS-023365218`. After a little digging, it is clear that this is a USGS monitoring station ID. However, it is not clear where it is. We could look it up manually, but then we will have to repeat the process for any other monitoring station. Alternatively, we can read about the [web services](https://www.waterqualitydata.us/webservices_documentation/) provided by the Water Quality Portal.\n",
    "\n",
    "The first step of interacting with API's is to read the documentation, since all API's will have different schemas and keywords. From the [Water Quality Portal web services documentation](https://www.waterqualitydata.us/webservices_documentation/) we can see that there is a special base URL for station data:\n",
    "\n",
    "`https://www.waterqualitydata.us/data/Station/search?`\n",
    "\n",
    "After that, we can see that there are lots of parameters we can use to search. The one called `siteid` seems promising. By looking at some examples, we can construct a search URL:\n",
    "\n",
    "`https://www.waterqualitydata.us/data/Station/search?siteid=USGS-02336523`\n",
    "\n",
    "This is a specific kind of API called a \"RESTful\" interface. These are very commonly used because they allow data access directly from URL's, and can hence be accessed by any programming language. Let's see how it works in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.waterqualitydata.us/data/Station/search?siteid=USGS-02336526\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still looks a little messy, but its better than the HTML! We can also read more in the [documentation](https://www.waterqualitydata.us/webservices_documentation/) to find out about the output format. It turns out that the default is .csv, or comma separated values. We can use a trick to convert this directly to a Pandas dataframe from the `read_csv` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO #this function makes strings act like files\n",
    "\n",
    "csv = StringIO(response.text)\n",
    "station_data = pd.read_csv(csv)\n",
    "station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there are duplicates. Another example of how real data is messy!\n",
    "\n",
    "Now we can easily extract the latitude and longitude from the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = station_data.loc[0,'LatitudeMeasure']\n",
    "long = station_data.loc[0,'LongitudeMeasure']\n",
    "print(lat, long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to ask the API for a different data format. If we read about the `mimeType` argument we see that there is something called a `geojson` file. Let's see what that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.waterqualitydata.us/data/Station/search?siteid=USGS-02336526&mimeType=geojson\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special kind of file called a \"JSON\", or Javascript Object Notation file. These are very common in data science because they are very flexible and easy to manipulate programatically. Sometimes it is useful to use a [JSON viewer](https://codebeautify.org/jsonviewer) to explore JSON files. You can copy/paste the text above into the viewer to see it. Python also has native support for working with JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "station_json = json.loads(response.text)\n",
    "print(station_json['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can do a bit more research and find that the specific `geojson` format is actually [supported by the `geopandas` library](http://geopandas.org/io.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_gpd = gpd.read_file(response.text)\n",
    "station_gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are lots of options for working with API data. When you want to access data via API it often pays off to do some research into what commands are available, and what tools have already been developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write a function that takes the station ID and returns the Monitoring Location Name, Latitude, and Longitude for a given station\n",
    "\n",
    "**Hint:** Using the JSON representation is probably easiest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we read more into the documentation, we find that we can also access water quality data from the API. The base URL is slightly different:\n",
    "\n",
    "`https://www.waterqualitydata.us/data/Result/search?`\n",
    "\n",
    "but most of the keywords are the same. Actually, this API was used to download the `water_quality.csv` file we worked with earlier! In the earlier exercise we downloaded a file with all the results from the Proctor Creek stations, then analyzed the resulting spreadsheet to extract the information we were interested in (nitrate concentration). A more efficient route is to just ask for the information we are interested in directly from the API. For example, we can pull all the \"Total Coliform\" data from the Proctor Creek station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.waterqualitydata.us/data/Result/search?siteid=USGS-02336526&CharacteristicName=Total%20Coliform'\n",
    "response = requests.get(url)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is not compatible with the `geojson` format, but we can easily convert the csv into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = StringIO(response.text)\n",
    "nitrate_data = pd.read_csv(csv)\n",
    "nitrate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more efficient than all the work we did earlier to get similar information, and the amount of data that we have to deal with is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data\n",
    "\n",
    "One of the major advantages of working with data programatically is that you can easily combine data from different sources to answer more complex questions. In this case, we will combine the data from the GIS Shapefile of rivers/streams/lakes with the water quality data from the Water Quality Portal.\n",
    "\n",
    "First, we can recall our data for Proctor Creek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proctor_creek.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine this with the data for the USGS monitoring station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = proctor_creek.plot()\n",
    "station_gpd.plot(ax=ax, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as expected, the Proctor Creek monitoring station is on Proctor Creek. We can also find out what other monitoring stations are on Proctor Creek. Reading the [Water Quality Portal documentation](https://www.waterqualitydata.us/webservices_documentation/) tells us that we can search for all stations within a \"bounding box\" of latitude/longitude. Remember that we can get bounds from a `linestring` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_geometry = proctor_creek.iloc[0]['geometry']\n",
    "pc_geometry.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation tells us that we need to specify the box based on west, south, east, north latitude/longitude. In this case, the URL corresponds to:\n",
    "\n",
    "\n",
    "`https://www.waterqualitydata.us/data/Station/search?bBox=-84.496,33.792,-84.454,33.807&mimeType=geojson`\n",
    "\n",
    "where we are asking for the `geojson` file since that was most convenient last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.waterqualitydata.us/data/Station/search?bBox=-84.496,33.792,-84.454,33.807&mimeType=geojson'\n",
    "response = requests.get(url)\n",
    "pc_stations = gpd.read_file(response.text)\n",
    "pc_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily plot these on the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = proctor_creek.plot()\n",
    "pc_stations.plot(ax=ax, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this with the [interactive map of Proctor Creek](http://interact.proctormap.org) to see that some of the stations are actually on small streams that branch off of Proctor Creek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try something a little more challenging. We will use the streams/rivers dataset along with the Water Quality Portal to identify all monitoring stations on the Chattahoochee river. First, we can find the `geopandas` dataframe for the river:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooch = streams_rivers[streams_rivers['NAME'] == 'Chattahoochee River']\n",
    "hooch.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are many different paths that represent the Chattahoochee. Let's just look at the section in Fulton County:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooch = hooch[hooch['COUNTY_FIP'] == '121']\n",
    "hooch.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the bounds will be a little more challenging, since we need to check multiple objects. However, we can create a bounding box using the minimum/maximum of the bounds. The minimum corresponds to west and south limits, and the maximum provides east and north:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bounds = []\n",
    "for i, stream in hooch.iterrows():\n",
    "    geom = stream['geometry']\n",
    "    if hasattr(geom, 'bounds'):\n",
    "        all_bounds.append(geom.bounds)\n",
    "\n",
    "all_bounds=np.array(all_bounds)\n",
    "min_bounds = all_bounds.min(axis=0)\n",
    "max_bounds = all_bounds.max(axis=0)\n",
    "print(min_bounds)\n",
    "print(max_bounds)\n",
    "west = min_bounds[0]\n",
    "south = min_bounds[1]\n",
    "east = max_bounds[2]\n",
    "north = max_bounds[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's construct our query programatically using string formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbox_url = \"https://www.waterqualitydata.us/data/Station/search?bBox={west},{south},{east},{north}&mimeType=geojson\"\n",
    "hooch_url = bbox_url.format(west=west, south=south, east=east, north=north)\n",
    "print(hooch_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can request the data. Note that this may take a long time, since we are requesting a lot of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(hooch_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read this directly into `geopandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hooch_stations = gpd.read_file(response.text, driver='GeoJSON', encoding='UTF=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sometimes this will cause an error if you have too many results. This is one of the downsides of working with external packages. They sometimes cause errors that are hard to diagonse, but overall they still save time in the long run. If this does occur we can get around it by writing it to a file and reading it back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data/Chattahoochee_Stations.geojson', 'w') as f:\n",
    "    f.write(response.text)\n",
    "    \n",
    "hooch_stations = gpd.read_file('data/Chattahoochee_Stations.geojson')\n",
    "hooch_stations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily plot the stations with the river:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hooch.plot()\n",
    "hooch_stations.plot(ax=ax, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of stations in the bounding box! We can narrow it down by using the geometry objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, stream in hooch.iterrows():\n",
    "    for j, station in hooch_stations.iterrows():\n",
    "        if stream['geometry'].touches(station['geometry']):\n",
    "            print(station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that no stations are exactly touching the Chattahoochee. We can try using distance instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "cutoff = 0.005\n",
    "for i, stream in hooch.iterrows():\n",
    "    for j, station in hooch_stations.iterrows():\n",
    "        if stream['geometry'].distance(station['geometry']) < cutoff:\n",
    "            idxs.append(j)\n",
    "            \n",
    "close_stations = hooch_stations.loc[idxs]\n",
    "ax = hooch.plot()\n",
    "close_stations.plot(ax=ax, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the distance cutoff to select stations that are closer or farther away, but a cutoff of 0.005 seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the most recent \"Total Coliform\" for each of the stations along the Chattahoochee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stations may not include data, and these should be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Data management and access can be very tedious and time consuming. When starting a new project you should budget a lot of time for preparing the dataset, and prepare to encounter many challenges and ambiguous situations. When you make an arbitrary decision it is always a good idea to note this as \"metadata\" somewhere in the resulting dataset. Keeping track of all relevant metadata, and carefully constructing a data pipeline that makes it easy to repeat analyses, takes a lot of time up front, but will save time in the long run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
