{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"images/underfitting_overfitting.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will discuss basic types of regression algorithms and explore strategies for optimizing and improving regression models. The focus will be on linear and pseudo/generalized linear problems, with some limited examples of non-linear regression.\n",
    "\n",
    "* Regression basics\n",
    "    - Problem statement for regression\n",
    "    - Properties of error distributions\n",
    "    - Accuracy metrics\n",
    "\n",
    "* Improving models by adding features\n",
    "    - Generating non-linear features\n",
    "    - Colinearity of features\n",
    "    - Principal Component Regression and Partial Least Squares\n",
    "    - Regularization of coefficients\n",
    "    - Kernel ridge regression\n",
    "\n",
    "* Improving performance by changing models\n",
    "    - Non-linear regression\n",
    "    - Neural networks\n",
    "    - kNN regression\n",
    "\n",
    "* Quantifying uncertainty\n",
    "    - Standard deviation of error\n",
    "    - Ensembles from re-sampling\n",
    "    - Gaussian process regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression problem statement\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}) + \\vec{\\epsilon}$\n",
    "\n",
    "where the output, $\\vec{y}$, is a vector (or scalar) of continuous, real values, $f$ is the model, $\\vec{x}$ is a vector (or scalar) of input features, and $\\vec{epsilon}$ is an error between the real and predicted values. It is common to write the predicted values as $\\vec{\\hat{y}}$:\n",
    "\n",
    "$\\vec{\\hat{y}} = f(\\vec{x})$\n",
    "\n",
    "such that $\\vec{\\epsilon}$ = $\\vec{y} - \\vec{\\hat{y}}$.\n",
    "\n",
    "The goal of regression is to minimize the error, $\\vec{\\epsilon}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of error distributions\n",
    "\n",
    "The distribution of errors, or \"residuals\" contains a lot of information about the performance of a model. There are two common classes of error distributions:\n",
    "\n",
    "* Normal (or Gaussian) vs. Non-normal error distributions\n",
    "\n",
    "\n",
    "Determined by how well the error distribution is described by the Gaussian probability distribution:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/normal_distribution.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "* Homoskedastic vs. Heteroskedastic error distributions\n",
    "\n",
    "Homoskedastic error distributions are *constant in the independent variables (features)*, while heteroskedastic errors vary across the input space:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/heteroskedastic.gif\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "Least-squares regression and many other regression models *assume* that errors are normally-distributed and homoskedastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics\n",
    "\n",
    "It is important to consider the context of a regression model and choose accuracy metrics that are relevant to its application. There are several common options:\n",
    "\n",
    "* #### Mean absolute error (MAE)\n",
    "\n",
    "$MAE = \\frac{1}{N} \\sum_{i=0}^N |y_i - \\hat{y}_i|$\n",
    "\n",
    "* #### Root-mean-sqaured error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=0}^N (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "MAE and RMSE are very similar. Both have units of the dependent variable ($y$), and can vary from 0 to $\\infty$ with lower values being better. MAE is less affected by outliers and sample size, but it is always lower than RMSE, so it is a less conservative estimate. MAE and RMSE are related by the inequalities:\n",
    "\n",
    "$MAE \\leq RMSE \\leq MAE \\times \\sqrt{N}$\n",
    "\n",
    "* #### $R^2$ value\n",
    "\n",
    "The $R^2$ metric is very common in regression models, and is the default \"score\" in `scikit-learn`. $R^2$ varies from 0-1, with higher values corresponding to better models. The $R^2$ value corresponds to the amount of variance in the independent variable that is explained by the model, and is defined as:\n",
    "\n",
    "$R^2 = \\frac{\\sum_{i=0}^N (y_i - \\bar{y})^2 - \\sum_{i=0}^N (y_i - \\hat{y})^2}{\\sum_{i=0}^N (y_i - \\bar{y})^2}$\n",
    "\n",
    "where $\\bar{y}$ is the mean of $y$. This is often written as:\n",
    "\n",
    "$R^2 = \\frac{SST - SSE}{SST}$\n",
    "\n",
    "where $SST = \\sum_{i=0}^N (y_i - \\bar{y})^2$ and $SSE = \\sum_{i=0}^N (y_i - \\hat{y})^2$\n",
    "\n",
    "* #### Parity plots\n",
    "\n",
    "Plotting $y$ vs. $\\hat{y}$ provides a visual analysis of the error.\n",
    "\n",
    "* #### Maximum error\n",
    "\n",
    "Sometimes it is useful to assess the maximum error of a model, $max(\\epsilon_i)$. This is useful to assess a worst-case scenario, and provides a conservative estimate of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How could you replicate the behavior of a model with $R^2 =0$ using a single parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Anscomb's Quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\n",
    "y1 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "y2 = np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])\n",
    "x4 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(x,y):    \n",
    "    y_bar = np.mean(y)\n",
    "    y_std = np.std(x)\n",
    "    m, b = np.polyfit(x,y,deg=1)\n",
    "    SST = sum((y - y_bar)**2)\n",
    "    SSE = sum((y - (m*x+b))**2)\n",
    "    R2 = (SST - SSE)/SST\n",
    "    return y_bar, y_std, m, b, R2\n",
    "\n",
    "stats1 = calc_stats(x,y1)\n",
    "print(\"Dataset 1: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats1))\n",
    "stats2 = calc_stats(x,y2)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats2))\n",
    "stats3 = calc_stats(x,y3)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats3))\n",
    "stats4 = calc_stats(x4,y4)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats4))\n",
    "avg, std, m, b, r2 = stats1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the statistics are identical for these datasets, and the $R^2$ implies that linear regression describes the same amount of variance in all cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "yhat = m*x + b\n",
    "axes[0].scatter(x,y1)\n",
    "axes[0].plot(x, yhat, ls='-', color='k')\n",
    "axes[1].scatter(x,y2)\n",
    "axes[1].plot(x, yhat, ls='-', color='k')\n",
    "axes[2].scatter(x,y3)\n",
    "axes[2].plot(x, yhat, ls='-', color='k')\n",
    "axes[3].scatter(x4,y4)\n",
    "axes[3].plot(x4, m*x4 + b, ls='-', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parity plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "yhat = m*x + b\n",
    "axes[0].scatter(y1, yhat)\n",
    "axes[0].plot(y1, y1, ls='--', color='k')\n",
    "axes[1].scatter(y2, yhat)\n",
    "axes[1].plot(y2, y2, ls='--', color='k')\n",
    "axes[2].scatter(y3, yhat)\n",
    "axes[2].plot(y3, y3, ls='--', color='k')\n",
    "axes[3].scatter(y4, m*x4 + b)\n",
    "axes[3].plot(y4, y4, ls='--', color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the error distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "axes[0].hist(y1 - yhat,density=1)\n",
    "axes[1].hist(y2 - yhat,density=1)\n",
    "axes[2].hist(y3 - yhat,density=1)\n",
    "axes[3].hist(y4 - (m*x4 + b),density=1)\n",
    "\n",
    "mu = 0\n",
    "sigma = np.std(y1 - yhat)\n",
    "x_resid = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x_resid,norm.pdf(x_resid, mu, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Based on these sets of plots, which dataset is best described by linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding features to improve performance\n",
    "\n",
    "One strategy to improve regression models is to give them more information in the form of independent variables, or \"features\". Even without changing the underlying model, this can lead to considerable improvements in accuracy. We will consider two strategies in improving the features of regression models:\n",
    "\n",
    "* Generating non-linear responses from \"linear\" models by creating *derived* features\n",
    "\n",
    "* Adding new information about samples and analyzing the property of these independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating non-linear derived features for least-squared regression\n",
    "\n",
    "We have already seen a classic example of this in previous lectures where we considered polynomial regression:\n",
    "\n",
    "$y_j = b + m*x_j + p*x_j^2 + ...$\n",
    "\n",
    "$y_j = \\beta_0 + \\beta_1 x_j + \\beta_2 x_j^2 + \\beta_3 x_j^3 + \\beta_4 x_j^4 + ...$\n",
    "\n",
    "$y_j = \\sum_i^N \\beta_i x_j^i$\n",
    "\n",
    "We can think of this as \"deriving\" non-linear polynomial features from $x$, then using these new features in a linear least-squares regression model.\n",
    "\n",
    "$x_j^{(k)} = x_j^k$\n",
    "\n",
    "$y_j = \\sum_k^N \\beta_k x_j^{(k)} = \\sum_k^N \\beta_k x_{jk} \\Rightarrow \\vec{y} = \\underline{\\underline{X}}\\vec{\\beta}$\n",
    "\n",
    "However, there is no reason that we have to use polynomials. To demonstrate this we will use the following three example datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/three_regression_examples.csv')\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(df['0'], df['1'])\n",
    "axes[1].scatter(df['0'], df['2'])\n",
    "axes[2].scatter(df['0'], df['3'])\n",
    "\n",
    "x = df['0'].values\n",
    "y1 = df['1'].values\n",
    "y2 = df['2'].values\n",
    "y3 = df['3'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we will consider two types of \"derived features\":\n",
    "\n",
    "* polynomials\n",
    "* Gaussian distributions\n",
    "\n",
    "We have already seen the \"polynomial features\" in a prior lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    # One-liner uses \"list comprehension\" to iterate through range 0 - N (note N+1 since range function is not inclusive)\n",
    "    # The input, x, is raised to the power of N for each value of N\n",
    "    # The result is converted to an array and transposed so that columns correspond to features and rows correspond to data points (individual x values)\n",
    "    return np.array([x**k for k in range(0,N)]).T\n",
    "\n",
    "X_poly = polynomial_features(x,2)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_poly.shape[1]):\n",
    "    ax.plot(x,X_poly[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is to expand the function in terms of Gaussian (normal) distributions with evenly-spaced means and a fixed standard deviation:\n",
    "\n",
    "$x^{(k)} = \\exp{\\left(-\\frac{(x-x_k)^2}{2\\sigma^2}\\right)}$\n",
    "\n",
    "where $x_k$ are the means and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_features(x, N , sigma = 1):\n",
    "    # x is a vector\n",
    "    # sigma is the standard deviation\n",
    "    xk_vec = np.linspace(min(x), max(x), N)\n",
    "    features = []\n",
    "    for xk in xk_vec:\n",
    "        features.append(np.exp(-((x - xk)**2/(2*sigma**2))))\n",
    "    return np.array(features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gauss = gaussian_features(x,6)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_gauss.shape[1]):\n",
    "    ax.plot(x,X_gauss[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have several strategies for creating non-linear derived features from the input. Let's see how they work on the example datasets. We will use our linear regression functions from last lecture. You should know how to derive these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.linalg  import solve\n",
    "def multilinear_regression(X,y):\n",
    "    ## Derive expression\n",
    "    A = np.dot(X.T, X)\n",
    "    b = np.dot(X.T, y)\n",
    "    beta = solve(A,b)\n",
    "    return beta\n",
    "\n",
    "def multilinear_prediction(X,beta):\n",
    "    return np.dot(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15 #<- Number of features\n",
    "\n",
    "# Generate features\n",
    "X_poly = polynomial_features(x,N)\n",
    "X_gauss = gaussian_features(x,N)\n",
    "\n",
    "#plot the original data\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(x, y1)\n",
    "axes[1].scatter(x, y2)\n",
    "axes[2].scatter(x, y3)\n",
    "\n",
    "# Iterate through feature options\n",
    "X_features = [X_poly, X_gauss]\n",
    "feature_functions = [polynomial_features,  gaussian_features]\n",
    "colors = ['r','b']\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "\n",
    "for yi, ax in zip([y1, y2, y3], axes):\n",
    "    for X, f_features, color in zip(X_features,feature_functions,colors):#<- useful way to iterate through synced lists\n",
    "        beta = multilinear_regression(X, yi)\n",
    "        X_dense = f_features(x_dense,N)\n",
    "        yhat = multilinear_prediction(X_dense,beta)\n",
    "        ax.plot(x_dense, yhat, color=color, ls='-')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Does N > 10 provide a reliable model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colinearity of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the properties of the features that we generated by investigating their covariance. We can get an intuitive feel for this by plotting them against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(N,N, figsize = (15,15))\n",
    "X = X_poly\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        xi = X[:,i]\n",
    "        xj = X[:,j]\n",
    "        axes[i,j].scatter(xi, xj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a clearer picture by looking at the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X[1:,1:] - X[1:,1:].mean(axis=0))/X[1:,1:].std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial features are highly correlated. This causes numerical issues for multi-linear regression since the system of equations becomes nearly redundant. This is the linear algebra equivalent of (almost) dividing by zero. Let's take a look at the coefficient values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "beta = multilinear_regression(X, y3)\n",
    "ax.plot(range(0,N),beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at two basic strategies for \"stabilizing\" the model:\n",
    "\n",
    "1) Ortogonalization of feature vectors\n",
    "\n",
    "2) Regularization of coefficients through the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonalizing coefficients\n",
    "\n",
    "Now we know that co-linear features cause unpredictable behavior in the model, so one strategy is to ensure that features are not co-linear. For derived features, we can enforce this when we create the feature set. A classic example is \"Legendre Polynomials\" which are an orthogonal polynomial basis set:\n",
    "\n",
    "$L_n(x) = \\frac{1}{2^n n!}\\frac{\\mathrm{d}^n}{\\mathrm{d}x^n}(x^2 - 1)^n $\n",
    "\n",
    "$L_0(x) = 1$\n",
    "\n",
    "$L_1(x) = x$\n",
    "\n",
    "$L_2(x) = \\frac{1}{2} (3x^2 -1)$\n",
    "\n",
    "$L_3(x) = \\frac{1}{2} (5x^3 - 3x)$\n",
    "\n",
    "...\n",
    "\n",
    "Luckily these are implemented in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial.legendre import legval\n",
    "\n",
    "def legendre_features(x, N):\n",
    "    features = []\n",
    "    for k in range(1,N+1):\n",
    "        alphas = np.zeros(k)\n",
    "        alphas[-1] = 1 #<- this gives us the kth legendre polynomial (see docs for legval)\n",
    "        x_normed = (x - min(x))/(max(x) - min(x))\n",
    "        x_normed = x_normed*2 - 1\n",
    "        features.append(legval(x_normed, alphas))\n",
    "    return np.array(features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the Legendre polynomials with regular polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "N = 6\n",
    "X_poly = polynomial_features(x,N)\n",
    "X_leg = legendre_features(x,N)\n",
    "\n",
    "for col in range(X_leg.shape[1]):\n",
    "    axes[0].plot(x,X_leg[:,col])\n",
    "\n",
    "for col in range(X_poly.shape[1]):\n",
    "    axes[1].plot(x,X_poly[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that they are orthogonal by checking the covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dense = np.linspace(min(x),max(x),100) #<- note that they will not be exactly orthogonal due to numerical issues, but as X gets more dense they become more orthogonal.\n",
    "X_leg = legendre_features(x_dense,N)\n",
    "cov_leg = np.cov(X_leg.T)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(cov_leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating orthogonal features is an elegant solution, but it isn't always practical. It is not always obvious how to determine the form of an orthogonal basis, and more importantly not all features are derived. Often features are \"observed\", not derived, so it is impossible to ensure that they are orthogonal.\n",
    "\n",
    "Fortunately, we have a solution. Remember that principal component analysis (PCA) gives us orthonormal eigenvectors of the covariance matrix. These eigenvectors can be used as features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PCA to create a set of orthogonal features from our Gaussian kernel basis set. We just need to remember that the principal component vectors are **eigenvectors of the covariance matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X_gauss - X_gauss.mean(axis=0))/X_gauss.std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "eigvals, eigvecs = np.linalg.eig(covar)\n",
    "print(eigvecs.shape)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can project the original feature space onto the eigenvector space using a dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gauss_PCA = np.dot(X_gauss, eigvecs)\n",
    "covar = np.cov(X_gauss_PCA.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our features are not only orthogonal, but they are also ordered by the amount of variance they explain. This provides a very convenient way to generate features for regression models. We can create a `PCA_features` function to do this for any given set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_features(X):\n",
    "    X_scaled = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "    covar = np.cov(X_scaled.T)\n",
    "    eigvals, eigvecs = np.linalg.eig(covar)\n",
    "    return np.dot(X, eigvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 #<- Number of features\n",
    "\n",
    "# Generate features\n",
    "X_poly = polynomial_features(x,N)\n",
    "X_leg = legendre_features(x,N)\n",
    "X_gauss = gaussian_features(x,N)\n",
    "\n",
    "X_gauss_full = gaussian_features(x,len(x)) #<- first create a lot of Gaussians\n",
    "X_gauss_PCA = PCA_features(X_gauss_full)[:,:N] #<- select the first N principal components\n",
    "\n",
    "\n",
    "#plot the original data\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(x, y1)\n",
    "axes[1].scatter(x, y2)\n",
    "axes[2].scatter(x, y3)\n",
    "\n",
    "# Iterate through feature options\n",
    "X_features = [X_poly, X_leg, X_gauss, X_gauss_PCA]\n",
    "colors = ['r','m','b','k']\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "\n",
    "for yi, ax in zip([y1, y2, y3], axes):\n",
    "    for X,  color in zip(X_features,colors):#<- useful way to iterate through synced lists\n",
    "        beta = multilinear_regression(X, yi)\n",
    "        yhat = multilinear_prediction(X,beta)\n",
    "        ax.plot(x, yhat, color=color, ls='-')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of features approaches the number of data points all models converge; however, the behavior at low N varies considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which of these choices of features is the *worst*? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Regression and Partial Least Squares\n",
    "\n",
    "Ultimately we want to not only numerically stabilize regression models, but also identify the simplest possible model (e.g. fewest number of features or parameters). We will explore two techniques for doing this:\n",
    "\n",
    "* Principal component regression (PCR) - determine principal component(s) that give the best fit.\n",
    "\n",
    "* Partial least squares (PLS) - determine the linear combinations of principal components that maximize covariance between inputs and outputs.\n",
    "\n",
    "We will look at a real data set of sensor data as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example [data set](https://archive.ics.uci.edu/ml/datasets/Air+Quality) consists of the raw output of metal oxide sensors for air pollutants in an Italian city. These metal oxide sensors are much cheaper than the standard sensors, but they are less accurate. The data set also includes measured concentrations from a calibrated sensor. The calibrated sensor will serve as the *ground truth*, and the goal is to determine the pollutant concentration based on the output of the metal oxide sensors. The units are mg/m$^3$ (CO), $\\mu$g/m$^3$ (hydrocarbons, NO2), and ppb (NOx).\n",
    "\n",
    "S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, [On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario](https://www.sciencedirect.com/science/article/pii/S0925400507007691?via%3Dihub), Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('datasets/pollution_sensors.xlsx')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's rename the columns to make life a little easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "naming_dict = {'CO(GT)':'true_CO', \n",
    "               'PT08.S1(CO)':'SnOx_CO', \n",
    "               'NMHC(GT)':'true_hydrocarbon', \n",
    "               'C6H6(GT)':'true_benzene',\n",
    "               'PT08.S2(NMHC)':'TiOx_hydrocarbon',\n",
    "               'NOx(GT)':'true_NOx',\n",
    "               'PT08.S3(NOx)':'WOx_NOx',\n",
    "               'NO2(GT)':'true_NO2',\n",
    "               'PT08.S4(NO2)':'WOx_NO2',\n",
    "               'PT08.S5(O3)':'InOx_O3',\n",
    "               'RH':'relative_humidity',\n",
    "               'AH':'absolute_humidity'\n",
    "              }\n",
    "df = df.rename(columns=naming_dict)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try to build a model to predict CO concentration. The simplest assumption is that CO concentration is correlated with the CO-targeted tin oxide (SnOx) sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = df['SnOx_CO'].values\n",
    "y = df['true_CO'].values\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that there are some weird -200 values here. If you read the dataset description this is the default value for no data, so let's throw these datapoints out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "valids = np.logical_and(y>0, x>0)\n",
    "x = x[valids]\n",
    "y = y[valids]\n",
    "print('There are {} valid points'.format(len(x)))\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the tin oxide (SnOx) sensor is a \"feature\" and the true CO concentration is the output. We can use this to create a baseline linear regression model. This time around we will do it with `scikit-learn` instead of our own functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(x.size,1)\n",
    "intercept = np.ones((x.size,1))\n",
    "X = np.append(intercept,x,1)\n",
    "y = y.reshape(y.size,1)\n",
    "beta = multilinear_regression(X, y)\n",
    "yhat = multilinear_prediction(X,beta)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "# print R^2\n",
    "ybar = np.mean(y)\n",
    "SST = np.sum((y-ybar)**2)\n",
    "SSE = np.sum((y-yhat)**2)\n",
    "print('R^2',(SST - SSE) / SST)\n",
    "# print MAE\n",
    "print('MAE',np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What other features can we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =['SnOx_CO', 'TiOx_hydrocarbon', 'WOx_NOx', 'WOx_NO2','InOx_O3', 'T', 'relative_humidity'] \n",
    "X = df[names]\n",
    "y = df['true_CO']\n",
    "X = X.values\n",
    "y = y.values\n",
    "valid_X = (X > 0).min(axis=1) #<- if any are false, this will be false\n",
    "valids = np.logical_and(valid_X, y > 0)\n",
    "valids = np.logical_and(valids, np.isfinite(X).all(axis=1)) #<- this removes inf and nan\n",
    "X = X[valids, :]\n",
    "y = y[valids]\n",
    "intercept = np.ones((X.shape[0],1))\n",
    "X = np.append(intercept, X, 1)\n",
    "X_full = X.copy() #<- we will use X_full later when we want the original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how well a linear model works with all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = multilinear_regression(X, y)\n",
    "yhat = multilinear_prediction(X,beta)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "\n",
    "# print R^2\n",
    "ybar = np.mean(y)\n",
    "SST = np.sum((y-ybar)**2)\n",
    "SSE = np.sum((y-yhat)**2)\n",
    "print((SST - SSE) / SST)\n",
    "# print MAE\n",
    "print(np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is improved, but do we really need all the features? We can check the covariance structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X[1:,1:] - X[1:,1:].mean(axis=0))/X[1:,1:].std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are some co-linear features here. We can use PCA to form orthogonal feature sets. This time we will use `scikit-learn` instead of our own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PC_model = PCA(n_components=X.shape[1])\n",
    "PC_model.fit(X)\n",
    "X_PCA = PC_model.transform(X)\n",
    "\n",
    "covar = np.cov(X_PCA.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a linear regression model with the first `N` principal component features. We will use the `scikit-learn` linear model instead of our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "N = 8\n",
    "\n",
    "X = X_PCA[:,:N]\n",
    "\n",
    "lr.fit(X,y)\n",
    "yhat = lr.predict(X)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "# print R^2\n",
    "print('R^2', lr.score(X,y))\n",
    "# print MAE\n",
    "print('MAE', np.mean(np.abs(y-yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which principal component improves the model the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA decomposition maximizes **variance within the feature space** along each orthogonal vector. However, what we really want is to maximize the **covariance between the features and the output**. This can be achieved with partial least squares, or PLS.\n",
    "\n",
    "We won't go into the math of PLS, but conceptually it is a **supervised** alternative to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "N = 8\n",
    "PLS = PLSRegression(n_components = N, tol=1e-8) #<- N_components tells the model how many sub-components to select\n",
    "PLS.fit(X_full,y) #<- we have to pass y into the fit function now\n",
    "yhat_PLS = PLS.predict(X_full)[:,0] #<- the prediction here is a column vector\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make a parity plot\n",
    "ax.scatter(y,yhat_PLS)\n",
    "ax.plot(y,y,ls='--',color='k')\n",
    "\n",
    "# print R^2\n",
    "print(PLS.score(X_full,y))\n",
    "\n",
    "# print MAE\n",
    "print(np.mean(np.abs(y-yhat_PLS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How many components are needed to achieve an $R^2$ similar to the model that contains all features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given number of components, the $R^2$ value of PLS will always be greater than PCA regression. For this reason PLS is typically preferred to PCA regression.\n",
    "\n",
    "Note that the MAE is not always lower. Again, this highlights the importance of making a good choice for the accuracy metric when assessing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization of Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the models we have seen so far are **parametric** - the number of parameters in the model does not depend on the number of data points. Parametric models are relatively straightforward to simplify by reducing the number of derived or observed features through linear combinations.\n",
    "\n",
    "Another possibility is to use features that are derived from the data points themselves. We will consider two possibilities:\n",
    "\n",
    "* Piecewise polynomials (or splines)\n",
    "* Gaussian kernels\n",
    "\n",
    "These are **non-parametric** models, and are much more susceptible to over-fitting. Simplifying them by removing feature terms is also not so intuitive, since it is equivalent to dropping specific data points. This is typically overcome by **regularizing** the coefficients of the model by including them in the loss function.\n",
    "\n",
    "We will return to our original example datasets to illsutrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/three_regression_examples.csv')\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(df['0'], df['1'])\n",
    "axes[1].scatter(df['0'], df['2'])\n",
    "axes[2].scatter(df['0'], df['3'])\n",
    "\n",
    "x = df['0'].values\n",
    "y1 = df['1'].values\n",
    "y2 = df['2'].values\n",
    "y3 = df['3'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise polynomials\n",
    "\n",
    "Consider \"features\" of the form:\n",
    "\n",
    "$x^{(k)} = max(0, x-x_k)^n$\n",
    "\n",
    "where $x_k$ are the input data points and $n$ is the order of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_polynomial_kernel(x, order, x_dense=None):\n",
    "    if x_dense is None:\n",
    "        x_dense = x #<- we will want to project this basis on to a finer grid sometimes.\n",
    "    features = []\n",
    "    for xi in x:\n",
    "        features.append(np.array([max(0,xj-xi)**order for xj in x_dense]))\n",
    "    return np.array(features).T\n",
    "\n",
    "X_pieces = piecewise_polynomial_kernel(x,1)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_pieces.shape[1]):\n",
    "    ax.plot(x,X_pieces[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is to use Gaussian kernels centered at each data point:\n",
    "\n",
    "$x^{(k)} = \\exp{\\left(-\\frac{(x-x_k)^2}{2\\sigma^2}\\right)}$\n",
    "\n",
    "Note that if $k=N$ and the data points are evenly spaced then this is equivalent to the Gaussian features from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, sigma = 1, x_dense=None):\n",
    "    # x is a vector\n",
    "    # sigma is the standard deviation\n",
    "    if x_dense is None:\n",
    "        x_dense = x #<- we will want to project this basis on to a finer grid sometimes.\n",
    "    features = []\n",
    "    for xk in x: #<- compare this to the function for parametric Gaussian basis\n",
    "        features.append(np.exp(-((x_dense - xk)**2/(2*sigma**2))))\n",
    "    return np.array(features).T\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),1000)\n",
    "#x_dense = x\n",
    "X_kernel = gaussian_kernel(x,0.5,x_dense)\n",
    "fig,ax = plt.subplots()\n",
    "for col in range(X_kernel.shape[1]):\n",
    "    ax.plot(x_dense,X_kernel[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "#plot the original data\n",
    "fig, axes = plt.subplots(1,3, figsize=(10, 4))\n",
    "axes[0].scatter(x, y1)\n",
    "axes[1].scatter(x, y2)\n",
    "axes[2].scatter(x, y3)\n",
    "\n",
    "# Generate features\n",
    "N_poly = 1\n",
    "X_poly = piecewise_polynomial_kernel(x,N_poly)\n",
    "sigma = 0.5\n",
    "X_gauss = gaussian_kernel(x,sigma)\n",
    "\n",
    "# Iterate through feature options\n",
    "X_features = [X_poly, X_gauss]\n",
    "\n",
    "# Create \"dense\" versions for predictions\n",
    "x_dense = np.linspace(min(x),max(x),1000)\n",
    "dense_features = [piecewise_polynomial_kernel(x, N_poly, x_dense), gaussian_kernel(x, sigma, x_dense)]\n",
    "feature_functions = [piecewise_polynomial_kernel,  gaussian_kernel]\n",
    "\n",
    "colors = ['r','b']\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "for yi, ax in zip([y1, y2, y3], axes):\n",
    "    for X, X_dense, color in zip(X_features,dense_features,colors):#<- useful way to iterate through synced lists\n",
    "        lr.fit(X, yi)\n",
    "        yhat = lr.predict(X_dense)\n",
    "        ax.plot(x_dense, yhat, color=color, ls='-')\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These \"kernel\" functions are very good at interpolating, but terrible at extrapolating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What happens if `N_poly` is increased or `sigma` is decreased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient values for these non-parametric models can also span a huge range, especially if the model is substantially over-fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = piecewise_polynomial_kernel(x,3)\n",
    "lr.fit(X_poly,y1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the features are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X_poly - X_poly.mean(axis=0))/X_poly.std(axis=0)\n",
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the magnitude of the parameters corresponds to the \"smoothness\" of the function. We can see this if we take the derivative of the basis functions, or empirically by playing with the parameters above.\n",
    "\n",
    "This is the basis of **regularization** as a route to control co-linearity and model complexity.\n",
    "\n",
    "Recall that least-square regression uses the sum of squared errors as the objective function:\n",
    "\n",
    "$min(\\sum_i \\epsilon_i^2)$ with respect to parameters $\\beta$. This is achieved by setting the derivative equal to zero:\n",
    "\n",
    "$\\frac{\\sum_i \\epsilon_i^2}{\\partial \\vec{\\beta}}=0$\n",
    "\n",
    "In a **regularized linear model** we include a **penalty** term on the magnitude of the coefficients in the loss function:\n",
    "\n",
    "$min(\\sum_i \\epsilon_i^2 + \\lambda || \\vec{\\beta} ||)$\n",
    "\n",
    "where $\\lambda$ is a regularization strength hyperparameter, $||.||$ is a \"norm\". The most common types of regularization are **ridge regression** which uses an $L_2$ norm ($\\sqrt{\\sum_i \\beta_i^2}$), **LASSO regression** which uses an $L_1$ norm ($\\sum_i | \\beta_i |$), and **elastic net** regression which uses a combination.\n",
    "\n",
    "We will revisit these in more detail, but for now you can remember the following:\n",
    "\n",
    "* Ridge regression: mininizes the magnitude of the coefficients, but not the number of coefficients. It is very stable, but does not reduce the number of features needed.\n",
    "\n",
    "* LASSO regression: minimizes both magnitude and number of coefficients. However, it can be unstable if features are very co-linear.\n",
    "\n",
    "* Elastic net regression: provides a flexible tradeoff between ridge and LASSO, but uses an additional hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel ridge regression (KRR)\n",
    "\n",
    "Kernel ridge regression is one of the most common regression models in machine learning. It is a **non-parametric** model that uses a \"kernel\" (typically Gaussian) at each training data point, and uses **regularization** through ridge regression to control the complexity of the model.\n",
    "\n",
    "We have already seen Gaussian kernels, and will combine this with ridge regression from `scikit-learn` to implement the model.\n",
    "\n",
    "We will not derive the equations for KRR, but **you should be able to derive them following the same procedure that was used for least-squares**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on a single dataset to illustrate the technique, and in order to test the model we will need to use **cross validation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.3)\n",
    "\n",
    "sigma = 1\n",
    "\n",
    "axes[0].scatter(x_train,y_train,color='b')\n",
    "axes[0].scatter(x_test,y_test,color='r')\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "X_kernel_dense = gaussian_kernel(x_train,sigma,x_dense) #<- Only TRAINING POINTS are included in the feature set!\n",
    "\n",
    "for col in range(X_kernel_dense.shape[1]):\n",
    "    axes[1].plot(x_dense,X_kernel_dense[:,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha = 0.01) #<- the regularization strength is called \"alpha\" in scikit-learn (instead of lambda)\n",
    "sigma = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_kernel =  gaussian_kernel(x_train,sigma)\n",
    "X_kernel_test = gaussian_kernel(x_train, sigma, x_test) #<- make sure you understand why x_test used as x_dense instead of x?\n",
    "X_kernel_dense = gaussian_kernel(x_train,sigma,x_dense)\n",
    "\n",
    "ridge.fit(X_kernel, y_train)\n",
    "yhat = ridge.predict(X_kernel_dense)\n",
    "y_predict = ridge.predict(X_kernel_test)\n",
    "\n",
    "ax.plot(x_dense, yhat, color=color, ls='-')\n",
    "\n",
    "ax.scatter(x_train,y_train,color='b')\n",
    "ax.scatter(x_test,y_test,color='r')\n",
    "\n",
    "prediction_error = np.mean(np.abs(y_test - y_predict))\n",
    "print('MAE',prediction_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How many hyper-parameters are in this model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cross-validation to optimize the hyperparameters, but it will be a lot easier if we just use the `scikit-learn` KRR implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.3)\n",
    "\n",
    "min_error = 1e99\n",
    "\n",
    "for alpha in [0, 0.01, 0.1, 0.5, 1, 10]:\n",
    "    for sigma in [0.05, 0.25, 0.5, 0.75, 1, 2, 4]:\n",
    "        gamma = 1./sigma #<- scikit learn uses the inverse of sigma\n",
    "        KRR = KernelRidge(alpha=alpha, gamma=gamma, kernel='rbf') #<- the \"rbf\" kernel is the same as a Gaussian kernel\n",
    "        X = x_train.reshape(x_train.size,1) #<- must be a column vector\n",
    "        KRR.fit(X,y_train)\n",
    "        X_test = x_test.reshape(x_test.size,1)\n",
    "        ypredict = KRR.predict(X_test)\n",
    "        prediction_error = np.mean(np.abs(y_test - ypredict))\n",
    "        if prediction_error < min_error:\n",
    "            best = [alpha, sigma, gamma, prediction_error]\n",
    "            min_error = prediction_error\n",
    "\n",
    "print(best)\n",
    "alpha, sigma, gamma,  err = best\n",
    "fig, ax = plt.subplots()\n",
    "KRR = KernelRidge(alpha=alpha, gamma=gamma, kernel='rbf')\n",
    "LR = KernelRidge(alpha=0,gamma=gamma, kernel='rbf') #<- compare regularized to non-regularized\n",
    "\n",
    "X = x_train.reshape(x_train.size,1) #<- must be a column vector\n",
    "KRR.fit(X,y_train)\n",
    "LR.fit(X,y_train)\n",
    "\n",
    "X_test = x_test.reshape(x_test.size,1)\n",
    "ypredict = KRR.predict(X_test)\n",
    "\n",
    "\n",
    "yhat = KRR.predict(x_dense.reshape(x_dense.size,1))\n",
    "ynoreg = LR.predict(x_dense.reshape(x_dense.size,1))\n",
    "\n",
    "ax.plot(x_dense, yhat, color='b', ls='-')\n",
    "ax.plot(x_dense, ynoreg, color='k', ls='-', alpha=0.5)\n",
    "\n",
    "ax.scatter(x_train,y_train,color='b')\n",
    "ax.scatter(x_test,y_test,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the block above a few times and see what happens. When does regularization help? When is it unnecessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approaches to regression\n",
    "\n",
    "The approach of generating non-linear features and fitting a linear model is extremely powerful, and forms the basis of many approaches to regression. However, there are also a number of other approaches that involve non-linear (non-convex) optimization including neural networks, and techniques that use discrete data representations such as kNN and decision tree regression. This lecture briefly provides demonstrations of a few approaches, though the technical details will not be covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear regression\n",
    "\n",
    "Non-linear regression involves optimizing an objective function using an iterative optimization algorithm rather than the direct solution that is obtained from the \"linear\" (convex) models. Non-linear models can take two forms:\n",
    "\n",
    "* Pre-defined models with fixed form\n",
    "    - Example: fitting standard deviation of Gaussians\n",
    "\n",
    "* Flexible models with varying complexity\n",
    "    - Example: neural networks\n",
    "\n",
    "The pre-defined models are common in the case where the behavior of a function is known, but its parameters must be determined, while flexible models are \"black boxes\" that can represent the behavior of any function if enough terms are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by revisiting the case of fitting a combination of Gaussians, although here we will fit the mean and standard deviation in addition to the coefficient.\n",
    "\n",
    "If we look at dataset 1 we can see that it looks like it may be well-represented by 3 Gaussian peaks. We can guess the positions and standard deviations, but they won't be perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "\n",
    "axes[0].scatter(x,y1)\n",
    "\n",
    "def arb_gaussian_features(x, mu_sigma_list):\n",
    "    # x is a vector\n",
    "    # sigma_mu_list is a list of [(mu1, sigma1), (mu2, sigma2), ...]\n",
    "    features = []\n",
    "    for mu,sigma in mu_sigma_list:\n",
    "        features.append(np.exp(-((x - mu)**2/(2*sigma**2))))\n",
    "    return np.array(features).T\n",
    "\n",
    "guesses = [(3,1),(6,0.5),(8,0.3)]\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "X_dense = arb_gaussian_features(x_dense, guesses)\n",
    "\n",
    "for col in range(X_dense.shape[1]):\n",
    "    axes[1].plot(x_dense,X_dense[:,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use linear regression to see how well this feature set performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = arb_gaussian_features(x, guesses)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,y1)\n",
    "yhat = lr.predict(X_dense)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y1)\n",
    "ax.plot(x_dense, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so good. We can improve things by playing with the guesses from above, but this isn't very efficient. A better approach is to optimize the means and standard deviations. This is an example of **non-linear regression** with a fixed functional form (sum of 3 Gaussians).\n",
    "\n",
    "We can use `scipy.optimize.minimize` to achieve this, but we need to set up an **objective function** to minimize, and re-structure our functions so that they work with the syntax of `minimize`.\n",
    "\n",
    "We will use least-squares non-linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(beta_mu_sigma_list, x=x, y=y1):\n",
    "    # beta_mu_sigma_list is a long list of [beta0, mu0, sigma0, beta1, mu1, sigma1, ...]\n",
    "    # the optional arguments x and y provide the real data\n",
    "    \n",
    "    #first we need to construct a mu_sigma list to pass to the `arb_gaussian_features` function:\n",
    "    beta = beta_mu_sigma_list[::3]\n",
    "    mus = beta_mu_sigma_list[1::3] #<- use the \"stride\" functionality to do this efficiently!\n",
    "    sigmas = beta_mu_sigma_list[2::3]\n",
    "    mu_sigma_list = list(zip(mus,sigmas)) #<- the \"zip\" function is very useful here! (note it must be converted to a list though)\n",
    "    # print(mu_sigma_list) #<- uncomment to convince yourself it worked!\n",
    "    \n",
    "    #now we can use the \"arb_gaussian_features\" function to generate X\n",
    "\n",
    "    X = arb_gaussian_features(x, mu_sigma_list)\n",
    "    \n",
    "    #now we can make a prediction using beta*X\n",
    "    yhat = np.dot(X,beta)\n",
    "    \n",
    "    #now we will take the sum of squared errors:\n",
    "    SSE = np.sum((y - yhat)**2)\n",
    "    \n",
    "    return SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "beta_mu_sigma_list = np.ones(9) #<- let's assume everything is 1\n",
    "#beta_mu_sigma_list = [1, 3, 1, 1, 6, 0.5, 1, 8, 0.3]\n",
    "\n",
    "objective(beta_mu_sigma_list)\n",
    "\n",
    "result = minimize(objective, beta_mu_sigma_list)\n",
    "#print(result)\n",
    "\n",
    "beta_mu_sigma_list = result.x\n",
    "beta = beta_mu_sigma_list[::3]\n",
    "mus = beta_mu_sigma_list[1::3] #<- use the \"stride\" functionality to do this efficiently!\n",
    "sigmas = beta_mu_sigma_list[2::3]\n",
    "mu_sigma_list = list(zip(mus,sigmas))\n",
    "\n",
    "X_dense = arb_gaussian_features(x_dense, mu_sigma_list)\n",
    "yhat = np.dot(X_dense,beta)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x,y1)\n",
    "ax.plot(x_dense, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Was the optimization successful? How can we improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear optimization with a fixed form is powerful if you have a good idea of the behavior of the function. However, there is no unique solution! The results you get will depend on the quality of your initial guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks\n",
    "\n",
    "Neural networks are a field of research on their own, and are beyond the scope of the course. However, they are a type of flexibile non-linear regression model of the form:\n",
    "\n",
    "$y(x) = \\sigma(\\sum_l W_l^{(1)} \\sigma(\\sum_k W_k^{(0)}x + b^{0}) + b^{(1)})$\n",
    "\n",
    "The function $\\sigma$ is the \"activation function\", the parameters $W_k^{(n)}$ are the \"weights\", and $b_k^{(n)}$ are \"biases\" for layer $n$. The inner layer is the \"hidden layer\", and the outer layer is the \"output layer\". Adding more hidden layers corresponds to nesting more functions.\n",
    "\n",
    "These networks are often represented as diagrams, since this formula is complex.\n",
    "\n",
    "<img src=\"images/ANN.png\" width=\"300\">\n",
    "\n",
    "One major challenge with neural networks is that they have a lot of hyperparameters:\n",
    "\n",
    "* activation function type ($\\sigma$)\n",
    "    - relu (renormalized linear unit)\n",
    "    - tanh\n",
    "    - sigmoid: ($1/(1+exp(-x)$)\n",
    "    - swish: ($x/(1+exp(-\\beta x))$)\n",
    "    \n",
    "* number of hidden layers\n",
    "\n",
    "* number of nodes\n",
    "\n",
    "Neural networks are also non-linear, so the solution will ultimately depend on the initial guesses for the \"weights\", $W$. Getting good initial guesses and optimizing all of these hyperparameters without over-fitting the model represents a major challenge. However, neural networks are powerful because of the [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) that proves that with enough nodes (even with a single hidden layer) a neural network can represent any function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN regression\n",
    "\n",
    "Another strategy for regression is to convert \"discrete\" models that are typically used for classification into regression models. An example of this is k-nearest neighbors regression. The algorithm is identical to kNN for classification, except that instead of having neighbors \"vote\" on a class, neighboring values are averaged through some weighting scheme. We will not explore this in detail, but can quickly demonstrate it for our example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "kNR = KNeighborsRegressor(n_neighbors=2, weights='uniform') #<- use a uniform averaging scheme\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.3) #<- remember that we have to split the data for kNN models!\n",
    "\n",
    "ax.scatter(x_train,y_train,color='b')\n",
    "ax.scatter(x_test,y_test,color='r')\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "kNR.fit(x_train.reshape(x_train.size,1), y_train.reshape(y_train.size,1))\n",
    "y_predict = kNR.predict(x_dense.reshape(x_dense.size,1))\n",
    "\n",
    "ax.plot(x_dense, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying uncertainty\n",
    "\n",
    "There are almost always limits to how accurate a regression model (or any machine-learning model) can be. In engineering applications (and others) it is often important to understand and quantify the accuracy of these models to ensure that they are used to make safe and reliable decisions. Uncertainty quantification is a complex topic, but there are a few simple techniques that can help provide estimates.\n",
    "\n",
    "**Note**: The uncertainty predicted by these models is not always rigorous. In practice you may need to think carefully about what error bars mean, and always be conservative with machine-learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation of error\n",
    "\n",
    "The simplest way of quantifying error is to rely on the normally-distributed, homoskedastic assumption of least-squares regression. In this case quantifying error is as simple as computing the standard deviation of the error distribution (accounting for the degrees of freedom, or number of parameters, in the regression model in the denominator to provide an unbiased estimate). \n",
    "\n",
    "Let's take Anscomb's quartet as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\n",
    "y1 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "y2 = np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])\n",
    "x4 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])\n",
    "\n",
    "m, b = np.polyfit(x,y1,deg=1) #<- it doesn't matter which dataset we choose since m, b, SSE, and R^2 are the same for all!\n",
    "yhat = m*x + b\n",
    "ybar = np.mean(y1)\n",
    "error_stdev = np.std(y1 - yhat, ddof=2)\n",
    "print(error_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify this as a function of `x` to account for the fact that the model is slightly better determined toward the middle of the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_error(x,y, x_data, yhat):\n",
    "    sigma_error = np.std(y-yhat, ddof=2)\n",
    "    xbar = np.mean(x_data)\n",
    "    y_error = sigma_error * np.sqrt(1 + 1/len(y) + ((x-xbar)**2)/(np.sum((x_data-xbar)**2)))\n",
    "    return y_error\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x4),50)\n",
    "\n",
    "y_error = regression_error(x_dense,y1, x, m*x+b)\n",
    "\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "\n",
    "axes[0].scatter(x,y1)\n",
    "axes[1].scatter(x,y2)\n",
    "axes[2].scatter(x,y3)\n",
    "axes[3].scatter(x4,y4)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x_dense, m*x_dense+b)\n",
    "    ax.plot(x_dense, m*x_dense+b + y_error, ls='--', color='0.5')\n",
    "    ax.plot(x_dense, m*x_dense+b - y_error, ls='--', color='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some curvature to the error bars, but not much unless we go to extreme values of `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles from re-sampling\n",
    "\n",
    "Another possibility that avoids the assumption of homoskedastic and normally-distributed errors is to use resampling techniques to generate a distribution of models. These models have distributions of parameters that capture the deviations in the data\n",
    "\n",
    "There are many ways to achieve this, but one of the most popular is \"bootstrapping\". In a bootstrapping approach the data is re-sampled by choosing the same number of points `N` randomly from the real dataset, but this is done **with replacement** so that each re-sample is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice #<- randomly select items from a list\n",
    "\n",
    "def bootstrap_linregress(x_all,y_all,N):\n",
    "    m_list = []\n",
    "    b_list = []\n",
    "    for n in range(N):\n",
    "        subset = choice(range(len(x_all)),size=len(x_all),replace=True)\n",
    "        xprime = [x_all[j] for j in subset]\n",
    "        yprime = [y_all[j] for j in subset]\n",
    "        m, b = np.polyfit(xprime,yprime,deg=1)\n",
    "        m_list.append(m)\n",
    "        b_list.append(b)\n",
    "    return m_list, b_list\n",
    "\n",
    "anscombs = [[x,y1],[x,y2],[x,y3],[x4,y4]]\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "fig_m, axes_m = plt.subplots(1,4,figsize=(15,4))\n",
    "fig_b, axes_b = plt.subplots(1,4,figsize=(15,4))\n",
    "\n",
    "N = 100\n",
    "\n",
    "for i, xy in enumerate(anscombs):\n",
    "    xi, yi = xy\n",
    "    m, b = np.polyfit(xi,yi,deg=1)\n",
    "    axes[i].scatter(xi,yi, color = 'r')\n",
    "    axes[i].plot(xi,m*xi+b, color='k', lw=2)\n",
    "\n",
    "    \n",
    "    m_list, b_list = bootstrap_linregress(xi,yi,N)\n",
    "    for mj, bj in zip(m_list,b_list):\n",
    "        axes[i].plot(xi, mj*xi+bj, color='k', alpha=0.05)\n",
    "        \n",
    "    axes_m[i].hist(m_list)\n",
    "    axes_b[i].hist(b_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the distribution of parameters for each model to get a feeling for whether there are outliers, and to estimate errors on the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian process regression\n",
    "\n",
    "Gaussian process regression is an extension of kernel ridge regression that uses the distance of prediction points from the training points to estimate errors. The math behind this is beyond the scope of this course, but we will briefly demonstrate it for the same dataset that we showed KRR for earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/three_regression_examples.csv')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = df['0'].values\n",
    "x = x.reshape(x.size,1)\n",
    "y1 = df['1'].values\n",
    "y1 = y1.reshape(y1.size,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.3)\n",
    "\n",
    "ax.scatter(x_train,y_train,color='b')\n",
    "ax.scatter(x_test,y_test,color='r')\n",
    "\n",
    "gpr = GaussianProcessRegressor()\n",
    "\n",
    "gpr.fit(x_train,y_train)\n",
    "\n",
    "x_dense = np.linspace(min(x),max(x),100)\n",
    "x_dense = x_dense.reshape(x_dense.size,1)\n",
    "y_gpr, y_std = gpr.predict(x_dense, return_std=True)\n",
    "\n",
    "\n",
    "ax.plot(x_dense, y_gpr, color='k')\n",
    "ax.fill_between(x_dense[:,0], y_gpr[:,0] - y_std, y_gpr[:,0] + y_std, color='k',alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [Hastie Elements of Statistical Learning Ch. 5](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "* [Python data science handbook linear regression](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb)\n",
    "* [Raschka regression discussion](https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html)\n",
    "* [Scikit-learn PLS regression](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html)\n",
    "* [Scikit=learn linear models](http://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
