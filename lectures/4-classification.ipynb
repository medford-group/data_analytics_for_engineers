{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classification\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"images/linearly_separable.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will discuss basic types of classification algorithms and considerations for assessing and improving their accuracy. The focus will be on generalized linear models for discrimination of datapoints, and several other types of classification will be briefly explored.\n",
    "\n",
    "* Classification basics\n",
    "    - Problem statement for classification\n",
    "    - Types of classification problems\n",
    "    - Accuracy metrics\n",
    "\n",
    "* Generalized linear models\n",
    "    - Perceptron\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Kernel SVMs\n",
    "\n",
    "* Other classification models\n",
    "    - Linear Discriminant Analysis\n",
    "    - Naive Bayes\n",
    "    - k-Nearest Neighbors\n",
    "    - Decision trees and random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement for classification\n",
    "\n",
    "A model that maps continuous or discrete inputs to a discrete (ordinal, categorical, or integer) output space.\n",
    "\n",
    "There are two basic approaches to this problem:\n",
    "\n",
    "#### Discriminative models:\n",
    "\n",
    "These models are most similar to regression. Rather than learning a line\\model that best represents the data we want to learn a line\\model that best separates (discriminates) between different classes. For a binary classifier we can write this as:\n",
    "\n",
    "$f(\\vec{x}) > p$ if class 1\n",
    "\n",
    "$f(\\vec{x}) < p$ if class 2\n",
    "\n",
    "where $p$ is some constant threshold that separates the classes.\n",
    "\n",
    "Another way to think of this is that we will establish a function that estimates the probability of a point belonging to a particular class, given its features:\n",
    "\n",
    "$P(y_i|\\vec{x}) = f(\\vec{x})$\n",
    "\n",
    "Then the classes can be discretized by establishing probability cutoffs. These models **directly** solve the problem of estimating class probability.\n",
    "\n",
    "#### Generative models\n",
    "\n",
    "Generative models are somewhat less intuitive, but can be very powerful. In a generative model the goal is to solve the \"inverse problem\" of predicting the probability of features given a class label output. Mathematically:\n",
    "\n",
    "$P(\\vec{x}|y_i) = f(\\vec{x})$\n",
    "\n",
    "This is is counter-intuitive, but the model can then be used in conjunction with Bayes' rule to **indirectly** solve the classification problem. Bayes rule is:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\rightarrow P(y_i|\\vec{x}) = \\frac{P(\\vec{x}|y_i) P(y_i)}{P(\\vec{x})}$\n",
    "\n",
    "The $P(y_i)$ term is available from the data (number of times each class appears) and the $P(\\vec{x})$ term is a constant so it can be dropped when computing relative probabilities.\n",
    "\n",
    "Generative models are more difficult to understand, but they have a key advantage: new synthetic data can be generated by using the function $P(\\vec{x}|y_i)$. This opens the possibility of iterative training schemes that systematically improve the estimate of $P(\\vec{x}|y_i)$ (e.g. Generative Artificial Neural Networks) and can also aid in diagnosing problems in models. We will only briefly discuss generative models in this lecture.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"images/discriminative_vs_generative.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different types/features of classification problems:\n",
    "\n",
    "* **Linearly separable**: A problem where it is possible to exactly separate the classes with a straight line (or plane) in the feature space.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/linearly_separable.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "* **Binary vs. Multi-class**: A binary classification problem has only 2 classes, while a multi-class problem has more than 2 classes. \n",
    "\n",
    "There are two approaches to dealing with multi-class problems:\n",
    "\n",
    "1) Convert multi-class problems to binary problems using a series of \"one vs. the rest\" binary classifiers\n",
    "\n",
    "<center>\n",
    "<img src=\"images/OvA.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "2) Consider the multi-class nature of the problem when deriving the method (e.g. kNN) or determining the cost function (e.g. logistic regression)\n",
    "\n",
    "<center>\n",
    "<img src=\"images/multiclass_cost.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "In the end, the difference between these approaches tend to be relatively minor, although the training procedures can be quite different. One vs. the rest is more efficient for parallel training, while multi-class objective functions are more efficient in serial.\n",
    "\n",
    "* **Balanced vs. Imbalanced**: A balanced problem has roughly equal numbers of examples in all classes, while an imbalanced problem has an (typically significantly) higher number of examples of some classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few basic strategies to overcome class imbalance:\n",
    "\n",
    "1) Re-balancing the cost function to penalize mis-classification of the under-represented class more.\n",
    "\n",
    "2) Undersampling: discarding information from over-represented class. This is inefficient since not all information is used.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/class_imbalance.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "\n",
    "3) Oversampling: add repeates of the under-represented class (very similar to re-balancing the cost function). This can lead to over-fitting of the decision boundary to the few examples of the under-represented class.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/oversampling.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "4) Resampling: Re-sample from the under-represented class, but add some noise. This is a robust solution, but requires some knowledge of the distribution of the under-represented data (e.g. generative models) or special techniques (e.g. SMOTE).\n",
    "\n",
    "<center>\n",
    "<img src=\"images/smote.png\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positives and negatives\n",
    "\n",
    "* Accuracy = (number correct)/(total) = (TP + TN)/(TP + TN + FP + FN)\n",
    "\n",
    "* Precision = TP/(TP + FP)\n",
    "\n",
    "* Recall = TP/(TP + FN)\n",
    "\n",
    "<center>\n",
    "<img src=\"images/precision_recall.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "#### Confusion matrices\n",
    "\n",
    "False positives and false negatives only apply to binary problems. The \"confusion matrix\" is a multi-class generalization of the concept, and can help identify which classes are \"confusing\" the algorithm.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/confusion_matrix.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "#### ROC curves\n",
    "\n",
    "The \"receiver operating characteristic\", or ROC curve, is useful for models where a threshold is used. The area under the curve can be used as a metric for how well the model performs.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/ROC_curve.jpg\" width=\"300\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work through some math and examine a toy dataset for \"generalized linear\" classification models. The toy dataset is simply two \"blobs\" that we will randomly generate using `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, n_features=2, random_state=0)\n",
    "\n",
    "\n",
    "#rescale\n",
    "y_new = []\n",
    "for yi in y:\n",
    "    if yi == 0:\n",
    "        y_new.append(-1)\n",
    "    else:\n",
    "        y_new.append(1)\n",
    "y = np.array(y_new).reshape(y.shape)\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"perceptron\"\n",
    "\n",
    "We will start by considering a discrimination problem:\n",
    "\n",
    "$f(\\vec{x}) > p$ if class 1\n",
    "\n",
    "$f(\\vec{x}) < p$ if class 2\n",
    "\n",
    "and let $f(\\vec{x}) = \\underline{\\underline{X}}\\vec{\\beta}$, where $\\underline{\\underline{X}} = [\\vec{x}, \\vec{1}]$ similar to linear regression.\n",
    "\n",
    "We can use $y$ as the output variable and arbitrarily assign \"class 1\" to 1 and \"class 2\" to -1, such that $p = 0$.\n",
    "\n",
    "$\\underline{\\underline{X}}\\vec{\\beta} > 0$ if $y_i=1$ (class 1)\n",
    "\n",
    "$\\underline{\\underline{X}}\\vec{\\beta} < 0$ if $y_i=-1$ (class 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.linalg  import solve\n",
    "\n",
    "def add_intercept(X):\n",
    "    intercept = np.ones((X.shape[0],1))\n",
    "    X_intercept = np.append(intercept,X,1)\n",
    "    return X_intercept\n",
    "\n",
    "def linear_classifier(X,beta):\n",
    "    X_intercept = add_intercept(X)\n",
    "    p = np.dot(X_intercept,beta)\n",
    "    return p > 0\n",
    "    \n",
    "beta = np.array([0,3.5,-6])\n",
    "prediction = linear_classifier(X,beta)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=prediction)\n",
    "\n",
    "\n",
    "#plot line\n",
    "m = -beta[1]/beta[2]\n",
    "b = -beta[0]/beta[2]\n",
    "axes[1].plot(X[:,0], m*X[:,0]+b, ls='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation: Find the equation for the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot like linear regression, but we still need an **objective function**. This is where things get tricky. Based on the definition of $\\pm$1 for classes, we can re-write this as a single line:\n",
    "\n",
    "$-y_i \\underline{\\underline{X}}\\vec{\\beta} < 0$\n",
    "\n",
    "Convince yourself that this is true!\n",
    "\n",
    "Now we can turn this into an equality by taking the maximum:\n",
    "\n",
    "$max(0, -y_i \\underline{\\underline{X}}\\vec{\\beta}) = 0$\n",
    "\n",
    "Now we are getting close. If a point $y_i$ is mis-classified then this will give a positive value, but if it is correctly classified it will return zero. Therefore we can get a cost for the entire dataset by summing the function over all data points:\n",
    "\n",
    "$C(\\vec{\\beta}) = \\sum_i max(0, -y_i \\underline{\\underline{X}}\\vec{\\beta})$\n",
    "\n",
    "and we can find the optimal $\\vec{\\beta}$ by minimizing it with respect to $\\vec{\\beta}$\n",
    "\n",
    "This is the \"max cost\" function, often commonly referred to as the \"perceptron\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_cost(beta, X, y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,beta)\n",
    "    return sum(np.maximum(0, -y*Xb))\n",
    "\n",
    "print(max_cost(beta,X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What will the optimal cost be if a dataset is linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two obvious problems with the perceptron:\n",
    "\n",
    "1) There is a \"trivial solution\" at $\\vec{\\beta} = 0$\n",
    "\n",
    "2) The cost function is not differentiable at all points\n",
    "\n",
    "We can overcome the second problem by creating some smooth approximation of the maximum function. This is achieved using the \"softmax\" function:\n",
    "\n",
    "$max(x,y) \\approx soft(x,y) = log(exp(x) + exp(y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, np.maximum(0,x), ls='-',color='k')\n",
    "ax.plot(x, np.log(np.exp(0) + np.exp(x)), ls='--', color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this also gets rid of the \"trivial solution\" at $\\vec{\\beta}=0$, so our problems are solved!\n",
    "\n",
    "Now we can write a \"softmax\" cost function:\n",
    "\n",
    "$C_{softmax}(\\vec{\\beta}) = \\sum_i log\\left\\{1 + exp(-y_i \\underline{\\underline{X}}\\vec{\\beta})\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(beta, X, y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,beta)\n",
    "    exp_yXb = np.exp(-y*Xb)\n",
    "    return sum(np.log(1 + exp_yXb))\n",
    "\n",
    "print(softmax_cost(beta,X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is differentiable, so we can minimize this with respect to $\\beta$ by setting the derivative equal to zero and solving for $\\beta$:\n",
    "\n",
    "$\\frac{\\partial C_{softmax}}{\\partial \\vec{\\beta}} = 0$\n",
    "\n",
    "It turns out this problem is not fully linear, and needs to be solved iteratively using e.g. Newton's method. The math is a little more complex than before, so we won't cover it in lecture, but it is covered in Ch. 4 of \"Machine Learning Refined\" if you are interested. This approximation is called **logistic regression**.\n",
    "\n",
    "The key concept to understand is that $\\vec{\\beta}$ is determined by minimizing the softmax cost function. We can do this numerically for our toy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "result = minimize(softmax_cost, beta, args=(X,y))\n",
    "beta_opt = result.x\n",
    "print(softmax_cost(beta_opt,X,y))\n",
    "print(max_cost(beta_opt,X,y))\n",
    "\n",
    "\n",
    "prediction = linear_classifier(X,beta_opt)\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=prediction)\n",
    "\n",
    "\n",
    "#plot line\n",
    "m = -beta_opt[1]/beta_opt[2]\n",
    "b = -beta_opt[0]/beta_opt[2]\n",
    "axes[1].plot(X[:,0], m*X[:,0]+b, ls='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There are other ways to derive \"logistic regression\". See Ch. 4 of ML refined for an alternative derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Recall the two problems with the max cost function:\n",
    "\n",
    "1) There is a \"trivial solution\" at $\\vec{\\beta} = 0$\n",
    "\n",
    "2) The cost function is not differentiable at all points\n",
    "\n",
    "Logistic regression uses a smooth approximation of the maximum to ensure differentiability, and the \"trivial solution\" goes away as a side effect.\n",
    "\n",
    "An alternative approach is to directly eliminate the trivial solution by introducing a \"margin\" cost function, where we recognize that there will be some \"buffer zone\" between the classes:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/margin_cost.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "We can write this mathematically as:\n",
    "\n",
    "$\\underline{\\underline{X}}\\vec{\\beta} \\geq 1$ if $y_i=1$ (class 1)\n",
    "\n",
    "$\\underline{\\underline{X}}\\vec{\\beta} \\leq -1$ if $y_i=-1$ (class 2)\n",
    "\n",
    "by using the same trick of multiplying by $y_i$ and taking a maximum we can write this as an equality:\n",
    "\n",
    "$max(0, 1 -y_i \\underline{\\underline{X}}\\vec{\\beta}) = 0$\n",
    "\n",
    "and the corresponding cost/objective function:\n",
    "\n",
    "$C_{margin}(\\vec{\\beta}) = \\sum_i max(0, 1-y_i \\underline{\\underline{X}}\\vec{\\beta})$\n",
    "\n",
    "Note that this is very similar to the cost function for the perceptron, but now there is no trivial solution at $\\vec{\\beta} = 0$. However, we can solve this with a few approaches:\n",
    "\n",
    "1) Use derivative-free numerical approximations\n",
    "\n",
    "2) Replax $max$ with a differentiable function like $softmax$ or $max^2$\n",
    "\n",
    "Let's see what happens with strategy 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_cost(beta, X, y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,beta)\n",
    "    return sum(np.maximum(0, 1-y*Xb))\n",
    "\n",
    "print(margin_cost(beta,X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(margin_cost, beta, args=(X,y))\n",
    "beta_opt_margin = result.x\n",
    "print(margin_cost(beta_opt_margin,X,y))\n",
    "print(max_cost(beta_opt_margin,X,y))\n",
    "\n",
    "prediction = linear_classifier(X,beta_opt_margin)\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=prediction)\n",
    "\n",
    "#plot line\n",
    "m = -beta_opt_margin[1]/beta_opt_margin[2]\n",
    "b = -beta_opt_margin[0]/beta_opt_margin[2]\n",
    "axes[1].plot(X[:,0], m*X[:,0]+b, ls='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works, but we get a different solution from logistic regression. Let's see how this compares to the $max^2$ and $softmax$ approximations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_cost_squared(beta, X, y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,beta)\n",
    "    return sum(np.maximum(0, 1-y*Xb)**2)\n",
    "\n",
    "def margin_cost_softmax(beta, X, y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,beta)\n",
    "    exp_yXb = np.exp(1-y*Xb)\n",
    "    return sum(np.log(1 + exp_yXb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = minimize(margin_cost_squared, beta, args=(X,y))\n",
    "beta_opt_margin2 = result.x\n",
    "\n",
    "result = minimize(margin_cost_softmax, beta, args=(X,y))\n",
    "beta_opt_softmax = result.x\n",
    "\n",
    "prediction = linear_classifier(X,beta_opt)\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=prediction)\n",
    "\n",
    "#plot lines\n",
    "def plot_line(ax, color, beta, X):\n",
    "    m = -beta[1]/beta[2]\n",
    "    b = -beta[0]/beta[2]\n",
    "    ax.plot(X[:,0], m*X[:,0]+b, ls='-', color=color)\n",
    "\n",
    "for beta_i, color in zip([beta_opt,beta_opt_margin, beta_opt_margin2, beta_opt_softmax],['r','g','b','k']):\n",
    "    plot_line(axes[1], color, beta_i, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which line is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are infinitely many lines that have equal cost for a linearly-separable dataset. The line that you find will depend on the approximation used, and can also depend on the initial guesses for the parameter $\\beta$.\n",
    "\n",
    "The idea of a \"support vector machine\" is to regularize the cost function so that the \"margins\" that define the buffer zone are maximized. This is intuitive since it allows the classifier to remain accurate even as data falls outside the original bounds.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/margin_size.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "Through geometrical arguments it is possible to prove that the width of the margin is equal to $2/||\\vec{\\tilde{\\beta}}||_2$ where $\\vec{\\tilde{\\beta}}$ does not include the intercept and $||.||_2$ is the 2-norm (see Ch. 4 of ML refined).\n",
    "\n",
    "If we want to maximize the margin, then we want to minimize $||\\vec{\\beta}||_2$. We can achieve this by adding an $L_2$ penalty to the cost function:\n",
    "\n",
    "$C_{SVM}(\\vec{\\beta}) = \\sum_i max(0, 1-y_i \\underline{\\underline{X}}\\vec{\\beta}) + \\lambda ||\\vec{\\beta}||_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines are simply an $L_2$-regularized version of the margin cost function. In practice it is common to use a smooth approximation of the margin cost function, and it is relatively simple to show that if the \"softmax\" approximation is used the cost function becomes very similar to that of logistic regression.\n",
    "\n",
    "In practice, support-vector machines and $L_2$-regularized logistic regression are nearly indistinguishable and can be used interchangeably.\n",
    "\n",
    "Let's apply this to our example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(beta, X, y, cost_fn, lamda=1):\n",
    "    cost = cost_fn(beta,X,y)\n",
    "    cost += lamda*np.linalg.norm(beta,2)\n",
    "    return cost\n",
    "    \n",
    "result = minimize(regularized_cost, beta, args=(X,y, margin_cost))\n",
    "beta_SVM1 = result.x \n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X,y, margin_cost_softmax))\n",
    "beta_SVM2 = result.x\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X,y, margin_cost_squared))\n",
    "beta_SVM3 = result.x\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X,y, softmax_cost))\n",
    "beta_SVM4 = result.x\n",
    "\n",
    "prediction = linear_classifier(X,beta_SVM1)\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=prediction)\n",
    "\n",
    "for beta_i, color in zip([beta_SVM1,beta_SVM2, beta_SVM3, beta_SVM4],['r','g','b','k']):\n",
    "    plot_line(axes[1], color, beta_i, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some differences, but the results are now much closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Which cost function will be worst if there are known outliers (points far from the decision boundary)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized cousins of the perceptron all perform similarly well as long as the classes are **linearly separable**. However, what happens if this is not the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=0)\n",
    "\n",
    "\n",
    "#rescale\n",
    "y_new = []\n",
    "for yi in y:\n",
    "    if yi == 0:\n",
    "        y_new.append(-1)\n",
    "    else:\n",
    "        y_new.append(1)\n",
    "y = np.array(y_new).reshape(y.shape)\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we re-apply the SVM algorithm to this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(regularized_cost, beta, args=(X,y, margin_cost))\n",
    "beta_SVM1 = result.x \n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X,y, margin_cost_softmax))\n",
    "beta_SVM2 = result.x\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X,y, margin_cost_squared))\n",
    "beta_SVM3 = result.x\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X,y, softmax_cost))\n",
    "beta_SVM4 = result.x\n",
    "\n",
    "prediction = linear_classifier(X,beta_SVM1)\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=prediction)\n",
    "\n",
    "for beta_i, color in zip([beta_SVM1,beta_SVM2, beta_SVM3, beta_SVM4],['r','g','b','k']):\n",
    "    plot_line(axes[1], color, beta_i, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variants of the cost function fail miserably! This is because the data is not linearly separable. We can solve this using the same trick as for generalized linear regression: create new non-linear features from the original data.\n",
    "\n",
    "For example, in this problem let's add a new feature that is a Gaussian distribution centered at (0,0) with a standard deviation of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gauss = np.exp(-(X ** 2).sum(axis=1))\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X_gauss,c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add this as a feature and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tot = np.append(X, X_gauss.reshape(X_gauss.size,1), axis=1)\n",
    "print(X_tot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.ones(4)\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X_tot,y, margin_cost))\n",
    "beta_SVM1 = result.x \n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X_tot,y, margin_cost_softmax))\n",
    "beta_SVM2 = result.x\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X_tot,y, margin_cost_squared))\n",
    "beta_SVM3 = result.x\n",
    "\n",
    "result = minimize(regularized_cost, beta, args=(X_tot,y, softmax_cost))\n",
    "beta_SVM4 = result.x\n",
    "\n",
    "prediction = linear_classifier(X_tot,beta_SVM1)\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,6))\n",
    "axes[0].scatter(X_tot[:,0],X_tot[:,1],c=y)\n",
    "axes[1].scatter(X_tot[:,0],X_tot[:,2],c=y)\n",
    "axes[2].scatter(X_tot[:,0],X_tot[:,2],c=prediction)\n",
    "\n",
    "\n",
    "#plot lines\n",
    "def plot_line_3d(ax, color, beta, X):\n",
    "    m1 = -beta[1]/beta[3]\n",
    "    m2 = -beta[2]/beta[3]\n",
    "    b = -beta[0]/beta[3]\n",
    "    ax.plot(X[:,0], m1*X[:,1] + m2*X[:,2] + b, ls='-', color=color)\n",
    "\n",
    "\n",
    "for beta_i, color in zip([beta_SVM1,beta_SVM2, beta_SVM3, beta_SVM4],['r','g','b','k']):\n",
    "    plot_line_3d(axes[2], color, beta_i, X_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/kernel_schematic.png\" width=\"400\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did we know to center this new feature at 0? What is a more general strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with kernel ridge regression, we can put a Gaussian at every single point and use this as a basis. This is called a \"kernel SVM\" and it is the classification equivalent of ridge regression. Rather than implementing this we can use the `scikit-learn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.datasets.samples_generator import make_moons\n",
    "\n",
    "#X, y = make_circles(n_samples=100, factor=0.3, noise=0.1)\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "model = SVC(kernel='rbf', gamma=0.1, C=1e10)\n",
    "model.fit(X, y)\n",
    "y_predict = model.predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How many continuous hyperparameters does the kernel SVM have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the decision function to better understand what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    if plot_support:\n",
    "        # plot support vectors\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "               model.support_vectors_[:, 1],\n",
    "               s=300, linewidth=1, facecolors='none', edgecolors='k');\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points at the edge of the margin are called the \"support vectors\", hence the term \"support vector machine\". The inverse penalty cost, `C`, controls the \"hardness\" of the margins (harder margins = more overfitting), and the radial basis function kernel coefficient, `gamma` is inversely proportional to the width of the Gaussian kernels at each point.\n",
    "\n",
    "Play around with the different datasets (circles, moons), the noise level, and the hyper-parameters of the model to get a feel for how this works under various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other classification models\n",
    "\n",
    "We will briefly discuss a few additional classification models and their advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is a classifiction method similar to PCA. The key difference is that while PCA finds the axes of maximum variance for all points, LDA finds the axes of maximum variance *between clusters*.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/lda_vs_pca.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "LDA is an example of a **generative model**, despite the very confusing appearance of \"discriminant\" in its name. The reason is that LDA works by assuming that each class follows a Gaussian distribution with identical (co)variance. The parameters of the Gaussian distribution are estimated from the data, and used to transform each class into a standardized class with a uniform standard deviation of 1. These standardized classes are then treated as points and PCA is used to find the directions of maximum variance.\n",
    "\n",
    "LDA is similar to PCA and PLS since it provides dimensional reduction along with classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA steps:\n",
    "\n",
    "* Compute the centerpoint of each cluster\n",
    "* Compute intraclass covariance\n",
    "* Compute interclass covariance\n",
    "* Calculate the largest eigenvalues/eigenvectors of the composite covariance matrix\n",
    "* Use hyperplanes perpendicular to the linear discriminant vectors to assign classes\n",
    "\n",
    "We will step through this for a toy \"blobs\" dataset, then compare to the `scikit-learn` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Compute class centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, n_features=2, random_state=0)\n",
    "\n",
    "mean_vectors = []\n",
    "classes = [0,1]\n",
    "for cl in classes:\n",
    "    class_mean = np.mean(X[y==cl, :], axis=0)\n",
    "    mean_vectors.append(class_mean)\n",
    "    print('Mean Vector class {}: {}\\n'.format(cl, mean_vectors[cl]))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "for mv in mean_vectors:\n",
    "    ax.plot(mv[0],mv[1],marker='*',markersize=20, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute the intra-class (within class) covariance\n",
    "\n",
    "Remember the definition of co-variance from PCA:\n",
    "\n",
    "$\\Sigma = \\frac{1}{n-1} \\left( (\\mathbf{X} - \\mathbf{\\bar{x}})^T\\;(\\mathbf{X} - \\mathbf{\\bar{x}}) \\right)$ \n",
    "\n",
    "Now we want the co-variance of each class, centered on the mean of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_covs = []\n",
    "for cl, center in zip(classes, mean_vectors):\n",
    "    subX = X[y==cl] #select only points from class\n",
    "    subX_centered = subX - center\n",
    "    cov = np.dot(subX_centered.T, subX_centered)\n",
    "    cov = cov/(subX.size - 1)\n",
    "    class_covs.append(cov) \n",
    "    print(cov, '\\n')\n",
    "    \n",
    "print(len(class_covs))\n",
    "class_covs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA assumes that all classes have the **same covariance**, so we will take an average over them to compute the **intra-class covariance matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra = sum(class_covs)/len(class_covs)\n",
    "print(intra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute inter-class covariance\n",
    "\n",
    "Now we also need to know the covariance matrix between different class centers. We will use the class means as points to achive this, and we will save some time by using `numpy`'s built-in covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_array = np.array(mean_vectors)\n",
    "inter = np.cov(center_array.T)\n",
    "print(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Take the largest eigenvalues/vectors of the composite covariance\n",
    "\n",
    "We need to \"divide\" the inter-class (across class) covariance by the inter-class (within class) covariance. However, since we are dealing with matrices we use the inverse:\n",
    "\n",
    "$\\underline{\\underline{C}}_{composite} = \\underline{\\underline{C}}_{intra}^{-1} \\underline{\\underline{C}}_{inter}$\n",
    "\n",
    "This is called the \"composite covariance matrix\", and the linear discrimination axes can be determined by taking its eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = np.dot(np.linalg.inv(intra), inter)\n",
    "eig_vals, eig_vecs = np.linalg.eig(comp)\n",
    "eig_vecs = eig_vecs.T\n",
    "\n",
    "print(eig_vals)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], c=y)\n",
    "for mv in mean_vectors:\n",
    "    ax.plot(mv[0],mv[1],marker='*',markersize=20, color='r')\n",
    "    \n",
    "LDvec1 = eig_vecs[0]\n",
    "LDvec2 = eig_vecs[1]\n",
    "\n",
    "mu = np.mean(X, axis=0)\n",
    "\n",
    "ax.plot([mu[0]],[mu[1]], marker='x', color='k')\n",
    "\n",
    "#ax.plot([mu[0], mu[0] + LDvec1[0]*LDvals[0]],[mu[1],mu[1] + LDvec1[1]*LDvals[0]], ls='-', color='r', alpha=0.5) #<- plot eigenvectors scaled by eigenvalues\n",
    "ax.plot([mu[0] - LDvec2[0], mu[0] + LDvec2[0]],[mu[1] -  LDvec2[1],mu[1] + LDvec2[1]], ls='-', color='r', alpha=0.5)\n",
    "\n",
    "# compare to PCA\n",
    "cov_all = np.cov(X.T)\n",
    "PC_vals, PC_vecs = np.linalg.eig(cov_all)\n",
    "PC_vecs = PC_vecs.T\n",
    "\n",
    "PCvec1 = PC_vecs[0]\n",
    "PCvec2 = PC_vecs[1]\n",
    "\n",
    "ax.plot([mu[0] - PCvec2[0], mu[0] + PCvec2[0]],[mu[1] -  PCvec2[1],mu[1] + PCvec2[1]], ls='-', color='b', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Find the perpendicular vector to use as the decision boundary\n",
    "\n",
    "In LDA the decision boundary line or hyperplane is perpendicular to the LDA axis. We can find a perpendicular vector using the 90-degress rotation matrix:\n",
    "\n",
    "$[[0, -1] \\\\ [1, 0]]$\n",
    "\n",
    "In higher dimensions this can be achieved with cross-products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = np.dot([[0, -1],[1,0]],LDvec2)\n",
    "ax.plot([mu[0] - boundary[0], mu[0] + boundary[0]],[mu[1] -  boundary[1],mu[1] + boundary[1]], ls='-', color='r', alpha=0.5)\n",
    "\n",
    "#compare to PCA boundary\n",
    "\n",
    "PCboundary = np.dot([[0, -1],[1,0]],PCvec2)\n",
    "ax.plot([mu[0] - PCboundary[0], mu[0] + PCboundary[0]],[mu[1] -  PCboundary[1],mu[1] + PCboundary[1]], ls='-', color='b', alpha=0.5)\n",
    "\n",
    "#ax.set_xlim(-1,4)\n",
    "#ax.set_ylim(0,5)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign classes by determining if a point is above/below the boundary line. There is no formal way for determining the threshold, but we have assumed that it is in the middle here. More sophisticated implementations typically optimize for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Under what circumstances would the PCA and LDA vectors differ more substantially?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is much easier to just use the `scikit-learn` implementation of LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X, y = make_blobs(n_samples=200, centers=4, cluster_std=2, n_features=2)\n",
    "#X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X,y)\n",
    "y_pred = lda.predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,6))\n",
    "axes[0].scatter(X[:,0],X[:,1],c=y)\n",
    "axes[1].scatter(X[:,0],X[:,1],c=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some different datasets using the code block above and explore the performance of LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA summary:\n",
    "\n",
    "LDA is a *generative model* that is based on the following assumptions about the distributions of the data:\n",
    "* Each class is *normally distributed*\n",
    "* The covariance of the distribution for each class is equivalent\n",
    "\n",
    "**Advantages**\n",
    "* Similar to PCA (relatively easy to understand)\n",
    "* Easy to analyze the resulting low-dimensional space\n",
    "* Relatively robust even if assumptions are violated\n",
    "\n",
    "**Disadvantages**\n",
    "* Assumes indpendent, normally distributed data\n",
    "* Assumes identical covariance within each class\n",
    "* Boundaries between classes must be linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is another algorithm that is closely related to LDA. It also \"naively\" assumes that the data in each class is normally distributed. It also naively assumes that **features are not correlated** so that the covariance matrix of each class is diagonal. This allows the distribution of each class to be estimated by computing the standard deviation with respect to each feature.\n",
    "\n",
    "The assumed Gaussian distribution for each class gives the probability function for $y$ (class) as a function of $X$ (features):\n",
    "\n",
    "$P(\\vec{x}|y_i) \\sim \\exp\\left(\\sum_j \\frac{(x_j - \\mu_i)^2}{2 \\sigma_j^2}\\right)$\n",
    "\n",
    "This can be used with Bayes' formula to estimate $P(y_i|\\vec{x})$:\n",
    "\n",
    "$P(y_i|\\vec{x}) = \\frac{P(\\vec{x}|y_i) P(y_i)}{P(\\vec{x})}$\n",
    "\n",
    "We won't go deep into the theory of Naive Bayes here, but it is implemented in `scikit-learn` and generally performs similarly to LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "The kNN algorithm was covered in a previous lecture, but we will briefly review it here.\n",
    "\n",
    "The class of a point is determined by letting its k-nearest neighbors \"vote\" on which class it should be in. The point is assigned to whichever class has the most votes. In the case of a tie, `k` is decreased by 1 until the tie is broken.\n",
    "\n",
    "The advantage of democracy is that it is \"nonlinear\" - we can distinguish classes with very complex structures.\n",
    "\n",
    "We need 3 functions to implement kNN:\n",
    "\n",
    "* distance metric - calculate the distance between 2 points. We will use the Euclidean distance.\n",
    "* get neighbors - find the k points nearest to a given point.\n",
    "* assign class - poll the neighbors to assign the point to a class\n",
    "\n",
    "You should review topic 2 (Basic Concepts) for a detailed implementation of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees and random forests\n",
    "\n",
    "Decision trees are a very powerful type of **discriminative** classification algorithm, and they are relatively easy to interpret. They also have the advantage of working well with discrete input variables (e.g. discrete feature spaces). The disadvantage of decision trees is that they are very prone to over-fitting. The \"Random forest\" approach overcomes this by training an ensemble of decision trees with subsets of the data (similar to the \"bootstrapping\" we saw before) and using this ensemble of models to produce an estimate.\n",
    "\n",
    "We will not go into the theory of decision trees here, but we will show a brief example using the well-known iris classification data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris=load_iris()\n",
    "df=pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "dtree=DecisionTreeClassifier()\n",
    "dtree.fit(X,y)\n",
    "y_predict = dtree.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a \"confusion matrix\" to assess the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap #<- seaborn is a plotting library, and \"heatmap\" is very useful for confusion matrices\n",
    "\n",
    "confusion = confusion_matrix(y, y_predict)\n",
    "\n",
    "heatmap(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is perfect! We need to use a test/train split to avoid over-fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "dtree.fit(X_train,y_train)\n",
    "y_predict = dtree.predict(X_test)\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "heatmap(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance is still relatively good, but classes 1 and 2 are often confused.\n",
    "\n",
    "We can also use a visualization library to look at how the decision tree is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dtree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The criterion for each node is listed at the top of a block, the `value` category tells how many samples are sorted into each of the 3 categories, `samples` gives the total number of samples into a node, and the `gini` coefficient is a measure of class \"impurity\" related to how often a sample will be mis-categorized at a given node. \n",
    "\n",
    "Further analysis of decision trees is beyond the scope of this course, but they are very useful tools to know about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification conclusions\n",
    "\n",
    "* There are two basic types of classification model: Generative and discriminative.\n",
    "    - Discriminative models find a decision boundary based on differences between classes: P(y|X)\n",
    "    - Generative models consider the properties of the distribution that underlies each class: P(X|y)\n",
    "\n",
    "* When approaching a classification problem you should consider if it is **linearly separable**, **class imbalanced**, and **binary or multiclass**.\n",
    "\n",
    "* It is important to consider the appropriate performance metrics for a classification problem. The **accuracy, precision, and recall** and the **ROC curve** are important measures for binary problems, and a **confusion matrix** is useful for multi-class problems.\n",
    "\n",
    "* The \"perceptron\" or max cost model is the conceptual basis for logistic regression and support vector machines.\n",
    "    - Logistic regression uses the \"softmax\" approximation to the max cost function\n",
    "    - Support vector machines use the \"margin\" cost function, and apply a regularization term to maximize margins\n",
    "    \n",
    "* Kernel support vector machines are the classification equivalent of kernel ridge regression (both have  **$L_2$ regularized cost functions**)\n",
    "\n",
    "* Linear discriminant analysis (LDA) assumes that all classes are normally distributed with equivalent covariance structures.\n",
    "    - LDA is closely related to PCA\n",
    "    - LDA is the supervised classification equivalent of partial least squares (PLS)\n",
    "    \n",
    "* Naive Bayes assumes independent multivariate Gaussian distributions for each class and derives probabilities based on Bayes' theorem.\n",
    "\n",
    "* Decision trees use cutoffs on each feature to assign classes. They have the advantage of being easily interpretable, but are also prone to over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "* [Machine Learning Refined - Chapter 4](http://docs.wixstatic.com/ugd/f09e45_6e2b4294ca2e46968c34071eed230d33.pdf)\n",
    "* [Python Data Science Handbook chapter on support vector machines](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)\n",
    "* Hastie Sec. 6.7, Ch. 12\n",
    "* [Machine Learning Mastery: How to handle class imbalance](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "* [Blog post on creating/visualizing decision trees](https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
