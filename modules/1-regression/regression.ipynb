{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#A-Machine-Learning-Perspective-on-Regression\" data-toc-modified-id=\"A-Machine-Learning-Perspective-on-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>A Machine-Learning Perspective on Regression</a></span></li><li><span><a href=\"#Parameter-Optimization\" data-toc-modified-id=\"Parameter-Optimization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameter Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parametric-vs.-Non-Parametric-Models\" data-toc-modified-id=\"Parametric-vs.-Non-Parametric-Models-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Parametric vs. Non-Parametric Models</a></span></li><li><span><a href=\"#Linear-and-Quadratic-Interpolation\" data-toc-modified-id=\"Linear-and-Quadratic-Interpolation-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Linear and Quadratic Interpolation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-Why-does-the-code-above-not-work?\" data-toc-modified-id=\"Discussion:-Why-does-the-code-above-not-work?-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Discussion: Why does the code above not work?</a></span></li><li><span><a href=\"#Exercise:-Use-every-third-data-point-of-the-spectra-dataset-to-train-a-linear-interpolation-model-(10-points)\" data-toc-modified-id=\"Exercise:-Use-every-third-data-point-of-the-spectra-dataset-to-train-a-linear-interpolation-model-(10-points)-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Exercise: Use every third data point of the spectra dataset to train a linear interpolation model (10 points)</a></span></li></ul></li><li><span><a href=\"#Kernel-Regression\" data-toc-modified-id=\"Kernel-Regression-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Kernel Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Evaluate-the-performance-of-the-rbf-kernel-as-a-function-of-kernel-width-(10-points)\" data-toc-modified-id=\"Exercise:-Evaluate-the-performance-of-the-rbf-kernel-as-a-function-of-kernel-width-(10-points)-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Exercise: Evaluate the performance of the rbf kernel as a function of kernel width (10 points)</a></span></li></ul></li></ul></li><li><span><a href=\"#Complexity-Optimization\" data-toc-modified-id=\"Complexity-Optimization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Complexity Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Quantifying-Complexity\" data-toc-modified-id=\"Quantifying-Complexity-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Quantifying Complexity</a></span></li><li><span><a href=\"#Information-Criteria\" data-toc-modified-id=\"Information-Criteria-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Information Criteria</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Use-the-BIC-to-determine-the-optimum-number-of-evenly-spaced-Gaussians-for-the-spectra-(10-points)\" data-toc-modified-id=\"Exercise:-Use-the-BIC-to-determine-the-optimum-number-of-evenly-spaced-Gaussians-for-the-spectra-(10-points)-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Exercise: Use the BIC to determine the optimum number of evenly-spaced Gaussians for the spectra (10 points)</a></span></li></ul></li><li><span><a href=\"#Regularization:-Ridge-Regression\" data-toc-modified-id=\"Regularization:-Ridge-Regression-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Regularization: Ridge Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-Why-would-the-magnitude-of-model-parameters-be-related-to-the-smoothness-of-a-function?\" data-toc-modified-id=\"Discussion:-Why-would-the-magnitude-of-model-parameters-be-related-to-the-smoothness-of-a-function?-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Discussion: Why would the magnitude of model parameters be related to the smoothness of a function?</a></span></li><li><span><a href=\"#Discussion:-What-happens-as-C-$\\rightarrow$-0-and-C-$\\rightarrow$-$\\infty$?\" data-toc-modified-id=\"Discussion:-What-happens-as-C-$\\rightarrow$-0-and-C-$\\rightarrow$-$\\infty$?-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Discussion: What happens as C $\\rightarrow$ 0 and C $\\rightarrow$ $\\infty$?</a></span></li></ul></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Cross-Validation</a></span></li><li><span><a href=\"#Data-Holdout\" data-toc-modified-id=\"Data-Holdout-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Data Holdout</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Use-cross-validation-to-determine-the-optimum-value-of-$C$-for-$\\sigma=10$-in-terms-of-MSE.-(10-points)\" data-toc-modified-id=\"Exercise:-Use-cross-validation-to-determine-the-optimum-value-of-$C$-for-$\\sigma=10$-in-terms-of-MSE.-(10-points)-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Exercise: Use cross-validation to determine the optimum value of $C$ for $\\sigma=10$ in terms of MSE. (10 points)</a></span></li></ul></li><li><span><a href=\"#Regularization:-LASSO\" data-toc-modified-id=\"Regularization:-LASSO-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Regularization: LASSO</a></span></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Optimize-the-hyperparameters-of-a-LASSO-model-for-the-spectrum-data-(15-points)\" data-toc-modified-id=\"Exercise:-Optimize-the-hyperparameters-of-a-LASSO-model-for-the-spectrum-data-(15-points)-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Exercise: Optimize the hyperparameters of a LASSO model for the spectrum data (15 points)</a></span></li></ul></li></ul></li><li><span><a href=\"#Assessing-Model-Performance-and-Quantifying-Uncertainty\" data-toc-modified-id=\"Assessing-Model-Performance-and-Quantifying-Uncertainty-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Assessing Model Performance and Quantifying Uncertainty</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-Metrics\" data-toc-modified-id=\"Accuracy-Metrics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Accuracy Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mean-absolute-error-(MAE)\" data-toc-modified-id=\"Mean-absolute-error-(MAE)-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Mean absolute error (MAE)</a></span></li><li><span><a href=\"#Root-mean-sqaured-error-(RMSE)\" data-toc-modified-id=\"Root-mean-sqaured-error-(RMSE)-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Root-mean-sqaured error (RMSE)</a></span></li><li><span><a href=\"#$r^2$-value\" data-toc-modified-id=\"$r^2$-value-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>$r^2$ value</a></span></li><li><span><a href=\"#Parity-plots\" data-toc-modified-id=\"Parity-plots-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Parity plots</a></span></li><li><span><a href=\"#Maximum-error\" data-toc-modified-id=\"Maximum-error-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>Maximum error</a></span></li><li><span><a href=\"#Exercise:-Compute-the-mean-absolute-error-for-each-dataset-in-Anscomb's-quartet-(5-points)\" data-toc-modified-id=\"Exercise:-Compute-the-mean-absolute-error-for-each-dataset-in-Anscomb's-quartet-(5-points)-4.1.6\"><span class=\"toc-item-num\">4.1.6&nbsp;&nbsp;</span>Exercise: Compute the mean absolute error for each dataset in Anscomb's quartet (5 points)</a></span></li></ul></li><li><span><a href=\"#Standard-Deviation-of-Error\" data-toc-modified-id=\"Standard-Deviation-of-Error-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Standard Deviation of Error</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-Are-these-uncertainty-bounds-valid-for-all-datasets-in-Anscomb's-quartet?\" data-toc-modified-id=\"Discussion:-Are-these-uncertainty-bounds-valid-for-all-datasets-in-Anscomb's-quartet?-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Discussion: Are these uncertainty bounds valid for all datasets in Anscomb's quartet?</a></span></li></ul></li><li><span><a href=\"#Resampling\" data-toc-modified-id=\"Resampling-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Resampling</a></span></li><li><span><a href=\"#Gaussian-Process-Regression\" data-toc-modified-id=\"Gaussian-Process-Regression-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Gaussian Process Regression</a></span></li></ul></li><li><span><a href=\"#Feature-Analysis-and-Selection\" data-toc-modified-id=\"Feature-Analysis-and-Selection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Analysis and Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scaling-Features-and-Outputs\" data-toc-modified-id=\"Scaling-Features-and-Outputs-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Scaling Features and Outputs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-could-go-wrong-with-rescaling-or-mean-scaling?\" data-toc-modified-id=\"Discussion:-What-could-go-wrong-with-rescaling-or-mean-scaling?-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Discussion: What could go wrong with rescaling or mean scaling?</a></span></li></ul></li><li><span><a href=\"#Multi-Linear-Regression\" data-toc-modified-id=\"Multi-Linear-Regression-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Multi-Linear Regression</a></span></li><li><span><a href=\"#Forward-Selection\" data-toc-modified-id=\"Forward-Selection-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Forward Selection</a></span></li><li><span><a href=\"#Feature-Correlations-and-Covariance\" data-toc-modified-id=\"Feature-Correlations-and-Covariance-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Feature Correlations and Covariance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-is-the-relationship-between-the-number-of-features-and-the-dimensions-of-the-covariance-matrix?\" data-toc-modified-id=\"Discussion:-What-is-the-relationship-between-the-number-of-features-and-the-dimensions-of-the-covariance-matrix?-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Discussion: What is the relationship between the number of features and the dimensions of the covariance matrix?</a></span></li><li><span><a href=\"#Exercise:-Use-for-loops-to-compare-the-covariance-matrix-to-a-matrix-of-slopes-from-linear-fits-(15-points)\" data-toc-modified-id=\"Exercise:-Use-for-loops-to-compare-the-covariance-matrix-to-a-matrix-of-slopes-from-linear-fits-(15-points)-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Exercise: Use for loops to compare the covariance matrix to a matrix of slopes from linear fits (15 points)</a></span></li></ul></li><li><span><a href=\"#Eigenvalues-and-Eigenvectors\" data-toc-modified-id=\"Eigenvalues-and-Eigenvectors-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Eigenvalues and Eigenvectors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Use-a-for-loop-to-show-that-the-eigenvectors-are-orthonormal-(5-points)\" data-toc-modified-id=\"Exercise:-Use-a-for-loop-to-show-that-the-eigenvectors-are-orthonormal-(5-points)-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Exercise: Use a for loop to show that the eigenvectors are orthonormal (5 points)</a></span></li></ul></li><li><span><a href=\"#Principal-Component-Regression\" data-toc-modified-id=\"Principal-Component-Regression-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Principal Component Regression</a></span></li><li><span><a href=\"#Partial-Least-Squares\" data-toc-modified-id=\"Partial-Least-Squares-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Partial Least Squares</a></span></li><li><span><a href=\"#Scaling-of-Non-linear-Models\" data-toc-modified-id=\"Scaling-of-Non-linear-Models-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Scaling of Non-linear Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-How-many-features-would-result-if-third-order-interactions-were-considered?\" data-toc-modified-id=\"Discussion:-How-many-features-would-result-if-third-order-interactions-were-considered?-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>Discussion: How many features would result if third-order interactions were considered?</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Machine-Learning Perspective on Regression\n",
    "\n",
    "The goal of regression is to find a function\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}) + \\vec{\\epsilon}$\n",
    "\n",
    "where $f$ is the model, $x$ is the model input, $y$ is the model output, and $\\epsilon$ is the error between the model in the data. The model inputs, $\\vec{x}$ are often called the **features** of a data point. In the previous example we created features using transformations of $x$ like polynomials and Gaussian functions. Sometimes, features may also be given in the dataset (e.g. multiple inputs correspond to a single output). Other times, the model input may be data that does not have obvious vector-based features (e.g. images, audio, molecules, etc.). In this case, we can think of the features as \"fingerprints\" of some more complex raw input data.\n",
    "\n",
    "Of course representing the model as $f$ is a gross oversimplification. The function must have some form, and it usually requires **parameters**. Previously we considered general linear regression models of the form:\n",
    "\n",
    "$y_i = \\sum_j \\beta_j X_{ij} + \\epsilon_i$\n",
    "\n",
    "where the **parameters** are given by $\\vec{\\beta}$. We also considered non-linear regression with Gaussian functions, which required more parameters, $\\vec{\\beta}$, $\\vec{\\mu}$, and $\\vec{\\sigma}$. We saw that in order to optimize these parameters we had to put them into a single vector. We could consider this to be a parameter vector, $\\vec{W} = [\\vec{\\beta}, \\vec{\\mu}, \\vec{\\sigma}]$, and re-write the model more generally as:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}) + \\vec{\\epsilon}$\n",
    "\n",
    "We also had to decide on how many parameters to include. In the case of polynomial regression this corresponded to the order of the highest polynomial, while for Gaussian regression it corresponded to the number of Gaussian functions to include. This number of parameters to include is called a **hyperparameter**. Hyperparameters control the complexity of the final model, and the parameters will depend on the hyperparameters, so we can think of the parameters as being a function of the hyperparameters, $\\vec{W}(\\vec{\\eta})$. If we put all this together we get a model form of:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta})) + \\vec{\\epsilon}$\n",
    "\n",
    "Machine learning differs from regular regression in that it seeks to optimize $\\vec{W}$ (parameter optimization), $\\vec{\\eta}$ (complexity optimization) in order to **obtain a model that generalizes to new input data**. Machine learning also sometimes involves selecting $\\vec{x}$ (feature selection) or generating $\\vec{x}$ from non-vectorized data such as text or images (feature generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered the basic math behind parameter optimization in the foundations module. The basic idea is to follow two steps:\n",
    "\n",
    "* Construct a loss function that quantifies how well your model fits the data\n",
    "* Minimize the loss function with respect to the model parameters\n",
    "\n",
    "The loss function itself could be the sum of squared errors, some other measure of error (e.g. absolute value of error), and can also contain constraints on the parameters themselves (e.g. force parameters to be positive).\n",
    "\n",
    "Minimizing the loss function can be achieved analytically in the case of general linear models, or numerically for non-linear models. Moving forward we will typically default to numerical optimization.\n",
    "\n",
    "In this section we will explore another aspect of model parameters by looking at a new class of models called \"non-parameteric\" models. The math of parameter optimization is the same, but the way the parameters are defined is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric vs. Non-Parametric Models\n",
    "\n",
    "A \"parametric\" model has parameters that do not explicitly depend on or include the input points. The polynomial regression model is an example of a parametric model. The number of parameters is fixed with respect to the number of data points.\n",
    "\n",
    "A \"non-parametric\" model includes parameters that are defined on the domain of the independent variables and depend on the inputs. A spline model is an example of a non-parametric model. The number of parameters in the model varies with the number of data points.\n",
    "\n",
    "Nonparametric models are generally excellent for interpolation, but fail miserably for extrapolation, while parametric models are less accurate for interpolation but provide more reasonable extrapolations. Nonparametric models tend to have many more parameters, and proper optimization of model complexity can lead to similar performance for both types.\n",
    "\n",
    "See [this post](https://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and Quadratic Interpolation\n",
    "\n",
    "Let's revisit the spectra dataset that we worked with during the last module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/ethanol_IR.csv')\n",
    "x_all = df['wavenumber [cm^-1]'].values\n",
    "y_all = df['absorbance'].values\n",
    "\n",
    "x_peak = x_all[475:575]\n",
    "y_peak = y_all[475:575]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak,y_peak, '-b', marker='.')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the common problem that we want to interpolate between points with a straight line. It turns out we can solve this by using a general linear model!\n",
    "\n",
    "The key is to use a basis of \"piecewise linear\" functions:\n",
    "\n",
    "$X_{ij} = max(0, x_i-x_j)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_linear(x):\n",
    "    N = len(x)\n",
    "    X = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            X[i,j] = max(0, x[i] - x[j])\n",
    "    return X\n",
    "            \n",
    "X = piecewise_linear(x_peak)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, X[:,50]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all of the basis functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(len(x_peak)):\n",
    "    ax.plot(x_peak, X[:,i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basis set, or \"features\" consist straight lines with slope 1 that originate at each data point. Now we can achieve linear interpolation by solving the general linear regression problem. We will use `scikit-learn` to make this easy, but you can verify the solution using the equations from the foundations module if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X, y_peak) #fit the model\n",
    "r2 = model.score(X, y_peak) #get the \"score\", which is equivalent to r^2\n",
    "\n",
    "yhat = model.predict(X) #create the model prediction\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '.b')\n",
    "ax.plot(x_peak, yhat, 'or', markerfacecolor='none')\n",
    "print('r^2 = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model goes through every point exactly, which we should know from $r^2=1$. However, we don't actually know what the model is doing in between the points. For this we need to predict on a new set of $x$ points that has a higher resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.linspace(2650, 3150, 500)\n",
    "X_predict = piecewise_linear(x_predict)\n",
    "\n",
    "#yhat_pred = model.predict(X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: Why does the code above not work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_linear(x_train, x_val=None):\n",
    "    if x_val is None:\n",
    "        x_val = x_train\n",
    "    N = len(x_val) #<- number of data points\n",
    "    M = len(x_train) #<- number of features\n",
    "    X = np.zeros((N,M))\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            X[i,j] = max(0, x_val[i] - x_train[j])\n",
    "    return X\n",
    "\n",
    "X_predict = piecewise_linear(x_peak, x_predict)\n",
    "\n",
    "yhat_predict = model.predict(X_predict)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '.b')\n",
    "ax.plot(x_predict, yhat_predict, '-r', markerfacecolor='none')\n",
    "print('r^2 = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model successful at interpolating between the points. This is an example of a **non-parametric** model. The number of parameters, $\\vec{\\beta}$ is equal to the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Use every third data point of the spectra dataset to train a linear interpolation model (10 points)\n",
    "\n",
    "First, select every third datapoint from the `(x_peak, y_peak)` dataset, and use this to train a linear interpolation model. Then, predict the full dataset using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Regression\n",
    "\n",
    "We are not limited to using piecewise linear functions. We can actually generalize this using the idea of a \"kernel\":\n",
    "\n",
    "$K(i, j) = f(x_i, x_j)$\n",
    "\n",
    "where $f$ can be any function. The most commonly used kernel is the \"radial basis function\", or `rbf` kernel:\n",
    "\n",
    "$rbf(i, j) = exp(-\\gamma (x_i - x_j)^2)$\n",
    "\n",
    "If you look closely, you will see that this is the same as a Gaussian function, where $\\mu = x_j$ and $\\gamma = \\frac{1}{2\\sigma^2}$:\n",
    "\n",
    "$G(x_i) = exp\\left(\\frac{-(x_i - \\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "Let's follow the same procedure as before, but now we will use a \"radial basis function\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rbf(x_train, x_val=None, gamma=1):\n",
    "    if x_val is None:\n",
    "        x_val = x_train\n",
    "    N = len(x_val) #<- number of data points\n",
    "    M = len(x_train) #<- number of features\n",
    "    X = np.zeros((N,M))\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            X[i,j] = np.exp(-gamma*(x_val[i] - x_train[j])**2)\n",
    "    return X\n",
    "\n",
    "sigma = 30\n",
    "gamma = 1./(2*sigma**2)\n",
    "x_val = np.linspace(min(x_peak), max(x_peak), 300)\n",
    "X_rbf = rbf(x_peak, x_val=x_val, gamma=gamma)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_val, X_rbf[:,50]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we are now putting a Gaussian basis set with a fixed width at every training point! Let's see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rbf(x_peak, gamma=gamma)\n",
    "\n",
    "model_rbf = LinearRegression() #create a linear regression model instance\n",
    "model_rbf.fit(X_train, y_peak) #fit the model\n",
    "r2 = model_rbf.score(X_train, y_peak) #get the \"score\", which is equivalent to r^2\n",
    "print('r^2 = {}'.format(r2))\n",
    "\n",
    "X_val = rbf(x_peak, x_val=x_val, gamma=gamma)\n",
    "\n",
    "yhat_rbf = model_rbf.predict(X_val) #create the model prediction\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '.b')\n",
    "ax.plot(x_val, yhat_rbf, 'or', markerfacecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Evaluate the performance of the rbf kernel as a function of kernel width (10 points)\n",
    "\n",
    "Use the same strategy as the previous exercise to select every third point in the spectra to use as the training set. Then, vary the width of the radial basis function with $\\sigma = [1, 10, 100]$, and compute the $r^2$ score for each *using the entire dataset*.\n",
    "\n",
    "Plot $r^2$ as a function of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [1, 10, 100]\n",
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the kernel-based methods provide a very good fit to the existing data, and sometimes they give very accurate interpolation in between. However, the ability to interpolate is not always good, and the extrapolation is extremely bad. In general, non-parametric models will always be **overfit** unless their complexity is optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to machine learning is creating models that generalize to new examples. This means we are looking for models with enough complexity to describe the behavior, but not so much complexity that it just reproduces the data points.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/underfitting_overfitting.png\" width=\"800\">\n",
    "</center>\n",
    "\n",
    "* Underfitting: The model is just \"guessing\" at the data, and will be equally bad at the data it has been trained on and the data that it is tested on.\n",
    "\n",
    "* Overfitting: The model has memorized all of the training data, and will be perfect on training data and terrible on testing data.\n",
    "\n",
    "* Optimal complexity: The model has *learned* from the training data and can *generalize* to the training data. The performance should be approximately as good for both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying Complexity\n",
    "\n",
    "Consider the general form of a machine-learning model introduced earlier:\n",
    "\n",
    "$\\vec{y} = f(\\vec{x}, \\vec{W}(\\vec{\\eta}))$\n",
    " \n",
    " The \"complexity\" of a model is defined by its hyperparameters ($\\vec{\\eta}$). The goal of machine learning is to **optimize the complexity** of a model so that it **generalizes to new examples**. In order to achieve this goal we first need a way to quantify complexity so that we can optimize it.\n",
    " \n",
    " In general there are a few strategies:\n",
    " \n",
    " * Number of parameters: \"Complexity\" varies linearly with number of parameters\n",
    " * Information criteria: \"Complexity\" varies with number of parameters and is balanced by the model error.\n",
    " * \"Smoothness\": \"Complexity\" is related to the maximum curvature of the model\n",
    " \n",
    " \"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.\"\n",
    " \n",
    " -- John Von Neumann\n",
    " \n",
    " (see an [example here](https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Criteria\n",
    "\n",
    "The idea behind an \"information criterion\" is that it quantifies the tradeoff between the number of parameters and the model error. The most commonly used information criterion is the \"Bayesian Information Criterion\", or BIC. The derivation of the BIC is beyond the scope of this course, but conceptually a lower BIC corresponds to a *more* probable model.\n",
    "\n",
    "If we assume that our error is normally distributed, the BIC can be easily computed as:\n",
    "\n",
    "$ BIC = n*\\ln{(\\sigma^2_e)} + k*\\ln(n)$\n",
    "\n",
    "where $n$ is the number of data points, $\\sigma_e$ is the standard deviation of the error, and $k$ is the number of parameters.\n",
    "\n",
    "There are a few other \"information critera\", with the Akaike Information Criterion, or AIC, being the other most commonly used. For now we will just consider the BIC, but they typically yield similar optimal models.\n",
    "\n",
    "Let's implement the BIC in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIC(y, yhat, k):\n",
    "    err = y - yhat\n",
    "    sigma = np.std(np.real(err))\n",
    "    n = len(y)\n",
    "    B = n*np.log(sigma**2) + k*np.log(n)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare some of the many different models we have used for modeling the spectrum from the previous module and this module. We will look at the following models:\n",
    "\n",
    "* Polynomial regression with 40 polynomials (40 parameters)\n",
    "* Gaussian regression 20 evenly-spaced Gaussians (20 parameters)\n",
    "\n",
    "We will re-implement the polynomial and Gaussian regressions using `scikit-learn` to make things easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    # One-liner uses \"list comprehension\" to iterate through range 0 - N (note N+1 since range function is not inclusive)\n",
    "    # The input, x, is raised to the power of N for each value of N\n",
    "    # The result is converted to an array and transposed so that columns correspond to features and rows correspond to data points (individual x values)\n",
    "    return np.array([x**k for k in range(0,N)]).T\n",
    "\n",
    "N = 40\n",
    "X_poly = polynomial_features(x_peak, N)\n",
    "\n",
    "LR_poly = LinearRegression() #create a linear regression model instance\n",
    "LR_poly.fit(X_poly, y_peak) #fit the model\n",
    "yhat_poly = LR_poly.predict(X_poly)\n",
    "\n",
    "BIC_poly = BIC(y_peak, yhat_poly, N)\n",
    "BIC_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_features(x, N , sigma = 25):\n",
    "    # x is a vector\n",
    "    # sigma is the standard deviation\n",
    "    xk_vec = np.linspace(min(x), max(x), N)\n",
    "    features = []\n",
    "    for xk in xk_vec:\n",
    "        features.append(np.exp(-((x - xk)**2/(2*sigma**2))))\n",
    "    return np.array(features).T\n",
    "\n",
    "N = 20\n",
    "X_gauss = gaussian_features(x_peak, N)\n",
    "\n",
    "LR_gauss = LinearRegression() #create a linear regression model instance\n",
    "LR_gauss.fit(X_gauss, y_peak) #fit the model\n",
    "yhat_gauss = LR_gauss.predict(X_gauss)\n",
    "\n",
    "BIC_gauss = BIC(y_peak, yhat_gauss, N)\n",
    "BIC_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (12,4))\n",
    "axes[0].plot(x_peak, y_peak, '.b')\n",
    "axes[1].plot(x_peak, y_peak, '.b')\n",
    "\n",
    "axes[0].plot(x_peak, yhat_poly, '--r')\n",
    "axes[1].plot(x_peak, yhat_gauss, '--r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the BIC correctly predicts that the Gaussian model is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Use the BIC to determine the optimum number of evenly-spaced Gaussians for the spectra (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information criteria are useful for optimizing complexity, but they are not perfect and should not be used as a substitute for common sense or intuition. However, the general idea of balancing number of parameters and model error provides a solid framework for thinking about complexity optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Ridge Regression\n",
    "\n",
    "Another way of penalizing complexity is by trying to penalize models that change very sharply. This is achieved by adding a penalty for parameters with very large values in the loss function. For example:\n",
    "\n",
    "$L = \\sum_i \\epsilon_i^2 + C \\sqrt{\\sum_j \\beta_j^2}$\n",
    "\n",
    "In this case, we introduce a new hyperparameter, $C$, which controls the strength of regularization. We also choose to regularize on the square root of the sum of squared parameters, which is often called the \"L2 norm\" and written as:\n",
    "\n",
    "$L = \\sum_i \\epsilon_i^2 + C ||\\vec{\\beta}||_2$\n",
    "\n",
    "We can also regularize in other ways, which can have advantages in some cases. We will discuss this more later, but will focus on the L2 norm for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: Why would the magnitude of model parameters be related to the smoothness of a function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is especially critical in the case of non-parametric models, where the number of parameters is always greater than the number of data points. If we use a kernel and regularize on the sum of squared parameters it is called \"Kernel Ridge Regression\", or KRR. We will not derive the equations here, but it can be done analytically (Hint: You should think about how you would do this).\n",
    "\n",
    "We will just use the `scikit-learn` implementation for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "help(KernelRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the parameters we see that we need to specify which kernel to use (we will use `rbf`), the `gamma` value corresponding to the width of the kernel, and `alpha`. Annoyingly, `alpha` is defined as `(2*C)^-1`:\n",
    "\n",
    "$\\alpha = \\frac{1}{2C}$\n",
    "\n",
    "This means that if $\\alpha$ is large, $C$ is small. This can get very confusing, so its often nice to just define $\\alpha$ in terms of $C$. Let's see how $C$ affects the kernel regression from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "gamma = 1./(2*sigma**2)\n",
    "\n",
    "C = 0.001\n",
    "alpha = 1./2*C\n",
    "\n",
    "KRR = KernelRidge(alpha=alpha, kernel='rbf', gamma=gamma)\n",
    "x_peak = x_peak.reshape(-1,1) #we need to convert these to columns\n",
    "y_peak = y_peak.reshape(-1,1)\n",
    "\n",
    "KRR.fit(x_peak, y_peak)\n",
    "\n",
    "x_predict = np.linspace(min(x_peak), max(x_peak), 300) #create prediction data\n",
    "yhat_KRR = KRR.predict(x_predict)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '.b')\n",
    "ax.plot(x_predict, yhat_KRR, '--r', markerfacecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What happens as C $\\rightarrow$ 0 and C $\\rightarrow$ $\\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the regularization clearly affects the model, but sometimes it seems to make it worse. We need some strategy for assessing what value of regularization to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "A good way to test if a model is too complex (or not complex enough) is to see if it will **generalize to new examples**. We can test this using \"cross-validation\", where some examples (\"validation\" examples) are hidden when the model is fit to \"training\" examples. Then, the loss function is assessed on the data that was hidden to see if the model is able to predict it.\n",
    "\n",
    "There are many strategies for cross-validation:\n",
    "\n",
    "* hold-out: randomly leave out a percentage (usually ~30%) of the data during training.\n",
    "* k-fold: select `k` (usually 3-5) randomly-assigned sub-groups of data, and train `k` times holding each group out.\n",
    "* leave p out: leave `p` (usually 1) samples out of the training and assess the error for the `p` that were left out. Repeat for all possible `p` subsets of the sample.\n",
    "* bootstrapping: random selection with replacement to generate a sample of the same size as the original dataset, with a number of repetitions.\n",
    "\n",
    "Cross-validation is used to determine hyperparameters. In this case, even the \"validation\" sets are used to optimize the model. It is common to select an additional \"test\" or \"holdout\" subset for a final testing of the model.\n",
    "\n",
    "Important (and often violated) assumption: *The collected data is representative of future data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Holdout\n",
    "\n",
    "The simplest cross-validation strategy is to simply hide some data from the model and then test it on this data. We can use this to get a quick idea of whether our model is over-fit or not. Let's try it with the spectrum data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_peak, y_peak, test_size=0.35)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x_train, y_train, 'ob')\n",
    "ax.plot(x_test, y_test, 'og');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use cross-validation to optimize hyperparameters such as regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "gamma = 1./(2*sigma**2)\n",
    "\n",
    "C = 1e-9\n",
    "alpha = 1./2*C\n",
    "\n",
    "KRR = KernelRidge(alpha=alpha, kernel='rbf', gamma=gamma)\n",
    "KRR.fit(x_train, y_train)\n",
    "\n",
    "x_predict = np.linspace(min(x_peak), max(x_peak), 300) #create prediction data\n",
    "yhat_KRR = KRR.predict(x_predict)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'ob')\n",
    "ax.plot(x_test, y_test, 'og')\n",
    "ax.plot(x_predict, yhat_KRR, '--r', markerfacecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see how the regularization affects the parameters, $\\vec{\\beta}$, by looking at the `dual_coef_` attribute of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs= KRR.dual_coef_\n",
    "print('The model has {} coefficients.'.format(len(coeffs)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(coeffs)\n",
    "print('The largest coefficient is {:.3f}.'.format(max(abs(coeffs))[0]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Use cross-validation to determine the optimum value of $C$ for $\\sigma=10$ in terms of MSE. (10 points)\n",
    "\n",
    "Use the test/train split defined above and use values of C from [1e-7, 1e-5, 1e-3, 1e-2, 1] and plot MSE vs. C.\n",
    "\n",
    "MSE = $\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression provides a good way to penalize model \"smoothness\", but it doesn't actually reduce the number of parameters. We can see that all of the coefficients are non-zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero = [f for f in np.isclose(coeffs,0) if f == False]\n",
    "print('Total number of non-zero parameters: {}'.format(len(nonzero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we could also use regularization to reduce the number of parameters. It turns out that this can be achieved using the L1 norm:\n",
    "\n",
    "$||L_1|| = \\sum_i |\\beta_i|$\n",
    "\n",
    "where $|.|$ is the absolute value. This is called \"least absolute shrinkage and selection operator\" regression, which is a terrible name with a great acronym: LASSO.\n",
    "\n",
    "We will not go through the derivation of *why* the L1 norm causes parameters to go to zero, but we can test it using `scikit-learn`. Unfortunately, we need to create our own feature (basis) matrix, $X_{ij}$, similar to linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "sigma = 30\n",
    "gamma = 1./(2*sigma**2)\n",
    "\n",
    "X_train = rbf(x_train, gamma=gamma)\n",
    "\n",
    "C = 1e-2\n",
    "alpha = 1./2*C\n",
    "\n",
    "LASSO = Lasso(alpha=alpha)\n",
    "LASSO.fit(X_train, y_train)\n",
    "print(len(LASSO.coef_))\n",
    "\n",
    "x_predict = np.linspace(min(x_peak), max(x_peak), 300) #create prediction data\n",
    "X_predict = rbf(x_train, x_val=x_predict, gamma=gamma)\n",
    "\n",
    "yhat_LASSO = LASSO.predict(X_predict)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'ob')\n",
    "ax.plot(x_test, y_test, 'og')\n",
    "ax.plot(x_predict, yhat_LASSO, '--r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how many non-zero parameters there are, and check the parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = LASSO.coef_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(coeffs)\n",
    "\n",
    "nonzero = [f for f in np.isclose(coeffs,0) if f == False]\n",
    "print('Total number of non-zero parameters: {}'.format(len(nonzero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "The KRR and LASSO models above have 2 hyperparameters: $\\gamma$ $\\left(=\\frac{1}{2\\sigma}\\right)$ and $\\alpha$ $\\left(=\\frac{1}{2C}\\right)$. In the exercise we optimized $\\alpha$, but the model performance (and optimal $\\alpha$) will also depend on $\\sigma$. You can probably see that optimizing these will get rather tedious.\n",
    "\n",
    "Fortunately, `scikit-learn` has some nice built-in tools to help. The most commonly used is `GridSearchCV`, which is a brute-force approach that searches over a grid of hyperparameters, and uses cross-validation at each grid point to assess model performace.\n",
    "\n",
    "Here we will use GridSearchCV to find the optimum KRR model and its score (related to $R^2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sigmas = np.array([5, 10, 15, 20, 25, 30,35, 40])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "\n",
    "Cs = np.array([1e-9, 1e-5, 1e-4,1e-3, 1e-2,1e-1, 1])\n",
    "alphas = 1./(2*Cs)\n",
    "\n",
    "parameter_ranges = {'alpha':alphas, 'gamma':gammas}\n",
    "\n",
    "KRR = KernelRidge(kernel='rbf')\n",
    "\n",
    "KRR_search = GridSearchCV(KRR, parameter_ranges, cv=3)\n",
    "KRR_search.fit(x_train,y_train)\n",
    "KRR_search.best_estimator_, KRR_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the best performance comes from a model with $\\alpha=0.5$ and $\\gamma=0.000555$. We can check the performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_KRR = KRR_search.best_estimator_.predict(x_predict)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'ob')\n",
    "ax.plot(x_test, y_test, 'og')\n",
    "ax.plot(x_predict, yhat_KRR, '--r', markerfacecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster than doing all the work yourself!\n",
    "\n",
    "One note is that the best model will depend on the parameters you search over, as well as the cross-validation strategy. In this case, `cv=3` means that the model performs 3-fold cross-validation at each gridpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Optimize the hyperparameters of a LASSO model for the spectrum data (15 points)\n",
    "\n",
    "Search over the same values of $C$ and $\\sigma$ as for KRR above, and use 3-fold cross validation.\n",
    "\n",
    "Note: You will probably need to use a for loop! Use `KRR_search.best_score_` as accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.array([5,10, 15, 20,25,30,35,40])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "\n",
    "Cs = np.array([1e-9, 1e-5, 1e-4, 1e-3, 1e-2,1e-1, 1, 10, 100, 1000])\n",
    "alphas = 1./(2*Cs)\n",
    "\n",
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model Performance and Quantifying Uncertainty\n",
    "\n",
    "So far we have discussed a few strategies for assessing model performance. The $r^2$ score was discussed in the foundations module, and the Bayesian Information Criterion was introduced earlier. We have also used visualization to decide if models are good or not. Visualization is always a good first step, but sometimes it is useful to have a more quantitative assessment, or to quantify the uncertainty associated with the model or parameters. We will investigate a few possibilities here and look at a famous dataset called Anscomb's quartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5])\n",
    "y1 = np.array([8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "y2 = np.array([9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73])\n",
    "x4 = np.array([8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anscomb's quartet is interesting because the statistics of all 4 datasets are the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(x,y):    \n",
    "    y_bar = np.mean(y)\n",
    "    y_std = np.std(x)\n",
    "    m, b = np.polyfit(x,y,deg=1)\n",
    "    SST = sum((y - y_bar)**2)\n",
    "    SSE = sum((y - (m*x+b))**2)\n",
    "    R2 = (SST - SSE)/SST\n",
    "    return y_bar, y_std, m, b, R2\n",
    "\n",
    "stats1 = calc_stats(x,y1)\n",
    "print(\"Dataset 1: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats1))\n",
    "stats2 = calc_stats(x,y2)\n",
    "print(\"Dataset 2: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats2))\n",
    "stats3 = calc_stats(x,y3)\n",
    "print(\"Dataset 3: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats3))\n",
    "stats4 = calc_stats(x4,y4)\n",
    "print(\"Dataset 4: mean={:.2f}, stdev={:.2f}, m={:.2f}, b={:.2f}, R2={:.2f}\".format(*stats4))\n",
    "avg, std, m, b, r2 = stats1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the datasets themselves are very different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "yhat = m*x + b\n",
    "axes[0].scatter(x,y1)\n",
    "axes[0].plot(x, yhat, ls='-', color='k')\n",
    "axes[1].scatter(x,y2)\n",
    "axes[1].plot(x, yhat, ls='-', color='k')\n",
    "axes[2].scatter(x,y3)\n",
    "axes[2].plot(x, yhat, ls='-', color='k')\n",
    "axes[3].scatter(x4,y4)\n",
    "axes[3].plot(x4, m*x4 + b, ls='-', color='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics\n",
    "\n",
    "It is important to consider the context of a regression model and choose accuracy metrics that are relevant to its application. There are several common options:\n",
    "\n",
    "* #### Mean absolute error (MAE)\n",
    "\n",
    "$MAE = \\frac{1}{N} \\sum_{i=0}^N |y_i - \\hat{y}_i|$\n",
    "\n",
    "* #### Root-mean-sqaured error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=0}^N (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "MAE and RMSE are very similar. Both have units of the dependent variable ($y$), and can vary from 0 to $\\infty$ with lower values being better. MAE is less affected by outliers and sample size, but it is always lower than RMSE, so it is a less conservative estimate. MAE and RMSE are related by the inequalities:\n",
    "\n",
    "$MAE \\leq RMSE \\leq MAE \\times \\sqrt{N}$\n",
    "\n",
    "* #### $r^2$ value\n",
    "\n",
    "The $r^2$ metric was introduced earlier, and is very common in regression models, and is the default \"score\" in `scikit-learn`. $r^2$ varies from 0-1, with higher values corresponding to better models. The $r^2$ value corresponds to the amount of variance in the independent variable that is explained by the model, and is defined as:\n",
    "\n",
    "$r^2 = \\frac{\\sum_{i=0}^N (y_i - \\bar{y})^2 - \\sum_{i=0}^N (y_i - \\hat{y})^2}{\\sum_{i=0}^N (y_i - \\bar{y})^2}$\n",
    "\n",
    "where $\\bar{y}$ is the mean of $y$. This is often written as:\n",
    "\n",
    "$r^2 = \\frac{SST - SSE}{SST}$\n",
    "\n",
    "where $SST = \\sum_{i=0}^N (y_i - \\bar{y})^2$ and $SSE = \\sum_{i=0}^N (y_i - \\hat{y})^2$\n",
    "\n",
    "* #### Parity plots\n",
    "\n",
    "Plotting $y$ vs. $\\hat{y}$ provides a visual analysis of the error.\n",
    "\n",
    "* #### Maximum error\n",
    "\n",
    "Sometimes it is useful to assess the maximum error of a model, $max(\\epsilon_i)$. This is useful to assess a worst-case scenario, and provides a conservative estimate of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Compute the mean absolute error for each dataset in Anscomb's quartet (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way of quantifying uncertainty is to assess the standard deviation of the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_stdev = np.std(y3 - yhat, ddof=2)\n",
    "print(error_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used `ddof=2` here, since we have 2 degrees of freedom removed due to the 2 parameters in the model. Also note that the standard deviation is the same for each dataset in Anscomb's quartet.\n",
    "\n",
    "We can use the following expression to account for how the uncertainty changes as a function of $x$ (if all assumptions of linear regression hold):\n",
    "\n",
    "$\\vec{\\sigma_y} = \\sigma_{error} \\sqrt{\\left(1 + \\frac{1}{n} + \\frac{(\\vec{x}-\\bar{x})^2}{(\\sum_j x_j - \\bar{x})^2} \\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\sigma_{error}$ is the standard deviation of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_error(x,y, x_data, yhat):\n",
    "    sigma_error = np.std(y-yhat, ddof=2)\n",
    "    xbar = np.mean(x_data)\n",
    "    y_error = sigma_error * np.sqrt(1 + 1/len(y) + ((x-xbar)**2)/(np.sum((x_data-xbar)**2)))\n",
    "    return y_error\n",
    "\n",
    "x_dense = np.linspace(min(x)-5,max(x4)+5,50)\n",
    "\n",
    "y_error = regression_error(x_dense,y1, x, m*x+b)\n",
    "\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "\n",
    "axes[0].scatter(x,y1)\n",
    "axes[1].scatter(x,y2)\n",
    "axes[2].scatter(x,y3)\n",
    "axes[3].scatter(x4,y4)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x_dense, m*x_dense+b)\n",
    "    ax.plot(x_dense, m*x_dense+b + y_error, ls='--', color='0.5')\n",
    "    ax.plot(x_dense, m*x_dense+b - y_error, ls='--', color='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: Are these uncertainty bounds valid for all datasets in Anscomb's quartet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility that avoids the assumptions of homoskedastic and normally-distributed errors is to use resampling techniques to generate a distribution of models. These models have distributions of parameters that capture the deviations in the data\n",
    "\n",
    "There are many ways to achieve this, but one of the most popular is \"bootstrapping\". In a bootstrapping approach the data is re-sampled by choosing the same number of points `N` randomly from the real dataset, but this is done **with replacement** so that each re-sample is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice #<- randomly select items from a list\n",
    "\n",
    "def bootstrap_linregress(x_all,y_all,N):\n",
    "    m_list = []\n",
    "    b_list = []\n",
    "    for n in range(N):\n",
    "        subset = choice(range(len(x_all)),size=len(x_all),replace=True)\n",
    "        xprime = [x_all[j] for j in subset]\n",
    "        yprime = [y_all[j] for j in subset]\n",
    "        if np.std(xprime) > 0:\n",
    "            m, b = np.polyfit(xprime,yprime,deg=1)\n",
    "        else:\n",
    "            m = 0\n",
    "            b = np.mean(yprime)\n",
    "        \n",
    "        m_list.append(m)\n",
    "        b_list.append(b)\n",
    "    return m_list, b_list\n",
    "\n",
    "anscombs = [[x,y1],[x,y2],[x,y3],[x4,y4]]\n",
    "fig, axes = plt.subplots(1,4,figsize=(15,4))\n",
    "fig_m, axes_m = plt.subplots(1,4,figsize=(15,4))\n",
    "fig_b, axes_b = plt.subplots(1,4,figsize=(15,4))\n",
    "\n",
    "N = 100\n",
    "\n",
    "for i, xy in enumerate(anscombs):\n",
    "    xi, yi = xy\n",
    "    m, b = np.polyfit(xi,yi,deg=1)\n",
    "    axes[i].scatter(xi,yi, color = 'r')\n",
    "    axes[i].plot(xi,m*xi+b, color='k', lw=2)\n",
    "\n",
    "    \n",
    "    m_list, b_list = bootstrap_linregress(xi,yi,N)\n",
    "    for mj, bj in zip(m_list,b_list):\n",
    "        axes[i].plot(xi, mj*xi+bj, color='k', alpha=0.05)\n",
    "        \n",
    "    axes_m[i].hist(m_list)\n",
    "    axes_m[i].set_xlabel('m')\n",
    "    axes_b[i].hist(b_list)\n",
    "    axes_b[i].set_xlabel('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is very powerful because it allows us to get estimates of the prediction error as well as the error distribution of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression\n",
    "\n",
    "Gaussian process regression is an extension of kernel ridge regression that uses the distance of prediction points from the training points to estimate errors. The math behind this is beyond the scope of this course, but we will briefly demonstrate it for the spectra dataset that we applied KRR to earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=RBF(1), alpha=0.000005)\n",
    "\n",
    "gpr.fit(x_train,y_train)\n",
    "\n",
    "y_gpr, y_std = gpr.predict(x_predict, return_std=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_train, y_train, 'ob')\n",
    "ax.plot(x_predict, y_gpr, '--r')\n",
    "ax.fill_between(x_predict[:,0], y_gpr[:,0] - y_std, y_gpr[:,0] + y_std, color='k',alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPR is properly tuned it can give good uncertainty estimates. However, it is also possible to get very unreliable estimates if the model is not trained properly, so be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only worked with datasets that have a single input dimension. We have generated \"features\" from this dimension, but we have not considered the case of a problem where multiple inputs are given. This is a very common scenario, and one of the main advantages of many machine-learning methods is that they work well for \"high-dimesional\" data, or data with many features.\n",
    "\n",
    "For this section we will start working with your case study data set, which is related to \"transparent conducting oxides\". The dataset contains examples of many possible transparent conducting oxide materials along with their band gap and formation energy. A material is promising if it has a negative formation energy and a large band gap.\n",
    "\n",
    "In this case, the goal to be able to predict the materials properties (formation energy or band gap) from parameters that define the material structure. This has the potential to more rapidly identify promising materials. You can read more about the dataset [here](https://www.kaggle.com/c/nomad2018-predict-transparent-conductors/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/materials_band_gaps.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for each material we know something about its structure (space group, lattice vectors) and composition. These are the \"features\" that we will use as inputs to try to predict the formation energy or band gap. In class we will work with the formation energies, and in your case study you will try to predict band gaps.\n",
    "\n",
    "We will learn more about the `pandas` library soon, but for now just run the cell below to generate the features and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:-2] #id is irrelevant\n",
    "x_names = X.columns.to_list()\n",
    "X = X.values\n",
    "y = df.iloc[:,-2].values #the next-to-last column is the formation energy\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will visualize the data to get a feel for it. We can start by making histograms for each feature and the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X dimensions: {}'.format(X.shape))\n",
    "print('Feature names: {}'.format(x_names))\n",
    "N = X.shape[-1]\n",
    "n = int(np.sqrt(N))\n",
    "fig, axes = plt.subplots(n, n+1, figsize = (5*n, 5*n))\n",
    "ax_list = axes.ravel()\n",
    "for i in range(N):\n",
    "    ax_list[i].hist(X[:,i])\n",
    "    ax_list[i].set_xlabel(x_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features and Outputs\n",
    "\n",
    "We can see that different features have very different ranges, and different units (e.g. degrees, percent, count).  Scaling data is like \"non-dimensionalizing\" or normalizing for different units. This is often critical to ensure that certain variables are not weighted more than others.\n",
    "\n",
    "Statistical methods don't know about physical units, so we can normalize or \"scale\" features to aid in comparison:\n",
    "\n",
    "* rescaling: 0 = min, 1 = max\n",
    "* mean scaling: 0 = mean, 1 = max, -1 = min\n",
    "* **standardization: 0 = mean, 1 = standard deviation**\n",
    "* unit vector: the length of each multi-dimensional vector is 1\n",
    "\n",
    "See the [scikit-learn documentation](http://scikit-learn.org/stable/modules/preprocessing.html) for more examples and discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What could go wrong with rescaling or mean scaling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"Minimum: {}, Maximum: {}\".format(X.min(), X.max()))\n",
    "print(\"Minimum scaled: {}, Maximum scaled: {}\".format(X_scaled.min(), X_scaled.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Linear Regression\n",
    "\n",
    "One thing we can try is to fit our data using linear regression. In this case, it is even easier than before because we don't have to do any transformations. This is called multi-linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_scaled, y) #fit the model\n",
    "r2 = model.score(X_scaled, y) #get the \"score\", which is equivalent to r^2\n",
    "\n",
    "yhat = model.predict(X_scaled) #create the model prediction\n",
    "\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $r^2$ score is pretty low, but we can't really visualize the model since we have 11-dimensional inputs. One thing to check is a \"parity plot\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y, yhat,alpha=0.15)\n",
    "ax.plot(y,y, '-k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model was perfect, the data would fall along the parity line. We see that there is a lot of scatter, but some correlation. In your case study you will apply kernel ridge regression to this dataset, but for now we will consider how we can select or modify the features to gain insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Selection\n",
    "\n",
    "The simplest strategy to select or rank features is to try them one-by-one, and keep the best feature at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_features = 4\n",
    "X_subset = X_scaled.copy()\n",
    "x_names_subset = np.copy(x_names)\n",
    "new_X = []\n",
    "new_X_names = []\n",
    "\n",
    "while len(new_X) < N_features:\n",
    "    r2_list = []\n",
    "    for j in range(X_subset.shape[1]):\n",
    "        model = LinearRegression() #create a linear regression model instance\n",
    "        xj = X_subset[:,j].reshape(-1,1)\n",
    "        model.fit(xj, y) #fit the model\n",
    "        r2 = model.score(xj, y) #get the \"score\", which is equivalent to r^2\n",
    "        r2_list.append([r2, j])\n",
    "    r2_list.sort() #sort lowest to highest\n",
    "    r2_max, j_max = r2_list[-1] #select highest r2 value\n",
    "    new_X.append(X_subset[:,j_max].copy())\n",
    "    new_X_names.append(x_names_subset[j_max])\n",
    "    np.delete(x_names_subset, j_max)\n",
    "    X_subset = np.delete(X_subset, j_max, axis=1)\n",
    "    \n",
    "print('The {} most linearly correlated features are: {}'.format(N_features, new_X_names))\n",
    "\n",
    "new_X = np.array(new_X).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the $r^2$ score changes if we use this subest of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(new_X, y) #fit the model\n",
    "r2 = model.score(new_X, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that even though there are 11 features, the model performance is similar with far fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlations and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that not all features improve model performance is related to **covariance**. Covariance is a measure of the correlation between different features. We can compute the covariance with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = np.cov(X_scaled.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(covar)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What is the relationship between the number of features and the dimensions of the covariance matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are visualizing the matrix here, and we can see that the diagonal is 1, while the off-diagonal elements vary from -1 to 1. The correlation turns out to be equivalent to the slope of a linear fit between the two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = np.polyfit(X_scaled[:,7], X_scaled[:,2], 1)\n",
    "m - covar[7,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Use for loops to compare the covariance matrix to a matrix of slopes from linear fits (15 points)\n",
    "\n",
    "Use `imshow` as above to compare the original covariance matrix with the one generated from linear fits.\n",
    "\n",
    "Note: The way these are computed is slightly different, so they may not match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not always true! If we compute the covariance of the unscaled matrix we get a different answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = np.cov(X.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(covar)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that all of the features have different \"units\", and therefore have much different variances. However, you can also use the \"correlation matrix\" to quickly get to the normalized result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(corr)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that some of the features are correlated means that they contain redundant information. Ideally, we would have features that all contained unique information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about uncorrelated features is that they should be \"orthogonal\". If we think of this in terms of Cartesian space, it is equivalent to saying that our axes are orthogonal. We can achieve this with linear algebra.\n",
    "\n",
    "The eigenvalue problem for a matrix $\\underline{\\underline{A}}$:\n",
    "\n",
    "$\\underline{\\underline{A}} v_n = \\lambda_n v_n$\n",
    "\n",
    "where $v_n$ is the $n$th eigenvector and $\\lambda_n$ is the $n$th eigenvalue.\n",
    "\n",
    "* Eigenvectors are **orthonormal**: $dot(v_i, v_j) = \\delta_{ij}$\n",
    "\n",
    "To calculate eigenvalues of a matrix, use the `eigvals` function, and for calculating both eigenvalues and eigenvectors, use the function `eig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigvals, eig\n",
    "\n",
    "eigvals, eigvecs = eig(corr)\n",
    "\n",
    "print(eigvals)\n",
    "print(np.dot(eigvecs[:,1], eigvecs[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Use a for loop to show that the eigenvectors are orthonormal (5 points)\n",
    "\n",
    "Use `np.dot(eigvecs.T,eigvecs)` and `imshow` as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that by taking the eigenvalues of the covariance matrix you are actually doing something called \"principal component analysis\". The eigenvectors of the covariance matrix identify the \"natural\" coordinate system of the data.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/PCA.gif\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues provide the variance in each direction, and we can use this to determine how much variance each principal component contributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCvals, PCvecs = eigvals, eigvecs\n",
    "total_variance = np.sum(np.real(PCvals))\n",
    "explained_variance = np.real(PCvals)/total_variance\n",
    "print(total_variance)\n",
    "print(explained_variance)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(explained_variance, 'o')\n",
    "ax.plot(np.cumsum(explained_variance),'or')\n",
    "ax.plot([0,10],[0.9, 0.9]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to say that 90% of the variance in the data is explained by the first 5 principal components.\n",
    "\n",
    "Finally, we can \"project\" the data onto the principal components. This is equivalent to re-defining the axes of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_projection = np.dot(X_scaled, PCvecs)\n",
    "print(PC_projection.shape)\n",
    "\n",
    "corr_PCs = np.corrcoef(PC_projection.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(corr_PCs)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After projection, we still have 11 features but they are now orthogonal - there is no covariance! This means that each one contains unique information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will talk a lot more about PCA during the dimensional reduction module, but for now you should know:\n",
    "\n",
    "* Principal component vectors are obtained from the eigenvalues of the covariance matrix\n",
    "* Principal components are orthogonal\n",
    "* Principal components explain the variance in multi-dimensional data\n",
    "* Data can be projected onto principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the projected data as inputs to a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(PC_projection, y) #fit the model\n",
    "r2 = model.score(PC_projection, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_scaled, y) #fit the model\n",
    "r2 = model.score(X_scaled, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the answer is the same. This is because we are still ultimately including all the same information. However, if we want to reduce the number of features we will see a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "model_PC = LinearRegression() #create a linear regression model instance\n",
    "model_PC.fit(PC_projection[:, :N], y) #fit the model\n",
    "r2 = model_PC.score(PC_projection[:, :N], y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 PCA = {}\".format(r2))\n",
    "\n",
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_scaled[:, :N], y) #fit the model\n",
    "r2 = model.score(X_scaled[:, :N], y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 regular = {}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA projection collects as much information as possible in each feature, and orders them by the amount of variance. We can also check them one-by-one to see how they correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "for j in range(PC_projection.shape[1]):\n",
    "    model = LinearRegression() #create a linear regression model instance\n",
    "    xj = PC_projection[:,j].reshape(-1,1)\n",
    "    model.fit(xj, y) #fit the model\n",
    "    r2 = model.score(xj, y) #get the \"score\", which is equivalent to r^2\n",
    "    score_list.append([r2, j])\n",
    "score_list.sort()\n",
    "score_list.reverse()\n",
    "\n",
    "for r, j in score_list:\n",
    "    print(\"{} : r^2 = {}\".format(j, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first principal component is the best, but the 5th is the second best, and the second is nearly last. This is because the principal components only use variance of the inputs, which may or may not correlate to the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Least Squares\n",
    "\n",
    "Finally, we will consider partial least squares, or PLS. Partial least squares is very similar to PCA, but instead of maximizing variance in the feature dimensions it maximizes the covariance between the inputs and outputs. The mathematics of this are somewhat complicated, so we will use the `scikit-learn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "N = 1\n",
    "model = PLSRegression(n_components=N)\n",
    "model.fit(X_scaled, y)\n",
    "r2 = model.score(X_scaled, y)\n",
    "print(\"{} Components: r^2 = {}\".format(N, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to PCA, PLS gives projections of the data. We can extract these from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PLS = np.dot(X_scaled, model.x_weights_)\n",
    "print(X_PLS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that we understand, we can use this PLS projection as an input to regular linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PC = LinearRegression() #create a linear regression model instance\n",
    "model_PC.fit(X_PLS, y) #fit the model\n",
    "r2 = model_PC.score(X_PLS, y) #get the \"score\", which is equivalent to r^2\n",
    "print(\"r^2 PCA = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $r^2$ matches that of the PLS model, even though we just used linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of Non-linear Models\n",
    "\n",
    "We see that the performance of the model is not very good, so we can see we will need to add some non-linearity. In 1-dimensional space we achieved this by adding transforms of the features as new features. However, for this is more challenging in a high-dimensional space since the number of features will scale with the number of dimension.\n",
    "\n",
    "#### Discussion: How many features would result if third-order interactions were considered?\n",
    "\n",
    "Kernel-based methods are very commonly used for high-dimensional spaces because they account for non-linear interactions, but the number of features does not exceed the number of data points.\n",
    "\n",
    "For your case study, you will explore KRR as a route to improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Regression models seek to map input \"features\" to a continuous output. There are two classes of regression models:\n",
    "\n",
    "* Parametric models: number of parameters does not depend on the number of data points\n",
    "* Non-parametric models: number of parameters increases as more data is added\n",
    "\n",
    "It is critical to optimize the complexity of regression models by varying their **hyperparameters**. The complexity can be optimized using **information criteria** or **regularization**.\n",
    "\n",
    "* Information criteria: Penalize the addition of more parameters to the model\n",
    "* Regularization: Reduces the magnitude of fitted parameters to control model smoothness\n",
    "\n",
    "**Cross-validation** is a key strategy for testing the ability of a regression model to generalize by \"hiding\" some of the data from the model when it is trained. There are three sets of data used in cross-validation:\n",
    "\n",
    "* training set: The model parameters are optimized using this data.\n",
    "* validation set: The model hyperparameters are optimized on this data.\n",
    "* test set: The model is tested on this set, but it is not used at any point in model development.\n",
    "\n",
    "In an ideal scenario the test set can be collected after the model is trained, but in most practical scenarios the test set is also simply hidden from the model in the beginning of the process. It is always important to consider the metrics you use to assess model accuracy, and quantify uncertainty wherever possible. Careful selection of a cross-validation and model assessment strategy is critical to developing a model that is useful in practice.\n",
    "\n",
    "Regression models can also be improved by selection or modification of the input **features**. This can be done via simple techniques such as **feature scaling** to ensure that the model is not affected by the physical dimensions of input features, or **forward selection** where features are added to the model based on how much they improve the performance. It is also possible to employ more complex approaches such as **principal component analysis** which creates new features as linear combinations of the original features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
