{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-Least-Squares-Regression*\" data-toc-modified-id=\"Linear-Least-Squares-Regression*-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Least-Squares Regression*</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-Libraries-and-Managing-Environments\" data-toc-modified-id=\"Importing-Libraries-and-Managing-Environments-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Importing Libraries and Managing Environments</a></span></li><li><span><a href=\"#Creating-vectors-and-basic-plots\" data-toc-modified-id=\"Creating-vectors-and-basic-plots-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Creating vectors and basic plots</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Write-code-that-extracts-the-$i^{th}$-value-from-$x$-and-$y$-(5-points)\" data-toc-modified-id=\"Exercise:-Write-code-that-extracts-the-$i^{th}$-value-from-$x$-and-$y$-(5-points)-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Exercise: Write code that extracts the $i^{th}$ value from $x$ and $y$ (5 points)</a></span></li><li><span><a href=\"#Exercise:-Create-a-new-dataset-for-linear-regression-(5-points)\" data-toc-modified-id=\"Exercise:-Create-a-new-dataset-for-linear-regression-(5-points)-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Exercise: Create a new dataset for linear regression (5 points)</a></span></li></ul></li><li><span><a href=\"#Derivation-of-Least-Squares-regression\" data-toc-modified-id=\"Derivation-of-Least-Squares-regression-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Derivation of Least-Squares regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-are-the-independent-variables-in-this-problem?\" data-toc-modified-id=\"Discussion:-What-are-the-independent-variables-in-this-problem?-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Discussion: What are the independent variables in this problem?</a></span></li><li><span><a href=\"#Exercise:-Compute-the-slope-and-intercept-for-the-$x$-and-$y$-dataset-defined-below-using-the-formulas-above.-Compare-the-results-to-the-actual-slope-and-intercept-by-computing-a-percent-error-(10-points)\" data-toc-modified-id=\"Exercise:-Compute-the-slope-and-intercept-for-the-$x$-and-$y$-dataset-defined-below-using-the-formulas-above.-Compare-the-results-to-the-actual-slope-and-intercept-by-computing-a-percent-error-(10-points)-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Exercise: Compute the slope and intercept for the $x$ and $y$ dataset defined below using the formulas above. Compare the results to the actual slope and intercept by computing a percent error (10 points)</a></span></li></ul></li><li><span><a href=\"#Assessing-Model-Accuracy*\" data-toc-modified-id=\"Assessing-Model-Accuracy*-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Assessing Model Accuracy*</a></span></li><li><span><a href=\"#Polynomial-Least-Squares-Regression-Derivation\" data-toc-modified-id=\"Polynomial-Least-Squares-Regression-Derivation-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Polynomial Least-Squares Regression Derivation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Construct-a-matrix,-$X_{ij}$-for-a-5th-order-polynomial-model-from-the-$x$-vector-given-below-(5-points)\" data-toc-modified-id=\"Exercise:-Construct-a-matrix,-$X_{ij}$-for-a-5th-order-polynomial-model-from-the-$x$-vector-given-below-(5-points)-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Exercise: Construct a matrix, $X_{ij}$ for a 5th-order polynomial model from the $x$ vector given below (5 points)</a></span></li></ul></li><li><span><a href=\"#Matrix-Algebra*\" data-toc-modified-id=\"Matrix-Algebra*-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Matrix Algebra*</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Write-a-Python-function-that-multiplies-two-matrices-based-on-the-header-below.-Do-not-use-anything-except-for-loops--(10-points).\" data-toc-modified-id=\"Exercise:-Write-a-Python-function-that-multiplies-two-matrices-based-on-the-header-below.-Do-not-use-anything-except-for-loops--(10-points).-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Exercise: Write a Python function that multiplies two matrices based on the header below. Do not use anything except for loops  (10 points).</a></span></li></ul></li><li><span><a href=\"#Linear-Systems-of-Equations*\" data-toc-modified-id=\"Linear-Systems-of-Equations*-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Linear Systems of Equations*</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Write-a-Python-function-that-solves-a-linear-system-of-equations-using-Gaussian-Elimination-(15-points)\" data-toc-modified-id=\"Exercise:-Write-a-Python-function-that-solves-a-linear-system-of-equations-using-Gaussian-Elimination-(15-points)-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Exercise: Write a Python function that solves a linear system of equations using Gaussian Elimination (15 points)</a></span></li></ul></li><li><span><a href=\"#Polynomial-Least-Squares-Regression*\" data-toc-modified-id=\"Polynomial-Least-Squares-Regression*-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Polynomial Least-Squares Regression*</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-Does-$r^2=1$-mean-that-this-is-a-good-model?\" data-toc-modified-id=\"Discussion:-Does-$r^2=1$-mean-that-this-is-a-good-model?-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Discussion: Does $r^2=1$ mean that this is a good model?</a></span></li><li><span><a href=\"#Exercise:-Use-a-for-loop-to-plot-the-value-of-$r^2$-as-a-function-of-the-number-of-polynomials-for-this-dataset-(10-points)\" data-toc-modified-id=\"Exercise:-Use-a-for-loop-to-plot-the-value-of-$r^2$-as-a-function-of-the-number-of-polynomials-for-this-dataset-(10-points)-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Exercise: Use a for loop to plot the value of $r^2$ as a function of the number of polynomials for this dataset (10 points)</a></span></li></ul></li><li><span><a href=\"#General-Linear-Models\" data-toc-modified-id=\"General-Linear-Models-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>General Linear Models</a></span></li></ul></li><li><span><a href=\"#Non-linear-Regression\" data-toc-modified-id=\"Non-linear-Regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Non-linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automatic-Differentiation\" data-toc-modified-id=\"Automatic-Differentiation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Automatic Differentiation</a></span></li><li><span><a href=\"#Multi-dimensional-optimization\" data-toc-modified-id=\"Multi-dimensional-optimization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Multi-dimensional optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-happens-if-we-change-the-parameters-of-the-steepest-descent-algorithm-(initial-guess,-number-of-steps,-step-size)?\" data-toc-modified-id=\"Discussion:-What-happens-if-we-change-the-parameters-of-the-steepest-descent-algorithm-(initial-guess,-number-of-steps,-step-size)?-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Discussion: What happens if we change the parameters of the steepest descent algorithm (initial guess, number of steps, step size)?</a></span></li></ul></li></ul></li><li><span><a href=\"#Python-Packages\" data-toc-modified-id=\"Python-Packages-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Python Packages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scipy-Optimizers\" data-toc-modified-id=\"Scipy-Optimizers-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Scipy Optimizers</a></span></li><li><span><a href=\"#Python-Classes\" data-toc-modified-id=\"Python-Classes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Python Classes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-How-would-you-create-a-loss-function-that-ensures-that-$\\beta_i$-is-positive?\" data-toc-modified-id=\"Discussion:-How-would-you-create-a-loss-function-that-ensures-that-$\\beta_i$-is-positive?-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Discussion: How would you create a loss function that ensures that $\\beta_i$ is positive?</a></span></li></ul></li><li><span><a href=\"#Scikit-Learn\" data-toc-modified-id=\"Scikit-Learn-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Scikit-Learn</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analytics is a topic that sits at the intersection of linear algebra, statistics, numerical methods, and computer programming. This course will not go deeply into any of these topics, and many of them should be topics you are familiar with, but it is useful to introduce some common terminology and review key concepts from these subjects.\n",
    "\n",
    "Furthermore, this course will utilize the Python programming language. Some of you may be familiar with Python, but others may have only worked with Matlab. Python is very similar to Matlab, but there are a few key differences and transitioning can take some effort. There is a nice [guide available for transitioning from MATLAB to Python](https://www.enthought.com/wp-content/uploads/Enthought-MATLAB-to-Python-White-Paper.pdf) which is strongly recommended for students who have not worked with Python before. The [Introduction to Python Programming](https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-1-Introduction-to-Python-Programming.ipynb) from Johansson is also a great place to start. We will not spend a lot of lecture time specifically discussing Python, but this lecture will introduce some important ideas and provide examples for using Python in data analytics. \n",
    "\n",
    "This module may feel very overwhelming, but don't worry. We will continue to review these concepts throughout the semester, and you are not expected to know all the details on the first pass. Topics that should be review from prior courses are marked with an astrerisk in the table of contents, so these should be familiar. The learning curve for this module may be steep, but we are here to help. We will hold extra help sessions, and encourage you to search internet resources like [Stack Overflow](https://stackoverflow.com/) and the official [Python tutorial](https://docs.python.org/3.6/tutorial/index.html) or work with more experienced students to learn.\n",
    "\n",
    "All of these concepts will be introduced from the context of regression. We will start with some toy datasets, then work with an infrared spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Least-Squares Regression*\n",
    "\n",
    "The simplest type of regression is linear least-squares regression, and is likely familiar to all engineers. The form of the model is given as:\n",
    "\n",
    "$y = mx + b + \\epsilon$\n",
    "\n",
    "where the $y$ is the independent value, $x$ is the dependent value, $m$ is the slope of the line, and $b$ is the intercept. This can also be written with indices on the data:\n",
    "\n",
    "$y_i = mx_i + b + \\epsilon_i$\n",
    "\n",
    "where $i$ refers to the index of the data point (e.g. the first, second, third, ... data point). We can also think of these quantities as vectors:\n",
    "\n",
    "$\\vec{y} = m\\vec{x} + b + \\vec{\\epsilon}$\n",
    "\n",
    "These notations will be used interchangeably throughout the course, so you should familiarize yourself with how they are equivalent. It is also common to use $\\hat{y}$ to designate the model prediction (without any error):\n",
    "\n",
    "$\\hat{y}_i = mx_i + b = y_i - \\epsilon_i$\n",
    "\n",
    "Let's use Python to create some data using this form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Managing Environments\n",
    "\n",
    "Before we can start creating or plotting data we need to understand some very basic things about how Python works. The Python programming language contains many features, and it also has the ability to be extended through the use of libraries. This is slightly different from MATLAB, where all functions and features are built in.\n",
    "\n",
    "Advantages:\n",
    "* Flexible\n",
    "* Lots of features\n",
    "\n",
    "Disadvantages:\n",
    "* Libraries must be imported/managed\n",
    "* Lack of standardization\n",
    "\n",
    "These libraries are hosted in different repositories, and must be installed and imported. Managing all of this can get rather tricky, and it is highly recommended that you us the [Anaconda](https://docs.anaconda.com/anaconda/install/) package for installing and managing Python. There are some additional [instructions](https://datacarpentry.org/OpenRefine-ecology-lesson/setup.html) provided by Software Carpentry to help you through this process. In this course we will use **Python 3.6**.\n",
    "\n",
    "Once you have everything installed correctly you should be able to launch and run this lecture on your own computer using the command `jupyter notebook` in your conda or operating system terminal.\n",
    "\n",
    "In this course we will use a variety of libraries, but the most basic are:\n",
    "\n",
    "* `numpy`: linear algebra and matrix manipulations. Many [MATLAB functions have `numpy` equivalents](https://docs.scipy.org/doc/numpy-1.15.0/user/numpy-for-matlab-users.html).\n",
    "\n",
    "* `scipy`: a collection of numerical methods algorithms useful for scientific analysis.\n",
    "\n",
    "* `matplotlib`: a plotting library for visualizing data.\n",
    "\n",
    "In order to use these libraries, we first need to install them. This can be acheived with one of two methods:\n",
    "\n",
    "`conda install numpy scipy matplotlib`\n",
    "\n",
    "`pip install numpy scipy matplotlib`\n",
    "\n",
    "There are slight differences between these two approaches, but they both work equally well in general. Typically `conda` is preferred, but many packages are only available via `pip`, so if `conda` can't find the package then try using `pip` instead.\n",
    "\n",
    "Finally, we can import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy\n",
    "import scipy\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happens... and that is a good thing! If you don't have any errors then that means the packages are successfully installed.\n",
    "\n",
    "This import strategy works, but there are few more tricks we need to discuss. As you will see later, it becomes very annoying to constantly type the full name of all the libraries you want, so it is very standard to \"alias\" commonly used libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also common that you only want specific functions or sub-packages from a library. We can import them directly rather than importing the whole library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about [import statements](https://docs.python.org/3.6/reference/import.html), but for now we have enough to do some plotting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vectors and basic plots\n",
    "\n",
    "Remember the functional form for our linear regression model:\n",
    "\n",
    "$y_i = mx_i + b + \\epsilon_i = \\hat{y}_i + \\epsilon_i$\n",
    "\n",
    "Let's use `numpy` to create some data for our dependent ($x$) variable and model prediction ($\\hat{y}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2.5 #arbitrarily choose m\n",
    "b = 12  #arbitrarily choos b\n",
    "x = np.linspace(0,10,10) #create a vector of 10 values ranging from 0 to 10\n",
    "yhat = m*x + b # create a dependent vector following the form of the model\n",
    "\n",
    "print(x)\n",
    "print(yhat)\n",
    "print(x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write code that extracts the $i^{th}$ value from $x$ and $y$ (5 points)\n",
    "\n",
    "**Hint:** Python starts indexing from 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "#write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the data using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.plot(x,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this looks like a line. We can use arguments similar to MATLAB to change the style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.plot(x,yhat, marker='o', linestyle='none', color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a line, as expected. However, it doesn't look like real data. We are missing the error term, $\\epsilon_i$.\n",
    "\n",
    "In order to add noise, we will use a Gaussian distribution, also known as a \"normal\" distribution. Hopefully you remember this from your statistics classes. We will need to use the `random` module of `numpy` to generate the random data. We can set the mean to zero, and the standard deviation to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "std_dev = 2\n",
    "epsilon = np.random.normal(mean,std_dev,size=10)\n",
    "y = m*x + b + epsilon\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, marker='o', color='k', linestyle='none')\n",
    "ax.plot(x, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every time you run the cell above you will get slightly different random data. Sometimes, especially in lectures, it is convenient to set a random \"seed\" so that we get a consistent dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "epsilon = np.random.normal(mean,std_dev,size=10)\n",
    "y = m*x + b + epsilon\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, marker='o', color='k', linestyle='none')\n",
    "ax.plot(x, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the results are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Create a new dataset for linear regression (5 points)\n",
    "\n",
    "The dataset should follow a linear model with a slope of 0.25 and an intercept of -0.1. The error should be normally distributed with a standard deviation of 0.1. The dataset should have 100 datapoints ranging from -1 to 1. \n",
    "\n",
    "Make a plot of the dataset, along with a line with a slope of 0.25 and an intercept of -0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Least-Squares regression\n",
    "\n",
    "In this case we know the slope and intercept because we created the dataset. However, in data analysis the goal is to find the slope and intercept given the data. The most common way to do this is with least-squares regression. The idea is simple: We want to minimize the sum of the squared error between the model and the data.\n",
    "\n",
    "First, we set up an \"objective function\" or \"loss function\" that quantifies the sum of squared errors:\n",
    "\n",
    "$L = \\sum_i \\epsilon_i^2$\n",
    "\n",
    "Now, we can solve for $\\epsilon_i$ from the model:\n",
    "\n",
    "$\\epsilon_i = y_i - mx_i - b$\n",
    "\n",
    "and substitute:\n",
    "\n",
    "$L = \\sum_i (y_i - mx_i -b)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What are the independent variables in this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to minimize the loss function, $L(m,b)$ so that we get the least possible value for the sum of squared errors - hence the name *least squares* regression.\n",
    "\n",
    "Hopefully you remember from calculus that we can find the minimum and maximum of a function by setting its derivatives equal to zero:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial m} = 0$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = 0$\n",
    "\n",
    "Calculating these derivatives is a little tedious, but its just basic calculus and algebra in the end. There is a good [walkthrough here](https://towardsdatascience.com/linear-regression-derivation-d362ea3884c2) if you want to see all the steps. The final result is:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = -2 \\sum_i (y_i - mx_i -b) = 0$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial m} =  \\sum_i -2x_i (y_i - mx_i -b) = 0$\n",
    "\n",
    "Manipulating the first expression yields:\n",
    "\n",
    "$b = \\sum_i (y_i - mx_i)/n = \\bar{y} - m\\bar{x}$\n",
    "\n",
    "where $n$ is the number of samples and $\\bar{x}$ and $\\bar{y}$ are the average of $x_i$ and $y_i$.\n",
    "\n",
    "Manipulating the second expression is a little tougher, but ultimately yields:\n",
    "\n",
    "$m = \\frac{\\sum_i^n (x_i y_i - \\bar{y}x_i)}{\\sum_i^n (x_i^2 - \\bar{x}x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Compute the slope and intercept for the $x$ and $y$ dataset defined below using the formulas above. Compare the results to the actual slope and intercept by computing a percent error (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [`linregress` function](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html) from `scipy` to find the slope and intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "m, b, r, p, std_slope = linregress(x,y)\n",
    "print('m: {:.3f}, b: {:.3f}'.format(m,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(linregress)\n",
    "from scipy import stats\n",
    "stats.linregress(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can see some examples of how to format strings in Python [here](https://mkaz.blog/code/python-string-format-cookbook/).\n",
    "\n",
    "These numbers should agree with the ones you computed above. We also see that the `linregress` function returns some other information (stored as `r`, `p`, and `stderr` above). This provides information about the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Model Accuracy*\n",
    "\n",
    "Now that we have solved the regression problem, we need some way of determining the quality of our solution. One very basic thing to consider, before even performing a regression analysis, is whether the data fits the assumptions of the model. In the case of linear regression, our assumptions are:\n",
    "\n",
    "* linear relationship between variables\n",
    "* normally distributed residuals\n",
    "* homoscedastic residuals\n",
    "\n",
    "The last two relate to the nature of the error distribution for the residuals, $\\epsilon_i$. It should follow a normal distribution, and \"homoskedastic\" is just a fancy way of saying that the standard deviation of the normal distribution does not vary with the independent variable. We can get a good sense for all of these by plotting the data along with the fit and the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize=(12,4))\n",
    "axes[0].plot(x,y,marker='o',linestyle='none',color='r')\n",
    "axes[0].plot(x, m*x +b, color='k')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[1].plot(x, y-m*x-b, linestyle='none', color='b', marker='o')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('$\\epsilon$', fontsize=14)\n",
    "axes[2].hist(y-m*x-b)\n",
    "axes[2].set_xlabel('$\\epsilon$')\n",
    "axes[2].set_ylabel('counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting your data and the error distribution is one of the quickest ways to see if the model is reasonable or not. In this case, we can see that there is a clear linear relationship between the variables, and that the model captures this relationship. We can also see that the error is reasonably normally distributed, and that the scatter does not seem to vary much as a function of $x$. None of this should be a surprise if you were paying attention to how the data was generated!\n",
    "\n",
    "There are also a few other metrics we can use to assess the quality of the model. The most common one is the \"r-squared\" value. The best way to think of this is a percent difference in the sum of squared error between the model and the average value of $y$:\n",
    "\n",
    "$r^2 = \\frac{\\sum_i (y_i - \\bar{y})^2 - \\sum_i (y_i - mx_i -b)^2}{\\sum_i (y_i - \\bar{y})^2} = \\frac{L(m,b)- \\sum_i (y_i - \\bar{y})^2}{\\sum_i (y_i - \\bar{y})^2}$\n",
    "\n",
    "We can calculate this and compare it to the Python output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.sum((y-m*x-b)**2)\n",
    "ydiff = np.sum((y-np.mean(y))**2)\n",
    "r2 = (ydiff-L)/(ydiff)\n",
    "\n",
    "print('r^2: {:.3f}'.format(r2))\n",
    "print('r^2 from scipy: {:.3f}'.format(r**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our results agree with the built-in implementation. It is worth noting that the two terms we used when computing $r^2$ are often called the \"sum of squared errors\" (SSE) and the \"total sum of squares\" (SST) in statistics:\n",
    "\n",
    "$SSE = L = \\sum_i (y_i - mx_i -b)^2$\n",
    "\n",
    "$SST = \\sum_i (y_i - \\bar{y})^2$\n",
    "\n",
    "We also have a few other outputs from the `linregress` function. The `p` is the $p$-value, which gives the probability that the coefficient is not zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is exceedingly small (less than one in a quadrillion), indicating that there is a very significant relationship between $x$ and $y$. This value is computed using a $t$-test which is a topic you may remember from statistics but we will not cover in this lecture. The other output, `std_slope` is the estimated standard deviation of the slope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('m = {:.3f} +/- {:.3f}'.format(m, std_slope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into how this is computed here, but it may be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Least-Squares Regression Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we looked at before was easy since it has a nice linear relationship. However, real data is rarely so simple. What happens if we have some more complex relationship? One common type of more complex data is based on polynomials:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 ... + \\epsilon_i$\n",
    "\n",
    "where $\\beta_0$ is an intercept (equivalent to $b$) and $\\beta_1$ is a linear slope (equivalent to $m$). This notation makes it easy to write a polynomial model as a summation:\n",
    "\n",
    "$y_i = \\sum_j^M \\beta_j x_{i}^j + \\epsilon_i$\n",
    "\n",
    "since $x_i^0=1$, we can see that these two forms are equivalent and the summation gives a polynomial model up to order $M$.\n",
    "\n",
    "We can also think of this slightly differently. Let's consider a matrix, $\\bar{\\bar{X}} = X_{ij}$, where each entry is given by:\n",
    "\n",
    "$X_{ij} = x_i^j$\n",
    "\n",
    "Let's see how this might work in Python for a 3rd-order polynomial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 3.5, 10)\n",
    "X = np.zeros((len(x), 4))\n",
    "X[:,0] = x**0\n",
    "X[:,1] = x**1\n",
    "X[:,2] = x**2\n",
    "X[:,3] = x**3\n",
    "\n",
    "X[2,3] == x[2]**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0,3] == x[0]**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Construct a matrix, $X_{ij}$ for a 5th-order polynomial model from the $x$ vector given below (5 points)\n",
    "\n",
    "**Hint:** You may want to [review how Python handles matrices and vectors](https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch02-code-listing.ipynb). One key note is that Python uses [element-wise multiplication by default](https://www.mathworks.com/help/matlab/ref/times.html), as opposed to the default matrix/vector multiplication in MATLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2.5, 2.5, 10)\n",
    "#insert code here\n",
    "X_list = []\n",
    "for i in range(0,6):\n",
    "    X_list.append(x**i)\n",
    "    \n",
    "X = np.array(X_list)\n",
    "X = X.T\n",
    "\n",
    "X[0,5] == x[0]**5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this new matrix we can write an even simpler expression:\n",
    "\n",
    "$y_i = \\sum_j^M \\beta_j X_{ij} + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Algebra*\n",
    "\n",
    "Now its time to switch gears and review some linear algebra. A few key definitions:\n",
    "\n",
    "* Dot product:\n",
    "\n",
    "$\\vec{a} \\cdot{} \\vec{b} = \\sum_i a_i b_i$\n",
    "\n",
    "* Matrix/vector multiplication:\n",
    "\n",
    "$\\bar{\\bar{A}} \\vec{x} = \\sum_j A_{ij} x_j$\n",
    "\n",
    "* Matrix/matrix multiplication:\n",
    "\n",
    "$\\bar{\\bar{A}} \\bar{\\bar{B}} = \\sum_j A_{ij} B_{jk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write a Python function that multiplies two matrices based on the header below. Do not use anything except for loops  (10 points).\n",
    "\n",
    "**Hint:** You may want review Python [function definitions](https://docs.python.org/3.6/tutorial/controlflow.html#defining-functions) and [for loops](https://docs.python.org/3.6/tutorial/controlflow.html#for-statements) for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiply(A, B):\n",
    "    n, m = A.shape\n",
    "    m, p = B.shape\n",
    "    AB = np.zeros((4,3)) #remove this line\n",
    "    #insert code here\n",
    "    return AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(4,5)\n",
    "B = np.random.rand(5,3)\n",
    "AB_numpy = np.dot(A,B)\n",
    "AB = matrix_multiply(A,B)\n",
    "np.isclose(AB_numpy, AB).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above block outputs `True` then your function is working correctly!\n",
    "\n",
    "**Note:** Comparing arrays can be tricky! You may want to read about the `isclose` function [here](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.isclose.html) and the `all` and `any` functions [here](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.logic.html#truth-value-testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the definition of matrix/vector multiplication with our simplified formula for a polynomial model we get:\n",
    "\n",
    "$y_i = \\sum_j^M \\beta_j X_{ij} + \\epsilon_i \\Rightarrow \\vec{y} =  \\bar{\\bar{X}} \\vec{\\beta} + \\vec{\\epsilon}$\n",
    "\n",
    "We can check this with Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3,10,10)\n",
    "X = np.zeros((len(x), 4))\n",
    "for j in range(4): #why not 3?\n",
    "    X[:,j] = x**j\n",
    "beta = [1.5, -1, 2, -0.1]\n",
    "y_linalg = np.dot(X,beta)\n",
    "y_linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = beta[0] + beta[1]*x + beta[2]*x**2 + beta[3]*x**3\n",
    "np.isclose(y_sum, y_linalg).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y_linalg, marker='x', color='r', linestyle='-')\n",
    "ax.plot(x, y_sum, marker='o', color='b', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can approach the problem the same way as simple linear regression, but we will have to use matrix algebra instead of linear algebra. Let's start by defining the sum of squared errors:\n",
    "\n",
    "$\\sum_i \\epsilon_i^2 = \\vec{\\epsilon}\\cdot\\vec{\\epsilon}^T$\n",
    "\n",
    "where $\\vec{\\epsilon}^T$ is the transpose of $\\vec{\\epsilon}$. Next we substitute in the model form:\n",
    "\n",
    "$\\vec{\\epsilon}\\cdot\\vec{\\epsilon}^T = (\\bar{\\bar{X}}\\vec{\\beta} - \\vec{y})(\\bar{\\bar{X}}\\vec{\\beta} - \\vec{y})^T$\n",
    "\n",
    "Recalling matrix transpose rules:\n",
    "\n",
    "$(\\bar{\\bar{X}}\\vec{\\beta} - \\vec{y})(\\bar{\\bar{X}}\\vec{\\beta} - \\vec{y})^T = (\\bar{\\bar{X}}\\vec{\\beta} - \\vec{y})(\\vec{\\beta}^T\\bar{\\bar{X}}^T - \\vec{y}^T)$\n",
    "\n",
    "and multiplying:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{\\beta}^T\\bar{\\bar{X}}^T\\bar{\\bar{X}}\\vec{\\beta} - \\vec{y}^T\\bar{\\bar{X}}\\vec{\\beta} - \\vec{\\beta}^T\\bar{\\bar{X}}^T\\vec{y} + \\vec{y}^T\\vec{y}$\n",
    "\n",
    "the middle two terms are both dot products of $\\vec{y}^T(\\bar{\\bar{X}}\\vec{\\beta})$ or $(\\bar{\\bar{X}}\\vec{\\beta})^T\\vec{y}$, which are symmetric and therefore equal giving:\n",
    "\n",
    "$\\vec{\\epsilon}\\cdot\\vec{\\epsilon}^T = \\vec{\\beta}^T\\bar{\\bar{X}}^T\\bar{\\bar{X}}\\vec{\\beta} - 2\\vec{y}^T\\bar{\\bar{X}}\\vec{\\beta} + \\vec{y}^T\\vec{y}$\n",
    "\n",
    "Next, we need to take the derivative of the loss function with respect to the parameters ($\\vec{\\beta}$) and set it equal to zero:\n",
    "\n",
    "$\\frac{\\partial \\vec{\\epsilon}\\cdot\\vec{\\epsilon}^T}{\\partial \\vec{\\beta}}$\n",
    "\n",
    "Taking derivatives with respect to vectors can be tricky, but the following two identities are useful:\n",
    "\n",
    "$\\frac{\\partial \\bar{\\bar{A}}\\vec{x}}{\\partial \\vec{x}} = \\bar{\\bar{A}}$\n",
    "\n",
    "$\\frac{\\partial \\vec{x}^T\\bar{\\bar{A}}^T\\bar{\\bar{A}}\\vec{x}}{\\partial \\vec{x}} = 2\\bar{\\bar{A}}^T\\bar{\\bar{A}}\\vec{x}$\n",
    "\n",
    "Using these identities you should be able to show that:\n",
    "\n",
    "$\\frac{\\partial \\vec{\\epsilon}\\cdot\\vec{\\epsilon}^T}{\\partial \\vec{\\beta}} = 2 \\bar{\\bar{X}}^T\\bar{\\bar{X}}\\vec{\\beta} - 2\\bar{\\bar{X}}^T\\vec{y}$\n",
    "\n",
    "Setting equal to zero gives:\n",
    "\n",
    "$\\bar{\\bar{X}}^T\\bar{\\bar{X}}\\vec{\\beta} = \\bar{\\bar{X}}^T\\vec{y}$\n",
    "\n",
    "Now we can notice that $\\bar{\\bar{X}}^T\\bar{\\bar{X}}$ is a matrix, which we can call $\\bar{\\bar{A}}$, and $\\bar{\\bar{X}}^T\\vec{y}$ is a vector, which we can call $\\vec{b}$. If we let $\\vec{\\beta} = \\vec{x}$ then we can see that this a system of linear equations:\n",
    "\n",
    "$\\bar{\\bar{A}}\\vec{x} = \\vec{b}$\n",
    "\n",
    "Let's set this up in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.dot(X.T, X)\n",
    "b = np.dot(X.T, y_linalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to know how to find $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Systems of Equations*\n",
    "\n",
    "Linear systems of equations are very common in engineering, and you should be familiar with [Gaussian Elimination](http://mathworld.wolfram.com/GaussianElimination.html) from your linear algebra course. Let's take a look at how to do this the easy way with `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.solve(A,b)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write a Python function that solves a linear system of equations using Gaussian Elimination (15 points)\n",
    "\n",
    "This should be good practice on writing Python functions, using for loops, and working with Numpy arrays. You can base your function on the header provided, and test it with the block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_elimination(A,b):\n",
    "    x = b #<- this is incorrect.\n",
    "    #write code here\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_x = np.linalg.solve(A, b)\n",
    "ge_x = gaussian_elimination(A, b)\n",
    "np.isclose(np_x, ge_x).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the block above returns `True` your function is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Least-Squares Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we recover exactly the same $\\beta$ vector that we input earlier. The reason is that we never added any error term to the data. Let's see what happens if we add some normally-distributed noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "stdev = 10\n",
    "y_noisy = y_linalg + np.random.normal(0,stdev,len(y_linalg))\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(x,y_linalg, '-ob')\n",
    "ax.plot(x,y_noisy, 'or')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-construct our $b$ vector (note that the $\\bar{\\bar{X}}$ matrix is the same) and solve for $\\vec{\\beta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.dot(X.T, y_noisy)\n",
    "beta_noisy = np.linalg.solve(A, b)\n",
    "beta_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are pretty different now! Let's see how the predicted data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.dot(X, beta_noisy)\n",
    "ax.plot(x,y_pred,'--k')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the coefficients are pretty far off, the prediction doesn't look too bad. This shows that it is sometimes possible to get a good fit, even if the model doesn't match the underlying data. We will discuss how to quantify error on these parameters later.\n",
    "\n",
    "We can also compute an $r^2$ value for the model. The definition is the same as before: it is the percent difference between the sum of squared errors and the sum of total errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = np.sum((y_noisy - np.dot(X, beta_noisy))**2)\n",
    "SST = np.sum((y_noisy - np.mean(y_noisy))**2)\n",
    "r2 = (SST - SSE)/SST\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 90.5% of the variance in the data is determined by the model, which is pretty good. Can we do better?\n",
    "\n",
    "One strategy is to add more polynomials, since in a real scenario we don't know how many polynomials are used to generate the data. Let's create a function that generates an independent variable matrix, $\\bar{\\bar{X}}$ for an arbitrary number of polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    # One-liner uses \"list comprehension\" to iterate through range 0 - N (note N+1 since range function is not inclusive)\n",
    "    # The input, x, is raised to the power of N for each value of N\n",
    "    # The result is converted to an array and transposed so that columns correspond to features and rows correspond to data points (individual x values)\n",
    "    return np.array([x**k for k in range(0,N)]).T\n",
    "\n",
    "x1 = np.linspace(0,10, 15)\n",
    "X1_poly = polynomial_features(x1,4)\n",
    "X1_poly[6,3] == x1[6]**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate $r^2$ for regression models with different numbers of polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "X_N = polynomial_features(x, N) #generate features\n",
    "b_N = np.dot(X_N.T, y_noisy) #generate b vector with new features\n",
    "A_N = np.dot(X_N.T, X_N) #generate A matrix with new features\n",
    "beta_N = np.linalg.solve(A_N, b_N) #solve Ax=b with new features\n",
    "print('beta = {}'.format(str(beta_N)))\n",
    "\n",
    "yhat_N = np.dot(X_N, beta_N) #compute predictions\n",
    "SSE_N = np.sum((y_noisy - yhat_N)**2) #compute sum of squared errors\n",
    "SST_N = np.sum((y_noisy - np.mean(y_noisy))**2) #compute total sum of squares\n",
    "r2_N = (SST_N - SSE_N)/SST_N # compute r^2\n",
    "print('r^2 = {}'.format(r2_N))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y_noisy, 'or')\n",
    "ax.plot(x, yhat_N, '--k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when $N=10$ our $r^2$ is 1.0, meaning the model explains 100% of the variance in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: Does $r^2=1$ mean that this is a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we try to use the model to predict new points that are not in the data we used for the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-3.1, 10.1, 50)\n",
    "X_test = polynomial_features(x_test, N)\n",
    "yhat_test = np.dot(X_test, beta_N)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y_noisy, 'or')\n",
    "ax.plot(x_test, yhat_test, '--k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so good - that is because the model is *overfitted* (we have the same number of parameters as data points). We will talk more about how to select the correct number of polynomials later in the course. For now, you should be able to recognize that as we add parameters to the model the $r^2$ always decreases, but the predictions may become worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Use a for loop to plot the value of $r^2$ as a function of the number of polynomials for this dataset (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Linear Models\n",
    "\n",
    "The idea of adding polynomials of increasing order into the model is a powerful concept because it allows us to make the model more flexible. However, we don't necessarily have to use polynomials (and as we will see later, simple polynomials are actually a bad idea).\n",
    "\n",
    "Let's consider a new dataset, and this time we will use something more realistic. We will load in a dataset from infrared spectroscopy of an ethanol molecule. The data was obtained [from NIST](https://webbook.nist.gov/cgi/cbook.cgi?ID=C64175&Units=SI&Type=IR-SPEC&Index=2#IR-SPEC), and we will use a new library called `pandas` to read it in. You will learn more about this in the \"Data Management\" module, but for now you just need to run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/ethanol_IR.csv')\n",
    "x_all = df['wavenumber [cm^-1]'].values\n",
    "y_all = df['absorbance'].values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_all,y_all)\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks a lot more complicated than our toy dataset from before. You don't need to understand the details of chemistry or infrared spectroscopy for this exercise. The key of analyzing these spectra is that the size and position of the peaks tells us information about the nature of the molecule. Let's make things easier by just selecting one of the peaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_peak = x_all[475:575]\n",
    "y_peak = y_all[475:575]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak,y_peak, '-b', marker='.')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we try to fit this with the polynomials from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "X_N = polynomial_features(x_peak, N) #generate features\n",
    "b_N = np.dot(X_N.T, y_peak) #generate b vector with new features\n",
    "A_N = np.dot(X_N.T, X_N) #generate A matrix with new features\n",
    "beta_N = np.linalg.solve(A_N, b_N) #solve Ax=b with new features\n",
    "print('beta = {}'.format(str(beta_N)))\n",
    "\n",
    "yhat_N = np.dot(X_N, beta_N) #compute predictions\n",
    "SSE_N = np.sum((y_peak - yhat_N)**2) #compute sum of squared errors\n",
    "SST_N = np.sum((y_peak - np.mean(y_peak))**2) #compute total sum of squares\n",
    "r2_N = (SST_N - SSE_N)/SST_N # compute r^2\n",
    "print('r^2 = {}'.format(r2_N))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'or')\n",
    "ax.plot(x_peak, yhat_N, '--k')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we add a lot of polynomials, the fit does not look very good. We never seem to capture the two peaks. Instead of polynomials we can see that the two peaks in the spectra look like the Gaussian distributions:\n",
    "\n",
    "$N(x, \\mu, \\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "We can use Gaussian functions, instead of polynomials, to construct our model:\n",
    "\n",
    "$y_i = \\sum_j \\beta_i \\exp\\left(-\\frac{(x_i-\\mu_j)^2}{2\\sigma_j^2}\\right)$\n",
    "\n",
    "Note the similarity to the polynomial expression:\n",
    "\n",
    "$y_i = \\sum_j \\beta_j x_i^j$\n",
    "\n",
    "Instead of transforming $x$ by making polynomials, we instead transform it by making Gaussian distributions. However, there is a catch. We need to set the mean, $\\mu_j$, and standard deviation, $\\sigma_j$ for each distribution.\n",
    "\n",
    "Let's first try to do this manually. If we look at the data, we can see that one peak is around 2900 and the other is around 2980. We can also guess that the standard deviation is around 25 wavenumber.\n",
    "\n",
    "Let's write this out:\n",
    "\n",
    "$y_i = \\beta_0 \\exp\\left(-\\frac{(x_i-2900)^2}{2(25^2)}\\right) + \\beta_1 \\exp\\left(-\\frac{(x_i-2980)^2}{2(25^2)}\\right)$\n",
    "\n",
    "Next, we can consider the transformed $x$ vectors as new vectors, $\\tilde{x}$:\n",
    "\n",
    "$y_i = \\beta_0 \\tilde{x}_{i,0} + \\beta_1 \\tilde{x}_{i,1}$\n",
    "\n",
    "where\n",
    "\n",
    "$\\tilde{x}_{i,0} = \\exp\\left(-\\frac{(x_i-2900)^2}{2(25^2)}\\right)$\n",
    "\n",
    "and\n",
    "\n",
    "$\\tilde{x}_{i,1} = \\exp\\left(-\\frac{(x_i-2980)^2}{2(25^2)}\\right)$\n",
    "\n",
    "Now, we can re-write the model as:\n",
    "\n",
    "$y_i = \\sum_{j=0}^{j=1} \\beta_j \\tilde{X}_{ij}$\n",
    "\n",
    "If we drop the tilde on the $X_{ij}$ matrix, we see that this is identical to the form of polynomial regression! This is actually called a \"generalized linear model\", since we can choose many different ways to transform $x$ to create the matrix $X$. Let's implement this one in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gauss = np.zeros((len(x_peak), 2))\n",
    "X_gauss[:,0] = np.exp(-(x_peak - 2900)**2/(2*(25**2)))\n",
    "X_gauss[:,1] = np.exp(-(x_peak - 2980)**2/(2*(25**2)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, X_gauss[:,0])\n",
    "ax.plot(x_peak, X_gauss[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can review the derivation for polynomial regression and convince yourself that the math is all the same in this case, so we can solve for $\\beta_i$ in the same way, we just use the new $X_{ij}$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.dot(X_gauss.T, X_gauss)\n",
    "b = np.dot(X_gauss.T, y_peak)\n",
    "beta = np.linalg.solve(A,b)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_peak = np.dot(X_gauss, beta)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'or')\n",
    "ax.plot(x_peak, yhat_peak, '--k')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! There are still some discrepancies, but things are much closer.\n",
    "\n",
    "Let's see what happens if we add more Gaussian peaks. Instead of picking them manually, we can just use a set number of Gaussians with a fixed standard deviation, and space them evenly. We can write a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_features(x, N , sigma = 25):\n",
    "    # x is a vector\n",
    "    # sigma is the standard deviation\n",
    "    xk_vec = np.linspace(min(x), max(x), N)\n",
    "    features = []\n",
    "    for xk in xk_vec:\n",
    "        features.append(np.exp(-((x - xk)**2/(2*sigma**2))))\n",
    "    return np.array(features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "X_N = gaussian_features(x_peak, N) #generate features\n",
    "b_N = np.dot(X_N.T, y_peak) #generate b vector with new features\n",
    "A_N = np.dot(X_N.T, X_N) #generate A matrix with new features\n",
    "beta_N = np.linalg.solve(A_N, b_N) #solve Ax=b with new features\n",
    "print('beta = {}'.format(str(beta_N)))\n",
    "\n",
    "yhat_N = np.dot(X_N, beta_N) #compute predictions\n",
    "SSE_N = np.sum((y_peak - yhat_N)**2) #compute sum of squared errors\n",
    "SST_N = np.sum((y_peak - np.mean(y_peak))**2) #compute total sum of squares\n",
    "r2_N = (SST_N - SSE_N)/SST_N # compute r^2\n",
    "print('r^2 = {}'.format(r2_N))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'or')\n",
    "ax.plot(x_peak, yhat_N, '--k')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the fit gets much better as more possible peaks are added. We can look at the $\\beta$ vector to see which peaks have large coefficients and use this to determine where peaks might be. However, there are a few problems with this approach:\n",
    "\n",
    "1) The peak width is fixed, so it may take multiple Gaussians to represent a wider peak.\n",
    "\n",
    "2) Some of the coefficients are negative, and we know that peaks should not have negative absorption.\n",
    "\n",
    "We will address these challenges in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we have only considered \"linear\" models that follow the form:\n",
    "\n",
    "$y_i = \\sum_j \\beta_j X_{ij} + \\epsilon_i$\n",
    "\n",
    "and all non-linear behavior has been captured by using non-linear transforms of $x_i$. However, in some cases we may want to optimize models that are not linear. For example, consider the Gaussian peak problem:\n",
    "\n",
    "$y_i = \\beta_0 \\exp\\left(-\\frac{(x_i-\\mu_0)^2}{2(\\sigma_0^2)}\\right) + \\beta_1 \\exp\\left(-\\frac{(x_i-\\mu_1)^2}{2(\\sigma_1^2)}\\right) + \\epsilon_i$\n",
    "\n",
    "Previously we just guessed values for $\\mu_i$ and $\\sigma_i$. However, it would be better if we could determine them from the data. Let's go back to the derivation of the linear regression equations. Remember that our goal is to minimize the sum of squared errors:\n",
    "\n",
    "$L = \\sum_i \\epsilon_i^2$\n",
    "\n",
    "We can solve for $\\epsilon_i$ from the model:\n",
    "\n",
    "$\\epsilon_i = y_i - \\beta_0 \\exp\\left(-\\frac{(x_i-\\mu_0)^2}{2(\\sigma_0^2)}\\right) + \\beta_1 \\exp\\left(-\\frac{(x_i-\\mu_1)^2}{2(\\sigma_1^2)}\\right) = y_i - \\sum_j \\beta_j N(x_i, \\mu_j, \\sigma_j)$\n",
    "\n",
    "and substitute:\n",
    "\n",
    "$L = \\sum_i (y_i - \\sum_j \\beta_j N(x_i, \\mu_j, \\sigma_j))^2$\n",
    "\n",
    "Now our loss function depends on all the parameters, $\\beta_j$, $\\mu_j$, and $\\sigma_j$!\n",
    "\n",
    "$L(\\beta_j, \\mu_j, \\sigma_j) = \\sum_i (y_i - \\sum_j \\beta_j N(x_i, \\mu_j, \\sigma_j))^2$\n",
    "\n",
    "Let's introduce a new vector, $\\vec{\\lambda}$, that is a vector containing all the parameters:\n",
    "\n",
    "$\\vec{\\lambda} = [\\vec{\\beta},\\; \\vec{\\mu},\\; \\vec{\\sigma}]$\n",
    "\n",
    "This is convenient since we can now write:\n",
    "\n",
    "$L(\\lambda_j) = \\sum_{i=0}^N (y_i - \\sum_j \\lambda_j N(x_i, \\lambda_{N+j}, \\lambda_{2N+j}))^2$\n",
    "\n",
    "and we can minimize the loss by setting the derivative equal to zero:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\lambda_j} = 0$\n",
    "\n",
    "This may look scary, but it is really just multivariate calculus!\n",
    "\n",
    "The hard part is how to get the derivative of $L$. Previously, for the case of linear regression, we derived this with matrix algebra, but that will be much more difficult in this case. Instead, we will use numerical methods this time around.\n",
    "\n",
    "First, let's implement our loss function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_loss(lamda, x, y, N=2):\n",
    "    yhat = np.zeros(len(y))\n",
    "    for i in range(N):\n",
    "        beta_i = lamda[i]\n",
    "        mu_i = lamda[N+i]\n",
    "        sigma_i = lamda[2*N+i]\n",
    "        yhat = yhat + beta_i*np.exp(-(x - mu_i)**2/(2*sigma_i**2))\n",
    "    squared_error = (y - yhat)**2\n",
    "    return np.sum(squared_error)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check by generating some data and testing the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,20)\n",
    "y = 0.3*np.exp((-(x-0.2)**2)/(2*(0.5**2))) + 0.7*np.exp(-(x-0.5)**2/(2*0.1**2))\n",
    "lamda = [0.3, 0.7, 0.2, 0.5, 0.5, 0.1]\n",
    "L = gaussian_loss(lamda, x, y, N=2)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our loss function is working properly. Now we need to know how to find the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Derivatives are needed a lot in machine learning! One development that has emerged from the field of computer science is the idea of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), also sometimes called \"algorithmic differentiation\". This is crucial to the success of well-known machine learning packages like \"TensorFlow\". The details of how it works are far too advanced for this course, and we will not use it often. However, it is definitely worth knowing about since many engineering applications also require derivatives.\n",
    "\n",
    "The simple version is that automatic differentiation does exactly what it sounds like: it gives you the derivative of a function automatically! We do need to use some special tools to do this in Python. The `autograd` package is the simplest, since it works well with `numpy`. We also need to write our functions in a specific way so that they only take one argument:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np   # autograd has its own \"version\" of numpy that must be used\n",
    "from autograd import grad # the \"grad\" function provides derivatives\n",
    "\n",
    "def L(lamda, x=x, y=y, N=2):\n",
    "    return gaussian_loss(lamda, x, y, N)\n",
    "\n",
    "diff_L = grad(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little annoying, but its worth it because now the derivatives are very easy to obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff_L(lamda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another sanity check: we know that the derivative should be zero if we are already at the optimum!\n",
    "\n",
    "Let's try with some other guess for $\\lambda_j$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_guess = [0.1, 1.0, 0.5, 0.3, 0.1, 0.4]\n",
    "print(diff_L(bad_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-dimensional optimization\n",
    "\n",
    "Now we know how to get the derivative ($\\frac{\\partial L}{\\partial \\lambda_j}$), but we still need to find a way to set it equal to zero ($\\frac{\\partial L}{\\partial \\lambda_j} = 0$).\n",
    "\n",
    "There are many ways we can achieve this. Some that should be familiar to you are:\n",
    "\n",
    "* Newton's method: Treat this as a root finding problem and use the second derivative, $\\frac{\\partial^2 L}{\\partial \\lambda_j \\lambda_k}$ to iteratively optimize.\n",
    "\n",
    "* Gradient descent/ascent: Increase or decrease the guess by \"walking\" along the gradient.\n",
    "\n",
    "These are typically \"iterative\" methods, which means we start with some initial guess then iteratively improve it. The simplest approach is to use gradient descent with a fixed step size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_guess = [0.35, 0.75, 0.21, 0.52, 0.53, 0.11]\n",
    "guess = bad_guess\n",
    "print('Initial Loss: {:.4f}'.format(L(guess)))\n",
    "\n",
    "N_iter = 100\n",
    "h = 0.5\n",
    "for i in range(N_iter):\n",
    "    guess -= h*np.array(diff_L(guess))\n",
    "    \n",
    "print('Final Loss: {:.4f}'.format(L(guess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss decreases after 100 iterations of gradient descent. Let's compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual Input: {}'.format(str(lamda)))\n",
    "print('Regression Result: {}'.format(str(guess)))\n",
    "\n",
    "\n",
    "def two_gaussians(lamda, x):\n",
    "    beta_0, beta_1, mu_0, mu_1, sigma_0, sigma_1 = lamda\n",
    "    y = beta_0*np.exp((-(x-mu_0)**2)/(2*(sigma_0**2))) + beta_1*np.exp(-(x-mu_1)**2/(2*sigma_1**2))\n",
    "    return y\n",
    "\n",
    "y = two_gaussians(lamda, x)\n",
    "yhat = two_gaussians(guess, x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, color='r')\n",
    "ax.plot(x, yhat, color='b', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this looks pretty good! The parameters look different, but it turns out that they are pretty close if you switch the order of the two peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What happens if we change the parameters of the steepest descent algorithm (initial guess, number of steps, step size)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of numerical algorithms for multi-dimensional optimization are tricky! However, most optimizers use a form of gradient descent. From now on we will typically let a built-in optimizer handle the hard work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Packages\n",
    "\n",
    "This lesson has reviewed many of the mathematical concepts needed for data analytics. However, in practice it is very inefficient to write your own algorithms. Writing algorithms that are fast and stable is hard work, so wherever possible you should rely on existing implementations.\n",
    "\n",
    "One big difference between Python and MATLAB is that Python is \"object oriented\". This means that the syntax can be quite different, as you may have noticed. We will not cover how to write Python classes in this course, but you will need to learn how to use classes written by others.  Things are even trickier since Python packages are developed by many different people, so the syntax differs between packages. However, we will only use a few packages in this class, and learning the syntax for a new package is generally a lot easier than writing it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy Optimizers\n",
    "\n",
    "Instead of trying to write a gradient descent algorithm let's use the [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html) function. This function has many algorithms available, but the `BFGS` algorithm is a good starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  scipy.optimize  import minimize\n",
    "\n",
    "result = minimize(L, bad_guess, method='BFGS')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output is a little complicated. That is because it is a Python \"class\". You don't need to know how to create classes, but you need to know how to interact with them. Classes are \"objects\" that have associated:\n",
    "\n",
    "* attributes: data/variables that are attached to a class\n",
    "* methods: functions that are attached to a class\n",
    "\n",
    "We can see the methods and attributes of a class using the `dir` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access attributes or methods using the `.` notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the optimization was successful, and gives us the final result. Let's compare this to the original input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual Input: {}'.format(str(result.x)))\n",
    "print('Regression Result: {}'.format(str(lamda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this was much faster than our naive gradient descent, and also appears to be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the real spectra we worked with earlier and try to optimize the peak positions and widths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we used guesses of the peak position and width:\n",
    "\n",
    "$y_i = \\beta_0 \\exp\\left(-\\frac{(x_i-2900)^2}{2(25^2)}\\right) + \\beta_1 \\exp\\left(-\\frac{(x_i-2980)^2}{2(25^2)}\\right)$\n",
    "\n",
    "then we optimized the parameters, $\\vec{\\beta}$, and found $\\beta_0 = 0.545$ and $\\beta_1 = 0.675$. We can convert these parameters into the $\\vec{\\lambda}$ format use our `two_gaussians` function to check the initial guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = [0.545, 0.675, 2900, 2980, 25, 25]\n",
    "y_guess = two_gaussians(guess, x_peak)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'b')\n",
    "ax.plot(x_peak, y_guess, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same loss function as before to optimize the other parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(lamda, x=x_peak, y=y_peak, N=2):\n",
    "    return gaussian_loss(lamda, x, y, N)\n",
    "\n",
    "result = minimize(L, guess, method='BFGS')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks successful! Let's see how well it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = result.x\n",
    "y_fitted = two_gaussians(fitted, x_peak)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'b')\n",
    "ax.plot(x_peak, y_fitted, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! We can also add constraints to the loss function. For example, we might expect that the peak width (standard deviation) should be similar for both peaks. We can enforce this by adding an additional term to the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_simwidth(lamda, x=x_peak, y=y_peak, N=2):\n",
    "    return gaussian_loss(lamda, x, y, N) + (lamda[-2] - lamda[-1])**2\n",
    "\n",
    "result = minimize(L_simwidth, guess, method='BFGS')\n",
    "fitted = result.x\n",
    "y_fitted = two_gaussians(fitted, x_peak)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'b')\n",
    "ax.plot(x_peak, y_fitted, 'r')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the fit quality is similar, but now the peak widths are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: How would you create a loss function that ensures that $\\beta_i$ is positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn\n",
    "\n",
    "Using the `minimize` function is great for minimizing the loss function, but it still requires you to set up the loss function. This is necessary for customizing methods, but most of the time we will use well-established models. In this case, we can utilize the `scikit-learn` package, which has most types of machine learning models.\n",
    "\n",
    "We will briefly examples of how to use `scikit-learn` for generalized linear regression and PCA. As with the `minimize` function there are some quirks to the syntax, but it is much easier than writing your own!\n",
    "\n",
    "First, let's use `scikit-learn` for generalized linear regression. One thing to note is that `scikit-learn` only solves the regression part of the problem, so we still need to set up the feature matrix, $X_{ij}$. We will keep working with the spectra example, and us the `gaussian_features` function that we wrote earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "N = 20\n",
    "\n",
    "X_N = gaussian_features(x_peak, N) #generate features\n",
    "\n",
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(X_N, y_peak) #fit the model\n",
    "r2 = model.score(X_N, y_peak) #get the \"score\", which is equivalent to r^2\n",
    "\n",
    "yhat = model.predict(X_N) #create the model prediction\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '-b')\n",
    "ax.plot(x_peak, yhat, '--r')\n",
    "print('r^2 = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that this is equivalent to the fitting we did earlier, and you can also see that making predictions and computing $r^2$ requires a lot less code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This module provides a rapid review/introduction to many of the key concepts in data analytics. In future modules we will discuss specific algorithms based on these concepts, and work on applying them to real datasets. It is recommended that you revisit this module occasionally to refresh yourself on the basic concepts. You are not expected to understand everything on the first time through, but hopefully by the end of the course these concepts will feel familiar and you will understand how they all come together in various data analytics and machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
