{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#What-error-metric-and-feature-scaling-strategy-will-you-choose-for-this-dataset?\" data-toc-modified-id=\"What-error-metric-and-feature-scaling-strategy-will-you-choose-for-this-dataset?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>What error metric and feature scaling strategy will you choose for this dataset?</a></span></li><li><span><a href=\"#What-hyperparameters-for-kernel-ridge-regression-provide-the-best-model-performance-if-all-features-are-included?\" data-toc-modified-id=\"What-hyperparameters-for-kernel-ridge-regression-provide-the-best-model-performance-if-all-features-are-included?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What hyperparameters for kernel ridge regression provide the best model performance if all features are included?</a></span></li><li><span><a href=\"#Is-it-possible-to-use-feature-selection-or-linear-feature-transformations-to-reduce-the-number-of-input-features-while-maintaining-similar-performance?\" data-toc-modified-id=\"Is-it-possible-to-use-feature-selection-or-linear-feature-transformations-to-reduce-the-number-of-input-features-while-maintaining-similar-performance?-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Is it possible to use feature selection or linear feature transformations to reduce the number of input features while maintaining similar performance?</a></span></li><li><span><a href=\"#What-is-the-best-accuracy-you-can-achieve-for-a-regression-model-evaluated-on-the-test-set?\" data-toc-modified-id=\"What-is-the-best-accuracy-you-can-achieve-for-a-regression-model-evaluated-on-the-test-set?-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>What is the best accuracy you can achieve for a regression model evaluated on the test set?</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Predicting band gaps of oxides from their structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson and excercises you were introduced to a dataset that contains structural information as well as formation energies and band gaps of transparent conducting oxide materials. We saw that linear models did not have very good performance, and in this case study you will build a kernel ridge regression (KRR) model to obtain more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "df = pd.read_csv('data/materials_band_gaps.csv')\n",
    "X = df.iloc[:,1:-2] #id is irrelevant\n",
    "x_names = X.columns.to_list()\n",
    "X = X.values\n",
    "y = df.iloc[:,-1].values #the last column is the band gap\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "print('Feature dimensions: {}'.format(X.shape))\n",
    "print('Output dimensions: {}'.format(y.shape))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(y)\n",
    "ax.set_xlabel('Band Gap [eV]')\n",
    "ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a test and a training set here. **All training and hyperparameter tuning should only include the training set!** This is very important, since accidentally optimizing parameters or hyperparameters on the test set can bias the model.  You can **only** use the test set to evaluate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)\n",
    "\n",
    "print('Feature dimensions: {}'.format(X.shape))\n",
    "print('Output dimensions: {}'.format(y.shape))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(y_train)\n",
    "ax.set_xlabel('Band Gap [eV]')\n",
    "ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our training data has the same general distribution as the testing data, which means it is a representative sample. Use this training set to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What error metric and feature scaling strategy will you choose for this dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What hyperparameters for kernel ridge regression provide the best model performance if all features are included?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it possible to use feature selection or linear feature transformations to reduce the number of input features while maintaining similar performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the best accuracy you can achieve for a regression model evaluated on the test set?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
