{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional Reduction\n",
    "\n",
    "<center>\n",
    "<img src=\"images/dimensional_reduction.png\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will introduce the concept of dimensional reduction and the benefits/challenges of high-dimensional data, along with a few key approaches to dimensional reduction.\n",
    "\n",
    "* Dimensional reduction basics\n",
    "    - Problem statement for dimensional reduction\n",
    "    - The curse and blessing of dimensionality\n",
    "    - Considerations in dimensional reduction\n",
    "    - Assessing performance of dimensional reduction\n",
    "* Familiar approaches\n",
    "    - Summary statistics\n",
    "    - Feature selection\n",
    "* Principal component analysis\n",
    "    - Linear PCA\n",
    "    - Kernel PCA\n",
    "    - Other variants\n",
    "* Other approaches\n",
    "    - Manifold-based models\n",
    "    - Autoencoding\n",
    "* Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional reduction basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement for dimensional reduction\n",
    "\n",
    "Reduction of the number of independendent variables under consideration while maintaining the properties of the statistical distribution of the underlying data.\n",
    "\n",
    "Dimensional reduction can be either **supervised** or **unsupervised**, but this lecture will focus on unsupervised approaches since they are the most distinct. Dimensional reduction can be used to extract/generate features from data, to get a better intuitive understanding of data, or to compress data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The blessing and the curse of dimensionality\n",
    "\n",
    "Dimensionality in mathematics is different from the dimensionality of the physical world. Engineers typically consider problems in 3 dimensions, but in data science the number of dimensions is equal to the number of features and can be extremely high (>10k). This leads to two phenomena:\n",
    "\n",
    "* The [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) refers to the fact that the volume of a high-dimensional space varies exponentially with the dimension. For example, consider the volume of a cube of length $L$ in dimension $d$:\n",
    "\n",
    "$V_d = L^d$\n",
    "\n",
    "If we want to sample this space with a resolution of $\\Delta L = L/N$ we would need a total of $N^d$ points. If $N=10$ and $d=100$ (a moderate number of dimensions in data science) then the total number of samples needed is $10^{100}$, which is [greater than the number of atoms in the known universe](https://www.universetoday.com/36302/atoms-in-the-universe/)!\n",
    "\n",
    "* The [blessing of dimensionality](https://arxiv.org/abs/1801.03421) is a somewhat lesser known phenomenon that occurs because of the data sparsity that arises from the curse of dimensionality. It essentially means that as the number of independent dimensions increases the data tends to be more easily separable and will look increasingly like well-separated points. This makes the data more well-suited to simplistic statistical assumptions such as being represented by a Gaussian distribution with equal covariance.\n",
    "\n",
    "The curse of dimensionality always applies, but the blessing is not guaranteed. This means that in general it is more challenging to work in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations for dimensional reduction\n",
    "\n",
    "* Matrix rank - how many independent dimensions are there?\n",
    "\n",
    "* Linearity of the subspace - are patterns linear or non-linear?\n",
    "\n",
    "* Projection - can a new high-dimensional point be projected onto the low-dimensional map?\n",
    "\n",
    "* Inversion - can a new low-dimensional point be projected back into high-dimensional space?\n",
    "\n",
    "* Supervised vs. unsupervised - are the training labels used to determine the dimensional reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing performance of dimensional reduction models\n",
    "\n",
    "As with clustering it can be challenging to assess the performance of dimensional reduction models, especially when unsupervised. Nonetheless there are a few approaches that can be used. Selecting the right approach will depend on the problem, but using a variety is always a good idea.\n",
    "\n",
    "* Variance\n",
    "\n",
    "One common idea in dimensional reduction is to assess the \"retained variance\" of the low-dimensional data. This is common in techniques such as PCA.\n",
    "\n",
    "* Distance\n",
    "\n",
    "The \"stress\" function compares the distance between points $i$ and $j$ in a low-dimensional space to the distance in the full-dimensional space (similar to the cophenetic coefficient for clustring linkages).\n",
    "\n",
    "$S(\\vec{x}_{0}, \\vec{x}_1, \\vec{x}_2, ... \\vec{x}_n) =  \\left(\\frac{\\sum_{i=0}^n \\sum_{i < j}(d_{ij} - ||x_i - x_j||)^2}{\\sum_{i=0}^n \\sum_{i < j} d_{ij}^2}\\right)^{1/2}$\n",
    "\n",
    "where $d_{ij}$ is the distance in the high-dimensional space and $\\vec{x}$ is the vector in the low-dimensional space. Some approaches seek to minimize this directly (e.g. multi-dimensional scaling), but it can also be used as an accuracy metric.\n",
    "\n",
    "* Visualization\n",
    "\n",
    "Where possible, visualizing the data in the low-dimensional space and looking for patterns is a very powerful approach. However this becomes challenging if the low-dimensional space is $>$3 dimensions.\n",
    "\n",
    "* Model performance\n",
    "\n",
    "If you have labels for the data one good approach is to construct a supervised model from both the low- and high-dimensional spaces and evaluating the accuracy of both. If the accuracy does not decrease then the key patterns are retained in the low-dimensional representation. This is also a pragmatic approach since one main use of dimensional reduction is to construct more efficient supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Familiar approaches\n",
    "\n",
    "First we will look at some approaches that should be familiar. In order to illustrate dimensional reduction we will use one of the most famous problems in machine learning: a database of hand-written digits. We can load this in and take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "digits,classes = load_digits(return_X_y=True)\n",
    "print(digits.data.shape)\n",
    "print(classes.shape)\n",
    "X_orig = np.array(digits.data)\n",
    "y = classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a dataset of 1797 data points with 64 dimensions. Lets see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(digit_data, n, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    img = digit_data[n].reshape(8,8)\n",
    "    ax.imshow(img,cmap='binary')\n",
    "    \n",
    "N = 3\n",
    "show_image(X_orig, N)\n",
    "print('Digit: {}, Min: {}, Max: {}'.format(y[N],X_orig.min(),X_orig.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What type of pre-processing would you recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement pre-processing\n",
    "#X = (X_orig - X_orig.mean(axis=0))/(np.max(X_orig.std(axis=0)+0.001))\n",
    "X = (X_orig - X_orig.min(axis=0))/(X_orig.max() - X_orig.min())\n",
    "#X = X > 0.4\n",
    "\n",
    "N = 9\n",
    "show_image(X, N)\n",
    "print('Digit: {}, Min: {}, Max: {}'.format(y[N],X.min(),X.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "A very basic way to reduce dimensions is to take \"summary statistics\". For example, we could take the mean and standard deviation of each 64 dimensional datapoint in order to create a 2-dimensional representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def mean_mode(X):\n",
    "    Xmean = X.mean(axis=1)\n",
    "    Xstd = X.std(axis=1)\n",
    "    X_ms = np.column_stack((Xmean,Xstd))\n",
    "    return X_ms\n",
    "\n",
    "print(X.shape)\n",
    "X_ms = mean_mode(X)\n",
    "print(X_ms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assess the distances between datapoints before/after the dimensional reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def stress(X_reduced, X):\n",
    "    D_red = pdist(X_reduced)\n",
    "    D_tot = pdist(X)\n",
    "    numerator = np.sum((D_tot - D_red)**2)\n",
    "    denom = np.sum(D_tot**2)\n",
    "    return np.sqrt(numerator/denom)\n",
    "\n",
    "print(stress(X, X))\n",
    "print(stress(X_ms, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is this a good dimensional reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have labels for the data we can also look at the low-dimensional data and see if it captures the distinguishing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_ms[:,0], X_ms[:,1], c=y)\n",
    "\n",
    "def add_labels(ax, cmap=plt.cm.viridis):\n",
    "    colors = [cmap((i/9)) for i in range(10)]\n",
    "    xpos = 0.1\n",
    "    for label in range(0,10):\n",
    "        c = colors[label]\n",
    "        ax.annotate(str(label), xy=[xpos, 1.1], xycoords='axes fraction', color=c, size=15)\n",
    "        xpos += 0.07\n",
    "\n",
    "add_labels(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly there are not any obvious patterns here. Summary statistics discard a considerable amount of information, but they are straightforward to use. Summary statistics are:\n",
    "\n",
    "* Unsupervised - We did not use the labels to determine the statistics\n",
    "* Projectable - It is also easy to project a new data point into the reduced dimensional space\n",
    "* Not invertible - There is no way to go back from the reduced dimensional space to the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection as dimensional reduction\n",
    "\n",
    "Another approach to dimensional reduction is to utilize a supervised model to perform feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "N = 10\n",
    "kbest = SelectKBest(f_classif, k=N)\n",
    "X_new = kbest.fit_transform(X, y)\n",
    "print(X_new.shape)\n",
    "print(stress(X_new,X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this is working better than the summary statistics. We can visualize the first two components in a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_new[:,0], X_new[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patterns are still not obvious, but there is clearly more separation. Lets take a look at the features that are selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "selected = kbest.get_support().reshape(8,8)\n",
    "ax.imshow(selected,cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Which features are least likely to be selected as we continue to increase the dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also assess the dimensional reduction by comparing the accuracy of a classification model trained on the full and reduced dimension models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "idxs = range(len(X))\n",
    "train_idxs, test_idxs, y_train, y_test = train_test_split(idxs, y, test_size=0.5)\n",
    "X_train_full = X[train_idxs]\n",
    "X_test_full = X[test_idxs]\n",
    "\n",
    "X_train_lowD = X_new[train_idxs]\n",
    "X_test_lowD = X_new[test_idxs]\n",
    "\n",
    "clf_full = svm.SVC()\n",
    "clf_full.fit(X_train_full,y_train)\n",
    "y_pred_full = clf_full.predict(X_test_full)\n",
    "accuracy_full = accuracy_score(y_pred_full, y_test)\n",
    "\n",
    "clf_lowD = svm.SVC()\n",
    "clf_lowD.fit(X_train_lowD,y_train)\n",
    "y_pred_lowD = clf_lowD.predict(X_test_lowD)\n",
    "accuracy_lowD = accuracy_score(y_pred_lowD, y_test)\n",
    "\n",
    "print(accuracy_full)\n",
    "print(accuracy_lowD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the feature selection approach works well, but it requires knowing the outputs. The feature selection approach is **supervised**. It is also projectable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Is feature selection invertible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "Hopefully this is also familiar, but as a quick refresher the principal component analysis is obtained via the **eigenvalues** of the **covariance matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.cov(X.T)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.imshow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of PCA from the perspective of an objective function. The objective function for PCA is:\n",
    "\n",
    "$||\\underline{\\underline{X}} - \\underline{\\underline{A}}||_F$\n",
    "\n",
    "where $||\\underline{\\underline{M}}||_F = \\sum_i \\sum_j M_{ij}^2$ is the \"Frobeneius norm\" and $\\underline{\\underline{A}}$ is a \"low rank\" approximation of $\\underline{\\underline{X}}$.\n",
    "\n",
    "Obviously if this objective function is simply minimized for all the elements of $\\underline{\\underline{A}}$ it will give us $\\underline{\\underline{X}}$, which is rather trivial. The key is that we apply a constraint on the rank:\n",
    "\n",
    "$\\min_{\\underline{\\underline{A}}} ||\\underline{\\underline{X}} - \\underline{\\underline{A}}||_F$ subject to $rank(\\underline{\\underline{A}}) \\leq k$\n",
    "\n",
    "In other words we are looking for the closest rank-$k$ matrix to the original matrix $\\underline{\\underline{X}}$.\n",
    "\n",
    "Note: We will not derive the connection between this minimization problem and the covariance matrix eigenvalues here, but details are available in Hastie 14.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the definition of matrix rank?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is also determined by its non-zero eigenvalues. Another way to think about PCA for dimensional reduction is that it uses only the $k$ largest eigenvalues of the covariance matrix to reconstruct the data. If we remember back to earlier lectures, the eigenvalues of the covariance data also give us the variance of the data along the principal component axes. Therefore we find that PCA gives a rank-$k$ approximation of the data that **retains the maximum possible amount of variance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(C)\n",
    "eig_vecs = eig_vecs.T #<- note that the eigenvectors are the *columns* by default\n",
    "eig_idxs = range(0,len(eig_vals)) #<- they are also not sorted by default, so we need to do some more work\n",
    "vals_ids = list(zip(eig_vals, eig_idxs))\n",
    "vals_ids.sort()\n",
    "vals_ids.reverse()\n",
    "eig_vals, eig_idxs = zip(*vals_ids)\n",
    "eig_vals = np.array(eig_vals)\n",
    "eig_idxs = list(eig_idxs)\n",
    "\n",
    "eig_vals_sorted = eig_vals[eig_idxs]\n",
    "eig_vecs_sorted = eig_vecs[eig_idxs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eig_vals_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eig_vals_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What is the rank of this matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the top 3 eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_matrix = np.row_stack(eig_vecs_sorted)\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "show_image(eig_matrix,0,ax=axes[0])\n",
    "show_image(eig_matrix,1,ax=axes[1])\n",
    "show_image(eig_matrix,2,ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the top $k$ eigenvalues to project the original data onto a $k$-dimensional space (the \"axes\" of this space are the eigenvectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "projector = np.column_stack(eig_vecs[:k])\n",
    "print(projector.shape)\n",
    "X_k = np.dot(X,projector)\n",
    "print(X_k.shape)\n",
    "\n",
    "print('Retained variance:', sum(eig_vals_sorted[:k])/sum(eig_vals_sorted))\n",
    "print('Stress:', stress(X_k, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize this in a 2-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "projector = np.column_stack(eig_vecs[:k])\n",
    "X_k = np.dot(X,projector)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_k[:,0], X_k[:,1], c=y)\n",
    "add_labels(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is substantial separation of the digits even in a 2-dimensional space.\n",
    "\n",
    "Another very useful feature of PCA is that it is **invertable**. We can move between the low-dimensional and high-dimensional representations using the projection matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "projector = np.column_stack(eig_vecs[:k])\n",
    "X_k = np.dot(X,projector)\n",
    "X_reconstruct = np.dot(X_k,projector.T) + X.mean()\n",
    "\n",
    "\n",
    "N = 0\n",
    "fig,axes = plt.subplots(1,2,figsize=(10,5))\n",
    "show_image(X, N, ax=axes[0])\n",
    "show_image(X_reconstruct, N, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the accuracy of a classification model as a function of number of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "k = 5\n",
    "projector = np.column_stack(eig_vecs[:k])\n",
    "X_k = np.dot(X,projector)\n",
    "\n",
    "idxs = range(len(X))\n",
    "train_idxs, test_idxs, y_train, y_test = train_test_split(idxs, y, test_size=0.5)\n",
    "X_train_full = X[train_idxs]\n",
    "X_test_full = X[test_idxs]\n",
    "\n",
    "X_train_lowD = X_k[train_idxs]\n",
    "X_test_lowD = X_k[test_idxs]\n",
    "\n",
    "clf_full = svm.SVC()\n",
    "clf_full.fit(X_train_full,y_train)\n",
    "y_pred_full = clf_full.predict(X_test_full)\n",
    "accuracy_full = accuracy_score(y_pred_full, y_test)\n",
    "\n",
    "clf_lowD = svm.SVC()\n",
    "clf_lowD.fit(X_train_lowD,y_train)\n",
    "y_pred_lowD = clf_lowD.predict(X_test_lowD)\n",
    "accuracy_lowD = accuracy_score(y_pred_lowD, y_test)\n",
    "\n",
    "print(accuracy_full)\n",
    "print(accuracy_lowD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can combine PCA with other dimensional reduction techniques like feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_PCA = 40\n",
    "projector = np.column_stack(eig_vecs[:k_PCA])\n",
    "X_k = np.dot(X,projector)\n",
    "print(X_k.shape)\n",
    "\n",
    "k_kB = 10\n",
    "kbest = SelectKBest(f_classif, k=k_kB)\n",
    "X_new = kbest.fit_transform(X_k, y)\n",
    "print(X_new.shape)\n",
    "print(kbest.get_support())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Will feature selection select the largest variance principal components for every problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is one of the most widely used techniques in dimensional reduction because it is:\n",
    "\n",
    "* Unsupervised - We did not use the labels to determine the statistics\n",
    "* Projectable - It is also easy to project a new data point into the reduced dimensional space\n",
    "* Invertible - It is easy to move from the low-dimensional space to the high dimensional space\n",
    "\n",
    "However, its weakness is that it is linear in the original space. It does not do well with non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "The solution to non-linearity in PCA is a familiar one: using a \"kernel\" to perform PCA in an even-higher dimensional space that captures non-linearities. The concept here is that rather than using the covariance matrix the eigenvalues of a \"kernel matrix\" are used:\n",
    "\n",
    "$K_{ij} = \\kappa(\\vec{x}_i, \\vec{x}_j)$ where $\\kappa$ is a kernel function such as the radial basis function:\n",
    "\n",
    "$\\kappa_{rbf}(\\vec{x}_i, \\vec{x}_j) = \\exp(-\\gamma ||\\vec{x}_i - \\vec{x}_j||^2)$\n",
    "\n",
    "We will not go into the details, but instead show an example of how it works with the `scikit-learn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "\n",
    "k = 1\n",
    "X_m, y_m = make_moons(n_samples=100, random_state=0)\n",
    "kPCA = KernelPCA(n_components=k, kernel='rbf', gamma=15, fit_inverse_transform=True)\n",
    "lPCA = PCA(n_components=k)\n",
    "\n",
    "lPCA.fit(X_m)\n",
    "X_PCA = lPCA.transform(X_m)\n",
    "\n",
    "kPCA.fit(X_m)\n",
    "X_kPCA = kPCA.transform(X_m)\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(15,5))\n",
    "axes[0].scatter(X_m[:,0], X_m[:,1], c=y_m)\n",
    "axes[1].scatter(X_PCA, np.zeros(X_PCA.size), c=y_m)\n",
    "axes[2].scatter(X_kPCA, np.zeros(X_PCA.size), c=y_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while PCA fails to separate the two datasets, kernel PCA is successful! However, we had to choose a hyperparameter ($\\gamma$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How could $\\gamma$ be selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA is also invertable, much like regular PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA_reconstruct = lPCA.inverse_transform(X_PCA)\n",
    "X_kPCA_reconstruct = kPCA.inverse_transform(X_kPCA)\n",
    "\n",
    "fig,axes = plt.subplots(1,3,figsize=(15,5))\n",
    "axes[0].scatter(X_m[:,0], X_m[:,1], c=y_m)\n",
    "axes[1].scatter(X_PCA_reconstruct[:,0], X_PCA_reconstruct[:,1], c=y_m)\n",
    "axes[2].scatter(X_kPCA_reconstruct[:,0], X_kPCA_reconstruct[:,1], c=y_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the reconstruction itself doesn't look particularly good, but some key features of the original structure are preserved with kPCA, while the PCA reconstruction looks drastically different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other PCA variants\n",
    "\n",
    "PCA is one of the most powerful dimensional reduction techniques and has many other variants. We will not go into the details, but a few worth mentioning are:\n",
    "\n",
    "* Partial least squares - supervised regression-based PCA that maximizes covariance between input and output\n",
    "* Linear discriminant analysis - supervised classification-based PCA that maximizes inter-class variance\n",
    "* Robust PCA - good for cases where there is sparse data and/or large errors/outliers\n",
    "\n",
    "Lets quickly compare PCA, kernel PCA and LDA for the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "k=2\n",
    "\n",
    "lPCA = PCA(n_components=k)\n",
    "lPCA.fit(X)\n",
    "X_PCA = lPCA.transform(X)\n",
    "\n",
    "kPCA = KernelPCA(n_components=k, kernel='rbf', gamma=0.5, fit_inverse_transform=True)\n",
    "kPCA.fit(X)\n",
    "X_kPCA = kPCA.transform(X)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda.fit(X,y)\n",
    "X_lda = lda.transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "axes[0].scatter(X_PCA[:,0], X_PCA[:,1],c=y)\n",
    "axes[1].scatter(X_kPCA[:,0], X_kPCA[:,1],c=y)\n",
    "axes[2].scatter(X_lda[:,0], X_lda[:,1],c=y)\n",
    "\n",
    "add_labels(axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other dimensional reduction approaches\n",
    "\n",
    "There are many other techniques that can be applied for dimensional reduction. Here we will briefly introduce two concepts: manifold learning and autoencoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold learning\n",
    "\n",
    "Manifold learning approaches utilize distance metrics between points to define their similarity, and then seek to minimize the difference between distance metrics in the high- and low-dimensional spaces. The advantage of distance metrics over variance is that the **local structure** of data (distances between points) can be more easily exploited. This makes manifold learning techniques much better suited for highly non-linear datasets.\n",
    "\n",
    "One prototype manifold learning technique is **multi-dimensional scaling**. The principle is that the \"stress\" metric which we introduced earlier is directly minimized. The stress is given by:\n",
    "\n",
    "$S(\\vec{x}_{0}, \\vec{x}_1, \\vec{x}_2, ... \\vec{x}_n) =  \\left(\\frac{\\sum_{i=0}^n \\sum_{i < j}(d_{ij} - ||x_i - x_j||)^2}{\\sum_{i=0}^n \\sum_{i < j} d_{ij}^2}\\right)^{1/2}$\n",
    "\n",
    "where $d_{ij}$ is the distance in the high-dimensional space and $\\vec{x}$ is the vector in the low-dimensional space. A typical choice is to use the Euclidean distance to compute $d_{ij}$, but there are variants of MDS that use other distance metrics. For example \"non-metric\" MDS uses distances based on ordering between different points, so that the relative ordering of distances is favored over the numerical value of distances.\n",
    "\n",
    "The optimization problem is rather challenging, so we will just use the `scikit-learn` implementation to see how MDS works for the hand-written digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, n_init=1, max_iter=100) #<- note that we need to give some max_iteration and initial guess parameters since this is iterative\n",
    "X_mds = mds.fit_transform(X) #<- note that there is no transform method. What does this mean?\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_mds[:,0], X_mds[:,1], c=y)\n",
    "add_labels(ax)\n",
    "print('Stress: ', stress(X_mds, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly some clusters, but the separation is not much better than PCA or other approaches. Adding more iterations or initial configurations may improve things.\n",
    "\n",
    "Another popular manifold-based method is tSNE, or t-distribution stochastic neighbor embedding. This uses a probabalistic similarity metric based on the t-distribution, which makes it somewhat better suited to retain both local and global structure. The details of this are well beyond this course, but we can see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30.0, \n",
    "            early_exaggeration=12.0, \n",
    "            learning_rate=200.0, \n",
    "            n_iter=1000,\n",
    "            init='random',\n",
    "            method='exact')\n",
    "\n",
    "X_tsne = mds.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_tsne[:,0], X_tsne[:,1], c=y)\n",
    "add_labels(ax)\n",
    "print('Stress: ', stress(X_tsne, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is slightly better separation than MDS and PCA, but still considerable overlap. There are also many additional hyper-parameters that don't have clear meaning (e.g. \"perplexity\"), and the outcome will depend on the initial guesses and the algorithms used. While tSNE can be powerful, it also typically requires substantial effort.\n",
    "\n",
    "Some other common manifold-based techniques include:\n",
    "\n",
    "* Isomap\n",
    "* Locally linear embedding (LLE)\n",
    "* Spectral embedding\n",
    "* Local tangent space alignment (LTSA)\n",
    "\n",
    "A [comparison](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html):\n",
    "\n",
    "<center>\n",
    "<img src=\"images/manifold_techniques.png\" width=\"800\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold techniques can give powerful insight into the high-dimensional structure of data; however, most suffer from several key disadvantages:\n",
    "\n",
    "* Not projectable - the low dimensional representation only applies to the training points.\n",
    "* Not invertible - no way to move back to high-dimensional space\n",
    "* Slow - manifold techniques use distance matrices and hence tend to scale as $N^2$\n",
    "\n",
    "For these reasons manifold techniques are best for providing insight into the structure of the data, but usually need to be combined with other dimensional reduction approaches for model construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoding\n",
    "\n",
    "The final approach we will discuss is \"autoencoding\", which is the use of neural networks for dimensional reduction. This is a relatively new approach without any implementation in `scikit-learn`, but it is conceptually different from others so it is included here.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/autoencoder.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "The idea is that you train a neural network with the same data as inputs and outputs, but use an intermediate hidden layer (or layers) with dimensionality smaller than the original data. This forces the data through a \"bottleneck\" where it is represented in a low-dimensional form. This has numerous advantages:\n",
    "\n",
    "* projectable and invertible - the link between the high/low dimensional representation is defined by the neural net\n",
    "* fast and scalable - neural networks are computationally efficient\n",
    "* non-linear and unsupervised - the autoencoder learns the non-linear manifold without needing labels\n",
    "\n",
    "However, the typical cautions of neural networks apply:\n",
    "\n",
    "* extremely large training datasets needed\n",
    "* architecture and hyperparameters need to be tuned/selected\n",
    "* no intuitive link between low- and high-dimensional representations\n",
    "\n",
    "This is a field of research on its own, but worth being aware of nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* High-dimensional data is common in engineering: images, signals, etc.\n",
    "\n",
    "* High-dimensional data has unique attributes:\n",
    "    - Curse of dimensionality\n",
    "    - Blessing of dimensionality\n",
    "    \n",
    "* Dimensional reduction techniques can be used to:\n",
    "    - Visualize data\n",
    "    - Engineer model features\n",
    "    - Improve model efficiency\n",
    "    - Compress data\n",
    "    \n",
    "* Various techniques can be used to assess the quality of reduced-dimensional data\n",
    "\n",
    "* Principal component analysis is a prototype for dimensional reduction that is:\n",
    "    - Unsupervised\n",
    "    - Projectable\n",
    "    - Invertable\n",
    "\n",
    "* Principal component analysis can be thought of as:\n",
    "    - determining the best rank-$k$ approximation of a matrix based on the Frobenius norm\n",
    "    - determining the rank-$k$ matrix that retains the maximum possible variance of the original data\n",
    "    - A projection based on the largest $k$ eigenvalues of the covariance matrix\n",
    "   \n",
    "* Three general approaches to dimensional reduction were introduced:\n",
    "    - Retaining variance (PCA-based approaches, global structure considered)\n",
    "    - Retaining distances (manifold approaches, local structure emphasized)\n",
    "    - Intermediate representation (autoencoders, neural-network based)\n",
    "    \n",
    "* A combination of different dimensional reduction techniques is typically required in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading:\n",
    "\n",
    "* Hastie Ch. 14.5-14.9\n",
    "* Hastie Ch. 18\n",
    "* [Sebastian Raschka PCA tutorial](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "* [Sebastian Raschka kernel PCA](https://sebastianraschka.com/Articles/2014_kernel_pca.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
