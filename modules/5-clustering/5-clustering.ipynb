{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Clustering-basics\" data-toc-modified-id=\"Clustering-basics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clustering basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-statement\" data-toc-modified-id=\"Problem-statement-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Problem statement</a></span></li><li><span><a href=\"#Types-of-problems/algorithms\" data-toc-modified-id=\"Types-of-problems/algorithms-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Types of problems/algorithms</a></span></li><li><span><a href=\"#Accuracy-and-distance-metrics\" data-toc-modified-id=\"Accuracy-and-distance-metrics-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Accuracy and distance metrics</a></span></li></ul></li><li><span><a href=\"#Dataset-Preparation\" data-toc-modified-id=\"Dataset-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset Preparation</a></span></li><li><span><a href=\"#Expectation-maximization-models\" data-toc-modified-id=\"Expectation-maximization-models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Expectation-maximization models</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-means\" data-toc-modified-id=\"k-means-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>k-means</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-How-many-clusters-will-the-algorithm-find?\" data-toc-modified-id=\"Discussion:-How-many-clusters-will-the-algorithm-find?-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Discussion: How many clusters will the algorithm find?</a></span></li><li><span><a href=\"#Discussion:-How-do-we-know-when-to-stop-iterating?\" data-toc-modified-id=\"Discussion:-How-do-we-know-when-to-stop-iterating?-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Discussion: How do we know when to stop iterating?</a></span></li><li><span><a href=\"#Exercise:-Write-a-function-that-performs-k-means-clustering-and-compare-the-runtime-for-your-function-to-the-one-from-scikit-learn-with-the-full-PCA-dataset\" data-toc-modified-id=\"Exercise:-Write-a-function-that-performs-k-means-clustering-and-compare-the-runtime-for-your-function-to-the-one-from-scikit-learn-with-the-full-PCA-dataset-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Exercise: Write a function that performs k-means clustering and compare the runtime for your function to the one from <code>scikit-learn</code> with the full PCA dataset</a></span></li></ul></li><li><span><a href=\"#Gaussian-mixture-models\" data-toc-modified-id=\"Gaussian-mixture-models-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Gaussian mixture models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Expectation-step:\" data-toc-modified-id=\"Expectation-step:-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Expectation step:</a></span></li><li><span><a href=\"#Maximization-step:\" data-toc-modified-id=\"Maximization-step:-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Maximization step:</a></span></li><li><span><a href=\"#Discussion:-How-do-we-know-how-many-clusters-there-are?\" data-toc-modified-id=\"Discussion:-How-do-we-know-how-many-clusters-there-are?-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Discussion: How do we know how many clusters there are?</a></span></li><li><span><a href=\"#Exercise:-Plot-the-Silhouette-Score-for-a-Gaussian-Mixture-model-with-full-covariance-as-a-function-of-number-of-clusters-(2-10)-for-the-PCA-dataset.\" data-toc-modified-id=\"Exercise:-Plot-the-Silhouette-Score-for-a-Gaussian-Mixture-model-with-full-covariance-as-a-function-of-number-of-clusters-(2-10)-for-the-PCA-dataset.-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Exercise: Plot the Silhouette Score for a Gaussian Mixture model with full covariance as a function of number of clusters (2-10) for the PCA dataset.</a></span></li></ul></li></ul></li><li><span><a href=\"#Density-based-models\" data-toc-modified-id=\"Density-based-models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Density-based models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mean-shift-algorithm\" data-toc-modified-id=\"Mean-shift-algorithm-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Mean shift algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-happens-if-the-initial-guess-is-very-far-away-from-a-cluster?\" data-toc-modified-id=\"Discussion:-What-happens-if-the-initial-guess-is-very-far-away-from-a-cluster?-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Discussion: What happens if the initial guess is very far away from a cluster?</a></span></li><li><span><a href=\"#Discussion:-How-could-we-modify-the-algorithm-to-find-more-clusters?\" data-toc-modified-id=\"Discussion:-How-could-we-modify-the-algorithm-to-find-more-clusters?-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Discussion: How could we modify the algorithm to find more clusters?</a></span></li></ul></li><li><span><a href=\"#DBSCAN\" data-toc-modified-id=\"DBSCAN-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>DBSCAN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Discussion:-What-happens-if-we-want-to-predict-the-cluster-of-a-new-point?\" data-toc-modified-id=\"Discussion:-What-happens-if-we-want-to-predict-the-cluster-of-a-new-point?-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Discussion: What happens if we want to predict the cluster of a new point?</a></span></li></ul></li></ul></li><li><span><a href=\"#Hierarchical-models\" data-toc-modified-id=\"Hierarchical-models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Hierarchical models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dendrograms\" data-toc-modified-id=\"Dendrograms-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Dendrograms</a></span></li><li><span><a href=\"#Agglomerative-hierarchical-clustering\" data-toc-modified-id=\"Agglomerative-hierarchical-clustering-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Agglomerative hierarchical clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Construct-a-dendrogram-for-the-data-generated-by-TSNE-dimensional-reduction.-Provide-a-recommendation-on-the-appropriate-number-of-clusters-using-any-of-the-strategies-described-above.\" data-toc-modified-id=\"Exercise:-Construct-a-dendrogram-for-the-data-generated-by-TSNE-dimensional-reduction.-Provide-a-recommendation-on-the-appropriate-number-of-clusters-using-any-of-the-strategies-described-above.-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Exercise: Construct a dendrogram for the data generated by TSNE dimensional reduction. Provide a recommendation on the appropriate number of clusters using any of the strategies described above.</a></span></li></ul></li></ul></li><li><span><a href=\"#Interpreting-Results\" data-toc-modified-id=\"Interpreting-Results-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Interpreting Results</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#Further-reading\" data-toc-modified-id=\"Further-reading-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Further reading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "<img src=\"images/clusters.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture will cover basic considerations and concepts in clustering data, and introduce a few basic classes of algorithms along with examples.\n",
    "\n",
    "* Clustering basics\n",
    "    - Problem statement for clustering\n",
    "    - Types of clustering problems/algorithms\n",
    "    - Accuracy and distance metrics\n",
    "* Expectation-Maximization models\n",
    "    - k-means and vector quantization\n",
    "    - Gaussian mixture models\n",
    "* Density-based models\n",
    "    - Mean shift\n",
    "    - DBSCAN\n",
    "* Hierarchical clustering\n",
    "    - Dendrograms\n",
    "    - Agglomerative clustering\n",
    "* Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "Clustering algorithms seek to identify data points that are similar to each other based on a set of descriptive features.\n",
    "\n",
    "Clustering algorithms are **unsupervised** since they do not include output labels. This is an example of **exploratory data analysis** in which the goal is to extract insight about the dataset based on its inherent structure, rather than to build a model that predicts an output. These algorithms can be used for data compression, group assignment, searching, and/or model evaluation or feature extraction for supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of problems/algorithms\n",
    "\n",
    "There are a few key types of clustering algorithms:\n",
    "\n",
    "**Expectation-maximization** algorthims iteratively compute \"expected\" clusters and then \"maximize\" the parameters of the cluster to optimize the expectations. This is somewhat similar to an iterative version of a generalized linear classification algorithm: \"classes\" are assigned, then boundaries are found to optimize a cost function based on these classes. After this optimization the new class boundaries are used to assign classes again, and the optimization is repeated with new \"class\" labels.\n",
    "\n",
    "**Density-based** algorithms utilize local information about data points to identify regions where the data has similar density. Regions where there is substantially lower density of data form boundaries between these clusters. This is somewhat similar to k-nearest neighbors where classes are defined by local environments.\n",
    "\n",
    "**Hierarchical** algorithms map out the full network of connectivity within a dataset, then use a variable distance cutoff to assign clusters. These algorithms can be understood visually through a dendrogram, and have relatively few hyperparameters but they are more computationally demanding.\n",
    "\n",
    "A few considerations when selecting a clustering algorithm:\n",
    "\n",
    "* Some algorithms requre defining the number of clusters explicitly (e.g. most expectation-maximization algorithms) while others find this implicitly based on choice of hyperparameters (e.g. density-based or hierarchical)\n",
    "\n",
    "* Some algorithms allow **mixed membership** where points can belong to multiple clusters based on probabilities.\n",
    "\n",
    "* Some algorithms can identify/ignore outliers/noise (e.g. density-based), while others attempt to assign clusters to all points (e.g. expectation-maximization and hierarchical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and distance metrics\n",
    "\n",
    "Computing the accuracy of unsupervised models is difficult because there is no \"right answer\". However, it is possible to compute some quantitative metrics based on the concept of a cluster.\n",
    "\n",
    "* **Silhouette score** is defined for *each point* and is related to two distances:\n",
    "    - $a$ is the average distance between a point and all other points in its cluster\n",
    "    - $b$ is the average distance between a point and the points in the next nearest cluster\n",
    "    - $S = \\frac{b-a}{max(a,b)}$ is the silhoutte score\n",
    "    - $S = -1$ implies totally incorrect, $S=1$ implies totally correct\n",
    "    - Works best for dense, well-separated clusters\n",
    "    - Does not work well for density-based clusters (e.g. DBSCAN)\n",
    "    \n",
    "The silhouette score can help identify individual points that are not well-clustered, or an average/max silhouette score can be used to evaluate the quality of the entire clustering model. Other metrics can be used to evaluate the overall model:\n",
    "\n",
    "* **Variance ratio criterion** or \"Calinski-Harabaz score\" is related to the \"within class\" variance (similar to intra-class variance for classificaiton) and the \"between class\" variance (similar to the interclass variance for classification). The mathematical definition is available [here](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101) but is beyond the scope of this course.\n",
    "    - Variance ratio will be higher for dense and well-separated clusters\n",
    "    - Not bounded so it can be difficult to know what is \"good\" and what is \"bad\"\n",
    "    - Does not work well for density-based clusters (e.g. DBSCAN)\n",
    "    \n",
    "These approaches can be utilized to identify hyperparameters such as the number of clusters in the case where there is no *a priori* expectation about the number of clusters.\n",
    "\n",
    "Another common technique is to use clustering for classification problems. In this case the error metrics from classification can be applied (e.g. confusion matrices, precision, recall, etc.). The comparison of clustering and classification can provide insight into how well the classes are captured by proximity in the feature space.\n",
    "\n",
    "Finally, it is worth noting that essentially all clustering algorithms rely on some form of **distance metric**. The way that distance is defined can have substantial impact on how clustering analyses perform. Some common choices to compute the distance between two points $i$ and $j$:\n",
    "\n",
    "* Euclidean distance ($L_2$ norm): $D_{ij} = \\sqrt{sum((\\vec{x}_i - \\vec{x}_j)^2)}$\n",
    "* Manhattan distance ($L_1$ norm): $D_{ij} = sum(abs(\\vec{x}_i - \\vec{x}_j))$\n",
    "* Chebyshev distance ($L_\\infty$ norm): $D_{ij} = max(abs(\\vec{x}_i - \\vec{x}_j))$\n",
    "* Minkowsky distance ($L_P$ norm): $D_{ij} = (sum((\\vec{x}_i - \\vec{x}_j)^P)^{1/P}$\n",
    "\n",
    "It is also possible to define a weighted distance metric that can implicitly standardize the data, or weight nearby points much higher than far away points. An example is the Mahalanobis distance:\n",
    "\n",
    "* Mahalanobis distance: $D_{ij} = (\\vec{x}_i - \\vec{\\mu})^T \\underline{\\underline{C}}^{-1} (\\vec{x}_j - \\vec{\\mu})$\n",
    "    - $\\mu$ is the mean vector\n",
    "    - $\\underline{\\underline{C}}$ is the covariance matrix\n",
    "    \n",
    "   \n",
    "* Kernel distance: $D_{ij} = (\\vec{x}_i)^T \\underline{\\underline{K}} (\\vec{x}_j)$\n",
    "    - $\\underline{\\underline{K}}$ is a kernel-based weight matrix\n",
    "\n",
    "For simplicity we will typically default to Euclidean distance in most examples; however, changing distance metrics can substantially improve performance in real problems so it is worthwhile to experiment. This is usually as simple as changing a keyword for `scikit-learn` models, or writing a short function to compute the necessary distance.\n",
    "\n",
    "It is also useful to consider the **cophenetic correlation coefficient** when dealing with different distance metrics or \"linkages\" in hierarchical representations of high-dimensional data. This can be considered as a comparison between distance metrics and the Euclidean distance. This will be discussed more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "In this lecture we will work with a dataset of chemical process data provided by Dow Chemicals. The data comes from a generic chemical process with the following setup:\n",
    "\n",
    "<img src=\"images/dow_process.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a number of operating conditions for each of the units in the process, as well as the concentration of impurities in the output stream. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "df = pd.read_excel('data/impurity_dataset-training.xlsx')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.set_index('Date')\n",
    "idx = 5\n",
    "col = df.columns[idx]\n",
    "print(col)\n",
    "df[col].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of things we could do with this dataset, and you can choose to explore them by working with it for your final project. For this lecture we will treat each time point as a vector (defined by its operating conditions and corresponding impurity concentration) and apply some dimensional reduction and clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[1:]].values\n",
    "print(X.shape)\n",
    "print(X.dtype)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data consists of 10,703 entries with 45 dimensions each. The `type` is `object`, which suggests we may have non-numerical values. Let's see what happens if we try to convert `X` into an array of floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X = np.array(X,dtype='float')\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some `!` in the data. These are used to represent missing values. We can also see that there are some other artifacts in the data. We can clean these up by only accepting rows where all entries are real, finite numbers.\n",
    "\n",
    "We can filter these out using the `applymap` function of `pandas`, but we need a function that can determine if an entry is real and finite. We can easily write one using the `isreal` and `isfinite` functions of `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_real_and_finite(x):\n",
    "    if not np.isreal(x):\n",
    "        return False\n",
    "    elif not np.isfinite(x):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_map = df[df.columns[1:]].applymap(is_real_and_finite)\n",
    "real_rows = numeric_map.all(axis=1).copy().values #True if all values in a row are real numbers\n",
    "X_num = np.array(X[real_rows,:], dtype='float')\n",
    "print(X_num.shape)\n",
    "print(X_num.dtype)\n",
    "print(X_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data in a matrix of floating point numbers. Let's standardize it and visualize it with some dimensional reduction techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X_num - X_num.mean(axis=0))/X_num.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will apply some different dimensional reduction techniques to visualize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "pca = PCA(n_components = n_components)\n",
    "%time X_pca = pca.fit_transform(X)\n",
    "\n",
    "kpca = KernelPCA(n_components = n_components, kernel='rbf', gamma=0.1)\n",
    "%time X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "tsne = TSNE(n_components = n_components)#, n_neighbors=20)\n",
    "%time X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize = (15,4))\n",
    "data = [X_pca, X_kpca, X_tsne]\n",
    "labels = ['PCA', 'Kernel PCA', 'TSNE']\n",
    "for X_i, label, ax in zip(data, labels,axes):\n",
    "    ax.scatter(X_i[:,0], X_i[:,1], alpha=0.2)\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the structure of the low-dimensional representations depends on the technique we use. We will use these datasets as examples for various types of clustering models to see how we might end up with different \"clusters\" of chemical process parameters depending on the dimensional reduction and clustering technique we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-maximization models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means\n",
    "\n",
    "The k-means algorithm is the simplest and most intuitive clustering algorithm. It performs remarkably well under a number of assumptions:\n",
    "\n",
    "* Number of clusters are known\n",
    "* Clusters are roughly spherical\n",
    "* Clusters are separated by linear boundaries\n",
    "\n",
    "Even if these assumptions are violated, it often works anyway, especially in high dimensions (the \"blessing\" of dimensionality).\n",
    "\n",
    "The k-means algorithm works using the principal of **expectation-maximization**. This is an iterative type of algorithm that contains two basic steps:\n",
    "\n",
    "* Expectation: Assign points based on some \"expectation\" metric.\n",
    "* Maximization: Revise expectations based on maximizing a fitness metric.\n",
    "\n",
    "In the case of k-means we:\n",
    "\n",
    "* Expect that points close to the center of a cluster belong to that cluster\n",
    "* Maximize the proximity of points to the center of a cluster by moving the center\n",
    "\n",
    "This process is interated until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple \"toy\" implementation of k-means, and see how it works for the chemical process datasests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(pt1, pt2):\n",
    "    \"Euclidean distance between two points\"\n",
    "    #note that this can also be performed with np.linalg.norm(pt1-pt2)\n",
    "    return np.sqrt(sum([(xi-yi)**2 for xi, yi in zip(pt1, pt2)]))\n",
    "\n",
    "def expected_assignment(pt, cluster_centers):\n",
    "    # Expectation: find the closest points to each cluster center\n",
    "    dists = [dist(pt,ci) for ci in cluster_centers] #<- find distance to each center\n",
    "    min_index = dists.index(min(dists)) #<- find the index (cluster) with the minimum dist\n",
    "    return min_index\n",
    "\n",
    "def new_centers(cluster_points, centers):\n",
    "    # Maximization: maximize the proximity of centers to points in a cluster\n",
    "    centers = list(centers)\n",
    "    for i,ci in enumerate(cluster_points):\n",
    "        if ci != []:\n",
    "            centers[i] = np.mean(ci, axis=0)\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need an \"initial guess\" of cluster centers and we can apply the algorithm to a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_pca[::10,:] #thin the dataset out so that things run faster\n",
    "print(X.shape)\n",
    "\n",
    "#cluster_centers = ([-0.5,0], [0.5,0])\n",
    "cluster_centers = ([5,0], [20,0], [30,-5])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], alpha=0.2)\n",
    "colors = {0:'r', 1:'g', 2:'b',3:'m'}\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    ax.plot(ci[0], ci[1], marker='*', markersize='12', color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: How many clusters will the algorithm find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below repeatedly to see how the algorithm converges. Re-run the cell above to re-start the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Plot old centers\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    ax.plot(ci[0], ci[1], marker='+', markersize='12', color=colors[i])\n",
    "    \n",
    "# Which cluster do we \"expect\" each point to belong to?\n",
    "clusters = [[],[],[],[]]\n",
    "for pt in X:\n",
    "    cluster_idx = expected_assignment(pt, cluster_centers)\n",
    "    clusters[cluster_idx].append(pt)\n",
    "    \n",
    "# What centers best represent these new assignments?\n",
    "cluster_centers = new_centers(clusters, cluster_centers)\n",
    "\n",
    "# Plot new assignments\n",
    "for i, ci in enumerate(clusters):\n",
    "    for pt in ci:\n",
    "        ax.plot(pt[0], pt[1], marker='o', color=colors[i], alpha=0.2)\n",
    "        \n",
    "# Plot new centers\n",
    "for i,ci in enumerate(cluster_centers):\n",
    "    print(ci)\n",
    "    ax.plot(ci[0], ci[1], marker='*', markersize='12', color=colors[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: How do we know when to stop iterating?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is more efficient to utilize the `scikit-learn` implementation of `KMeans`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 3\n",
    "random_state = 20\n",
    "X = X_pca #scikit-learn is much more efficient, so we can run it on the whole datset\n",
    "\n",
    "model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "model.fit(X)\n",
    "y_predict = model.predict(X)\n",
    "centers = model.cluster_centers_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y_predict, cmap='viridis')\n",
    "for center in centers:\n",
    "    x_i = center[0]\n",
    "    y_i = center[1]\n",
    "    ax.plot(x_i, y_i, marker='*', color='k', mec='w', markersize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works for the other dimensional reduction techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [X_pca, X_kpca, X_tsne]\n",
    "labels = ['PCA', 'Kernel PCA', 'TSNE']\n",
    "fig, axes = plt.subplots(1,3, figsize = (15,4))\n",
    "\n",
    "n_clusters = 4\n",
    "random_state = 0\n",
    "\n",
    "for X_i, label, ax in zip(data, labels, axes):\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    model.fit(X_i)\n",
    "    y_predict = model.predict(X_i)\n",
    "    centers = model.cluster_centers_\n",
    "    ax.scatter(X_i[:,0], X_i[:,1], c=y_predict, cmap='viridis', alpha=0.5)\n",
    "    for center in centers:\n",
    "        x_i = center[0]\n",
    "        y_i = center[1]\n",
    "        ax.plot(x_i, y_i, marker='*', color='k', mec='w', markersize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Write a function that performs k-means clustering and compare the runtime for your function to the one from `scikit-learn` with the full PCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian mixture models\n",
    "\n",
    "Gaussian mixture models, or GMM's, are another clustering approach based on expectation maximization. The approach is to model each cluster as a Gaussian distribution, and to model the entire dataset as a mixture of Gaussians. Mathematically:\n",
    "\n",
    "$ P(\\vec{x}) = \\sum_k \\phi_k \\mathcal{N}(\\vec{x}, \\vec{\\mu}, \\vec{\\sigma})$\n",
    "\n",
    "where $\\mathcal{N}$ is the normal distribution:\n",
    "\n",
    "* one dimension: $N(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left( \\frac{-(x-\\mu)^2}{2 \\sigma ^2} \\right)$\n",
    "\n",
    "* multi-dimensional: $N(\\vec{x}, \\vec{\\mu}, \\underline{\\underline{\\Sigma}}) = \\frac{1}{(2 \\pi |\\underline{\\underline{\\Sigma}}|)} \\exp \\left( \\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\underline{\\underline{\\Sigma}}^{-1}  (\\vec{x} - \\vec{\\mu}) \\right)$\n",
    "\n",
    "where $\\underline{\\underline{\\Sigma}}$ is the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation step:\n",
    "\n",
    "We can calculate the expected probability that a point $i$ is in a cluster $k$ with the following formula for a 1d Gaussian:\n",
    "\n",
    "$\\gamma_{ik} = \\frac{\\phi_k \\mathcal{N}(\\vec{x}_i, \\vec{\\mu}_k, \\vec{\\sigma}_k)}{\\sum_j \\phi_j \\mathcal{N}(\\vec{x}_i, \\vec{\\mu}_j, \\vec{\\sigma}_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximization step:\n",
    "\n",
    "The parameters of the distributions can then be updated by calculating the maximum likelihood estimators for $\\phi$, $\\mu$, and $\\sigma$, similar to the way these parameters would be estimated for a single distribution:\n",
    "\n",
    "* $\\phi_k = \\sum_{i=1}^N \\frac{\\gamma_{ik}}{N}$\n",
    "* $\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}} $\n",
    "* $\\sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k)^2}{\\sum_{i=1}^N \\gamma_{ik}} $\n",
    "\n",
    "These parameters are derived by maximizing $P(\\vec{x})$ with respect to each parameter. The formulas for multi-dimensional Gaussians are derived in the same way but are more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/GMM.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture models are much more flexible than k-means models. Let's see how GMM's perform for some of the earlier datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "n_clusters = 3\n",
    "random_state = 0\n",
    "covariance_type = 'full' #full, tied, spherical\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize = (15,4))\n",
    "\n",
    "for X_i, label, ax in zip(data, labels, axes):\n",
    "    model = GaussianMixture(n_components=n_clusters, random_state=random_state, covariance_type=covariance_type)\n",
    "    model.fit(X_i)\n",
    "    y_predict = model.predict(X_i)\n",
    "    centers = model.means_\n",
    "    ax.scatter(X_i[:,0], X_i[:,1], c=y_predict, cmap='viridis', alpha=0.2)\n",
    "    for center in centers:\n",
    "        x_i = center[0]\n",
    "        y_i = center[1]\n",
    "        ax.plot(x_i, y_i, marker='*', color='k', mec='w', markersize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: How do we know how many clusters there are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "X_i = X_pca\n",
    "n_clusters = 6\n",
    "\n",
    "model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "model.fit(X_i)\n",
    "y_predict = model.predict(X_i)\n",
    "\n",
    "silhouette = silhouette_score(X_i, y_predict)\n",
    "c_h_score = calinski_harabasz_score(X_i, y_predict)\n",
    "\n",
    "print(silhouette)\n",
    "print(c_h_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Plot the Silhouette Score for a Gaussian Mixture model with full covariance as a function of number of clusters (2-10) for the PCA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-based models\n",
    "\n",
    "Density-based clustering algorithms consider local density of points and utilize this information to group points into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean shift algorithm\n",
    "\n",
    "The simplest density-based algorithm is the \"mean shift\" algorithm. This is similar to k-means in that we seek the centroid of each cluster. The difference is that in mean shift the number of clusters does not need to be specified. Instead a \"window\" is specified, and at each iteration the centroids are updated to centroid of all points in each window. Let's see how this works for a single point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(x1, x2):\n",
    "    # we will use the numpy 2-norm to calculate Euclidean distance:\n",
    "    return np.linalg.norm(x1-x2, 2) #<- the 2 is optional here since 2 is the default.\n",
    "\n",
    "def get_nearby_points(x, x_list, r):\n",
    "    # r is the radius\n",
    "    dist_pairs = []\n",
    "    for i,xi in enumerate(x_list):\n",
    "        dist = get_distance(x, xi)\n",
    "        dist_pairs.append([dist, i, xi]) #<- gives us the distance for each point\n",
    "    in_window = [pt[-1] for pt in dist_pairs if pt[0] <= r]\n",
    "    return in_window\n",
    "\n",
    "def get_new_centroid(old_centroid, x_list, r):\n",
    "    in_range = get_nearby_points(old_centroid, x_list, r)\n",
    "    if len(in_range) == 0:\n",
    "        new_centroid = old_centroid\n",
    "    else:\n",
    "        new_centroid = np.array(in_range).mean(axis=0)\n",
    "    return new_centroid\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the similarity to the kNN functions for prior lectures. It is a good idea to \"abstract out\" the distance function so that we could try other distance metrics easily.\n",
    "\n",
    "Let's apply this to a single point in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = [10,0] #<- set an initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_pca[::10,:] #thin the dataset out since our toy algorithm won't be very efficient\n",
    "r = 5\n",
    "\n",
    "nearby = get_nearby_points(guess, X, r)\n",
    "nearby = np.array(nearby)\n",
    "new = get_new_centroid(guess, X, r)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(guess[0], guess[1], marker='*', color='k', markersize=15)\n",
    "ax.scatter(X[:,0], X[:,1], color='k', alpha=0.3)\n",
    "ax.scatter(nearby[:,0], nearby[:,1], color='r', alpha=0.5)\n",
    "ax.plot(new[0], new[1], marker='*', color='r', markersize=15, mec='k')\n",
    "\n",
    "guess = new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run the block above to watch the point converge. You can play with the initial guess to see how it changes things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What happens if the initial guess is very far away from a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean shift algorithm causes an initial guess for a centroid to move toward a point of higher density. However, it isn't clear exactly how to get initial guesses. If we choose random points then some will have no points around them and not move. It also isn't clear how to decide how many initial guess points we should use.\n",
    "\n",
    "The solution to this is to use each point of the dataset as an initial guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift_iteration(x_list, r):\n",
    "    centroids = []\n",
    "    for centroid in x_list:\n",
    "        new = get_new_centroid(centroid, x_list, r)\n",
    "        centroids.append(new)\n",
    "    return centroids\n",
    "\n",
    "new_centroids = mean_shift_iteration(centroids, r)\n",
    "\n",
    "news = np.array(new_centroids)\n",
    "olds = np.array(centroids)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], color='k', alpha=0.3)\n",
    "ax.scatter(olds[:,0], olds[:,1], color='k', marker='*', alpha=0.5)\n",
    "ax.scatter(news[:,0], news[:,1], color='r', marker='*', alpha=0.5)\n",
    "\n",
    "centroids = new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the algorithm we just need to iterate until the new centrods are the same as the old centroids, and assign points to the nearest centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mean_shift_clustering(x_list, r, tolerance=0.01):\n",
    "    # tolerance will define when new and old centroids are the same.\n",
    "    old_centroids = np.array(x_list)\n",
    "    new_centroids = np.zeros(x_list.shape)\n",
    "    delta = np.linalg.norm(old_centroids - new_centroids)\n",
    "    while delta >= tolerance:\n",
    "        print('Working: delta = {:.2f}'.format(delta))\n",
    "        new_centroids = mean_shift_iteration(old_centroids, r)\n",
    "        delta = np.linalg.norm(old_centroids - new_centroids)\n",
    "        old_centroids = np.array(new_centroids)\n",
    "        \n",
    "    unique_centroids = []\n",
    "    for centroid in new_centroids:\n",
    "        unique = True\n",
    "        for uc in unique_centroids:\n",
    "            if np.linalg.norm(uc - centroid) <= tolerance:\n",
    "                unique = False\n",
    "        if unique == True:\n",
    "            unique_centroids.append(centroid)\n",
    "            \n",
    "    labels = []\n",
    "    for pt in x_list:\n",
    "        min_dist = 1e99\n",
    "        for j,centroid in enumerate(unique_centroids):\n",
    "            if get_distance(pt,centroid) < min_dist:\n",
    "                label = j\n",
    "                min_dist = get_distance(pt,centroid)\n",
    "        labels.append(label)\n",
    "            \n",
    "    return labels, np.array(unique_centroids)\n",
    "\n",
    "r = 5\n",
    "%time labels, centroids = mean_shift_clustering(X, r)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=labels, cmap='viridis', alpha=0.4)\n",
    "ax.scatter(centroids[:,0], centroids[:,1], marker='*', s=120, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: How could we modify the algorithm to find more clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare our mean shift algorithm to the one from `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "model = MeanShift(bandwidth=5)\n",
    "%time model.fit(X)\n",
    "labels = model.labels_\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=labels, cmap='viridis', alpha=0.4)\n",
    "ax.scatter(centroids[:,0], centroids[:,1], marker='*', s=120, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are the same, but `scikit-learn` is much faster. One note is that `scikit-learn` uses some slightly different techniques to speed things up, so `bandwidth` may not be exactly the same as `radius`. However, the results are generally similar and the concept is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "The DBSCAN algorithm also uses a local sliding window similar to mean shift, but instead of defining clusters by centroids it defines the cluster by whether or not a point falls within the sliding window. We will not go through the algorithm in detail, but the general steps are:\n",
    "\n",
    "1) Start with a random point and find its neigbhors within distance $r$.\n",
    "\n",
    "2) If there are a sufficient number of neigbhors (defined by a minimum points argument) then the clustering process starts. If not then the point is labeled as noise and a new point is selected until the clustering process starts.\n",
    "\n",
    "3) The neighbors within a distance $r$ are added to the cluster.\n",
    "\n",
    "4) The nearest neighbor is selected as the next point, and the same process is repeated until all points within distance $r$ of any point within a cluster are defined as being part of that cluster.\n",
    "\n",
    "5) Once a cluster has finished, a new point is selected and a new cluster is started. The process is repeated until all points have been assigned to a cluster or labeled as noise.\n",
    "\n",
    "The key hyperparameters are:\n",
    "    * r - the radius to include in a cluster\n",
    "    * min_samples - the minimum number of samples within a radius of $\\epsilon$ such that a point is not considered noise.\n",
    "    \n",
    "The following animation illustrates how the DBSCAN algorithm works:\n",
    "\n",
    "<img src=\"images/DBSCAN.gif\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of DBSCAN is that it can find clusters defined by highly non-linear boundaries, unlike k-means, mean shift, or even GMM's. Let's see how the `scikit-learn` implementation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = X_pca\n",
    "\n",
    "model = DBSCAN(eps=1, min_samples=3)\n",
    "y_predict = model.fit_predict(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y_predict, cmap='viridis', alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the clustering can be very sensitive to the hyperparameters! These hyperparameters will be related to the density of points, so you may be able to get a good guess based on intuition about the data or by looking at the data. However, some tuning is nearly always necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion: What happens if we want to predict the cluster of a new point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical models\n",
    "\n",
    "The final type of clustering we will discuss are \"hierarchical\" models. These models construct linkages between different points and use distance cutoffs to assign clusters. Examining the hierarchy of points is a useful way to get insight into the structure of a high-dimensional dataset without dimensional reduction. The downside is that it can be rather slow, since the algorithms scale as $N^3$. However, for the relatively small sizes of datasets typically encountered in engineering it is usually feasible to construct these hierarchies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrograms\n",
    "\n",
    "A \"dendrogram\" is a graphical representation of the distances between different points in some high-dimensional space. One intuitive but qualitative example of a dendrogram are the [species trees](https://www.instituteofcaninebiology.org/how-to-read-a-dendrogram.html) commonly used in biology:\n",
    "\n",
    "<img src=\"images/bio_dendrogram.png\" width=\"500\">\n",
    "\n",
    "We can see that it is possible to create different \"clusters\" of species based on different defining characteristics. By choosing more or less specific \"cutoffs\" we could create a few large clusters or many small clusters. The idea is similar for data sets. Let's see how it looks for some of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "X = X_pca[::10,:]\n",
    "\n",
    "Z = linkage(X, method='single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a closer look at the \"linkage\" output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape: {}'.format(X.shape))\n",
    "print('Z shape: {}'.format(Z.shape))\n",
    "print('Z[0]:', Z[150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"linkage\" output has 4 members. Each entry corresponds to the formation of a new cluster from smaller clusters, hence there are (n-1) entries (since the first entry involves 2 points, and each iteration adds one more point).\n",
    "\n",
    "* the first two entries are the index of the two points/clusters that are being combined (point 185 and 270)\n",
    "* the third entry is the distance between these clusters (0.0095)\n",
    "* the fourth entry is the total number of points in the new cluster (2 in this case)\n",
    "\n",
    "Note that we passed a \"method\" argument into the linkage. This describes the method that is used to calculate the distance between two clusters that have multiple points. There are more details available [here](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage), but a very qualitative descripton of some of the options:\n",
    "\n",
    "* `single`: take the minimum distance between any two points in the two clusters\n",
    "* `complete`: take the maximum distance between any two points in the two clusters\n",
    "* `average`: use an average of distances between points in the two clusters\n",
    "* `weighted`: weight distances differently between the agglomerated cluster and one being added\n",
    "* `centriod`: use the distance between cluster centroids\n",
    "* `ward`: use the distance that minimizes the variance between the clusters\n",
    "\n",
    "So, which one should we choose? This is where we can use the \"cophenetic coefficient\", which measures the ratio of the distance in \"linkage\" space to the distance in the high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Dij = pdist(X, metric='minkowski', p=10)\n",
    "for method in ['single','complete','average','weighted','centroid','ward']:\n",
    "    Z = linkage(X,method=method)\n",
    "    C, coph_dists = cophenet(Z,Dij)\n",
    "    print('cophenetic coefficient of {}: {}'.format(method, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dendrogram` function is a visual representation of this \"linkage\" structure. The \"color threshold\" tells the dendrogram a distance (y-axis value) below which to identify separate branches of the dendrogram as different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "Z = linkage(X,method='centroid')\n",
    "dendrogram(Z, color_threshold=20, ax=axes[1])\n",
    "axes[0].scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is easier to not show every single datapoint, and truncate the dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
    "dendrogram(Z, color_threshold=20, truncate_mode='lastp', p=10, ax=axes[1])\n",
    "axes[0].scatter(X[:,0],X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative hierarchical clustering\n",
    "\n",
    "Agglomerative clustering is easy to understand once the \"linkage\" structure makes sense. The number of clusters can be defined either explicitly (move up the tree until there are 'k' clusters) or implicitly (provide a linkage distance that defines separate clusters).\n",
    "\n",
    "The following animations illustrates this nicely:\n",
    "\n",
    "<img src=\"images/agglomerative.gif\" width=\"700\">\n",
    "\n",
    "The mechanics of doing this can be a little tricky, but luckily there are built-in functions to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "max_d = 20\n",
    "k = 4\n",
    "\n",
    "clusters_dist = fcluster(Z, max_d, criterion='distance')\n",
    "clusters_k = fcluster(Z, k, criterion='maxclust')\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,6))\n",
    "dendrogram(Z, color_threshold=max_d, truncate_mode='lastp', p=k, ax=axes[0])\n",
    "axes[1].scatter(X[:,0],X[:,1],c=clusters_dist)\n",
    "axes[2].scatter(X[:,0],X[:,1],c=clusters_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are options for determining the cutoffs automatically, but none of them are great! The most common is the inconsistency method, which monitors for \"jumps\" in the distance:\n",
    "\n",
    "* $I = \\frac{h-avg}{std}$\n",
    "    - $h$: merge height of cluster (length in y-directon on dendrogram)\n",
    "    - $avg$: average height of last $d$ merges\n",
    "    - $std$: standard deviation of last $d$ merges\n",
    "    \n",
    "If $I >= t$ where t is a specified threshold then this will be used as the cutoff. Let's see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "I_cutoff = 5\n",
    "clusters_I = fcluster(Z, I_cutoff, criterion='inconsistent', depth=10)\n",
    "n_clusters = max(clusters_I)\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(15,6))\n",
    "dendrogram(Z, color_threshold=3, truncate_mode='lastp', p=int(n_clusters), ax=axes[0])\n",
    "axes[1].scatter(X[:,0],X[:,1],c=clusters_dist)\n",
    "axes[2].scatter(X[:,0],X[:,1],c=clusters_k)\n",
    "axes[3].scatter(X[:,0],X[:,1],c=clusters_I)\n",
    "print('Number of clusters:', n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Construct a dendrogram for the data generated by TSNE dimensional reduction. Provide a recommendation on the appropriate number of clusters using any of the strategies described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "* Clustering algorithms are **unsupervised**.\n",
    "* Clustering is used for **exploratory data analysis**.\n",
    "* Assessing the accuracy of clustering methods is challenging if the labels are not known.\n",
    "* Distance metrics can make a big difference in how clustering algorithms perform.\n",
    "* Three types of algorithms were discussed:\n",
    "    -  Expectation Maximization\n",
    "    -  Density-based\n",
    "    -  Hierarchical\n",
    "* Some considerations for choosing an algorithm:\n",
    "    -  Is the number of clusters known?\n",
    "    -  Are data points expected to belong to a single cluster, or is **mixed membership** expected?\n",
    "    -  Is noise expected in the data?\n",
    "* Clustering algorithms are useful for:\n",
    "    -  Data compression\n",
    "    -  Searching data\n",
    "    -  Grouping data\n",
    "    -  Establishing intuition\n",
    "* When clustering data it is a good idea to try many approaches with difference distance/error metrics to get a feel for which factors are important. \n",
    "\n",
    "* If possible, visualizing the data directly can provide substantial intuition about which methods will perform best.\n",
    "* Dendrograms are a useful way to indirectly visualize data in a high-dimensional space, and the difference between a dendrogram and a direct measure of distance can be quantified with the **cophenetic coefficient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* [Evaluation of clustering algorithms](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
    "* [Overview of 5 key clustering algorithms](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n",
    "* [Python Data Science Handbook: k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "* [Python Data Science Handbook: GMM](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n",
    "* [GMM formulas](https://brilliant.org/wiki/gaussian-mixture-model/)\n",
    "* [Color quantization example](http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html)\n",
    "* [Dendrogram and hierarchical clustering tutorial](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)\n",
    "* Hastie Ch. 13 (pg. 459)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
